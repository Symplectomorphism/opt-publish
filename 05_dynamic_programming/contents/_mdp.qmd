# Markov Decision Processes (MDP)

## Introduction

* MDPs are a classical formalization of sequential decision making.
  - Actions influence not just immediate rewards, but also subsequent states,
    and through those, future rewards.
  - They are meant to be a straightforward framing of the problem of learning
    from interaction to achieve a goal.
* MDPs involve delayed reward and the need to trade off immediate and delayed 
reward.
* We will estimate the value $q_\ast(s, a)$ of each action $a$ in each state
$s$,
  - or we estimate the value $v_\ast(s)$ of each state given optimal action
    selections.
* These state-dependent quantities are essential to accurately assigning credit
for long-term consequences to individual action selections.

## The Agent &ndash; Environment Interface {.smaller}

:::: {.columns}

::: {.column width="50%"}

::: {.callout-warning appearance="minimal"}
* The decision maker is called the _agent_ or _controller_.
* The thing it interacts with, everything outside the agent, is called the
_environment_ or _plant_.
* These interact continually, the agent selecting _actions_ (or _control
 signal_) and the environment responding to these control signals and presenting
 new situations to the agent.
* The environment also gives rise to _rewards_, special numerical values that the
agent seeks to maximize over time through its choice of actions.
:::

::: {.callout-warning appearance="minimal"}
* The agent and environment interact at each of a sequence of discrete time
steps, $t = 0, 1, 2, \ldots$. 
  - At each time step $t$, the agent receives some respresentation of the
    environment's _state_$, $S_t \in \mathcal{S}$, and on that basis selects an
    action $A_t \in \mathcal{A}(s)$.
  - One time step later, in part as a consequence of its actions, the agent
    receives a numerical reward, $R_{t+1} \in \mathcal{R} \in \mathbb{R}$ and
    finds itself in a new state, $S_{t+1}$.
* The MDP and agent together give rise to a trajectory that begins like this:
$$ S_0, A_0, S_1, R_1, A_2, R_2, S_2, A_2, R_3, \ldots $$ {#eq-traj}
:::

:::

::: {.column width="50%"}
<center>
<img src="contents/assets/agent_environment.png" width="95%" /img>
</center>

::: {.callout-important appearance="minimal"}
* In a _finite_ MDP, the sets of states, actions, and rewards 
($\mathcal{S}$, $\mathcal{A}$, and $\mathcal{R}$) all have a finite number of elements.
  - In this case, the random variables $R_t$ and $S_t$ have
    well-defined discrete probability distributions dependent on the preceding
    state and action.

$$ p(s', r \mid s, a) \triangleq \mathbb{P}\{S_t = s', R_t = r \mid S_{t-1} = s,
A_{t-1} = a\} $$ {#eq-dyn}

* This function $p$ defines the _dynamics_ of the MDP.
  - It specifies a probability distribution for each choice of $s$ and $a$,
    i.e.,

$$ \sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s', r \mid s, a) = 1,
\quad \forall s \in \mathcal{S}, \; a \in \mathcal{A}(s). $$ {#eq-pprob}

* This is called the _Markov property_.
:::

:::


::::
