[
  {
    "objectID": "05_dynamic_programming.html#optimization-theory-and-practice",
    "href": "05_dynamic_programming.html#optimization-theory-and-practice",
    "title": "05_dynamic_programming",
    "section": "Optimization Theory and Practice",
    "text": "Optimization Theory and Practice\n\n\nDynamic Programming\n\n\n\n\nInstructor: Aykut Satici, Ph.D.¬†  Mechanical and Biomedical Engineering  Electrical and Computer Engineering  Boise State University, Boise, ID, USA\n\n\nTopics:  Markov Decision Processes  Policies and Value Functions  Policy Evaluation  Policy Improvement  Policy Iteration  Value Iteration"
  },
  {
    "objectID": "05_dynamic_programming.html#introduction",
    "href": "05_dynamic_programming.html#introduction",
    "title": "05_dynamic_programming",
    "section": "Introduction",
    "text": "Introduction\n\nMDPs are a classical formalization of sequential decision making.\n\nActions influence not just immediate rewards, but also subsequent states, and through those, future rewards.\nThey are meant to be a straightforward framing of the problem of learning from interaction to achieve a goal.\n\nMDPs involve delayed reward and the need to trade off immediate and delayed reward.\nWe will estimate the value q*(s,a)q_\\ast(s, a) of each action aa in each state ss,\n\nor we estimate the value v*(s)v_\\ast(s) of each state given optimal action selections.\n\nThese state-dependent quantities are essential to accurately assigning credit for long-term consequences to individual action selections."
  },
  {
    "objectID": "05_dynamic_programming.html#the-agent-environment-interface",
    "href": "05_dynamic_programming.html#the-agent-environment-interface",
    "title": "05_dynamic_programming",
    "section": "The Agent ‚Äì Environment Interface",
    "text": "The Agent ‚Äì Environment Interface\n\n\n\n\n\n\nThe decision maker is called the agent or controller.\nThe thing it interacts with, everything outside the agent, is called the environment or plant.\nThese interact continually, the agent selecting actions (or control signal) and the environment responding to these control signals and presenting new situations to the agent.\nThe environment also gives rise to rewards, special numerical values that the agent seeks to maximize over time through its choice of actions.\n\n\n\n\n\n\n\n\nThe agent and environment interact at each of a sequence of discrete time steps, t=0,1,2,‚Ä¶t = 0, 1, 2, \\ldots.\n\nAt each time step tt, the agent receives some respresentation of the environment‚Äôs state$, St‚ààùíÆS_t \\in \\mathcal{S}, and on that basis selects an action At‚ààùíú(s)A_t \\in \\mathcal{A}(s).\nOne time step later, in part as a consequence of its actions, the agent receives a numerical reward, Rt+1‚àà‚Ñõ‚àà‚ÑùR_{t+1} \\in \\mathcal{R} \\in \\mathbb{R} and finds itself in a new state, St+1S_{t+1}.\n\nThe MDP and agent together give rise to a trajectory that begins like this: S0,A0,S1,R1,A2,R2,S2,A2,R3,‚Ä¶(1) S_0, A_0, S_1, R_1, A_2, R_2, S_2, A_2, R_3, \\ldots  \\qquad(1)\n\n\n\n\n\n\n\n\n\n\n\n\nIn a finite MDP, the sets of states, actions, and rewards (ùíÆ\\mathcal{S}, ùíú\\mathcal{A}, and ‚Ñõ\\mathcal{R}) all have a finite number of elements.\n\nIn this case, the random variables RtR_t and StS_t have well-defined discrete probability distributions dependent on the preceding state and action.\n\n\np(s‚Ä≤,r‚à£s,a)‚âú‚Ñô{St=s‚Ä≤,Rt=r‚à£St‚àí1=s,At‚àí1=a}(2) p(s', r \\mid s, a) \\triangleq \\mathbb{P}\\{S_t = s', R_t = r \\mid S_{t-1} = s,\nA_{t-1} = a\\}  \\qquad(2)\n\nThis function pp defines the dynamics of the MDP.\n\nIt specifies a probability distribution for each choice of ss and aa, i.e.,\n\n\n‚àës‚Ä≤‚ààùíÆ‚àër‚àà‚Ñõp(s‚Ä≤,r‚à£s,a)=1,‚àÄs‚ààùíÆ,a‚ààùíú(s).(3) \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r \\mid s, a) = 1,\n\\quad \\forall s \\in \\mathcal{S}, \\; a \\in \\mathcal{A}(s).  \\qquad(3)\n\nThis is called the Markov property.\n\n\n\n\n\n\n\n\nOptimization Theory and Practice ‚Ä¢ Aykut C. Satici"
  }
]