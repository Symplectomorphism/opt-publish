[
  {
    "objectID": "05_mdp_lp.html#optimization-theory-and-practice",
    "href": "05_mdp_lp.html#optimization-theory-and-practice",
    "title": "05_mdp_lp",
    "section": "Optimization Theory and Practice",
    "text": "Optimization Theory and Practice\n\n\nMarkov Decision Processes and Linear Programming\n\n\n\n\nInstructor: Aykut Satici, Ph.D.   Mechanical and Biomedical Engineering  Electrical and Computer Engineering  Boise State University, Boise, ID, USA\n\n\nTopics:  Markov Decision Processes  Policies and Value Functions"
  },
  {
    "objectID": "05_mdp_lp.html#introduction",
    "href": "05_mdp_lp.html#introduction",
    "title": "05_mdp_lp",
    "section": "Introduction",
    "text": "Introduction\n\nMDPs are a classical formalization of sequential decision making.\n\nActions influence not just immediate rewards, but also subsequent states, and through those, future rewards.\nThey are meant to be a straightforward framing of the problem of learning from interaction to achieve a goal.\n\nMDPs involve delayed reward and the need to trade off immediate and delayed reward.\nWe will estimate the value q*(s,a)q_\\ast(s, a) of each action aa in each state ss,\n\nor we estimate the value v*(s)v_\\ast(s) of each state given optimal action selections.\n\nThese state-dependent quantities are essential to accurately assigning credit for long-term consequences to individual action selections."
  },
  {
    "objectID": "05_mdp_lp.html#the-agent-environment-interface",
    "href": "05_mdp_lp.html#the-agent-environment-interface",
    "title": "05_mdp_lp",
    "section": "The Agent – Environment Interface",
    "text": "The Agent – Environment Interface\n\n\n\n\n\n\nThe decision maker is called the agent or controller.\nThe thing it interacts with, everything outside the agent, is called the environment or plant.\nThese interact continually, the agent selecting actions (or control signal) and the environment responding to these control signals and presenting new situations to the agent.\nThe environment also gives rise to rewards, special numerical values that the agent seeks to maximize over time through its choice of actions.\n\n\n\n\n\n\n\n\nThe agent and environment interact at each of a sequence of discrete time steps, t=0,1,2,…t = 0, 1, 2, \\ldots.\n\nAt each time step tt, the agent receives some respresentation of the environment’s state, St∈𝒮S_t \\in \\mathcal{S}, and on that basis selects an action At∈𝒜(s)A_t \\in \\mathcal{A}(s).\nOne time step later, in part as a consequence of its actions, the agent receives a numerical reward, Rt+1∈ℛ∈ℝR_{t+1} \\in \\mathcal{R} \\in \\mathbb{R} and finds itself in a new state, St+1S_{t+1}.\n\nThe MDP and agent together give rise to a trajectory that begins like this: S0,A0,S1,R1,A2,R2,S2,A2,R3,…(1) S_0, A_0, S_1, R_1, A_2, R_2, S_2, A_2, R_3, \\ldots  \\qquad(1)\n\n\n\n\n\n\n\n\n\n\n\n\nIn a finite MDP, the sets of states, actions, and rewards\n(𝒮,𝒜,ℛ)(\\mathcal{S}, \\mathcal{A}, \\mathcal{R}) all have a finite number of elements.\n\nIn this case, the random variables RtR_t and StS_t have well defined discrete probability distributions dependent on the preceding state and action.\n\n\np(s′,r∣s,a)≜ℙ{St=s′,Rt=r∣St−1=s,At−1=a}(2) p(s', r \\mid s, a) \\triangleq \\mathbb{P}\\{S_t = s', R_t = r \\mid S_{t-1} = s,\nA_{t-1} = a\\}  \\qquad(2)\n\nThis function pp defines the dynamics of the MDP.\n\nIt specifies a probability distribution for each choice of ss and aa, i.e.,\n\n\n∑s′∈𝒮∑r∈ℛp(s′,r∣s,a)=1,∀s∈𝒮,a∈𝒜(s).(3) \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r \\mid s, a) = 1,\n\\quad \\forall s \\in \\mathcal{S}, \\; a \\in \\mathcal{A}(s).  \\qquad(3)\n\nThis is called the Markov property."
  },
  {
    "objectID": "05_mdp_lp.html#the-agent-environment-interface-1",
    "href": "05_mdp_lp.html#the-agent-environment-interface-1",
    "title": "05_mdp_lp",
    "section": "The Agent – Environment Interface",
    "text": "The Agent – Environment Interface\n\nFrom the four-argument dynamics function, pp, one can compute anything else one might want to know about the environment\n\nthe state-transition probabilities p(s′∣s,a)≜ℙ{St=s′∣St−1=a,At−1=a}=∑r∈ℛp(s′,r∣s,a).(4) p(s' \\mid s, a) \\triangleq \\mathbb{P}\\{S_t = s' \\mid S_{t-1} = a, A_{t-1} =\na\\} = \\sum_{r \\in \\mathcal{R}} p(s', r \\mid s, a).  \\qquad(4)\nthe expected rewards for state-action pairs as a two-argument function r(s,a)≜𝔼[Rt∣St−1=s,At−1=a]=∑r∈ℛr∑s′∈𝒮p(s′,r∣s,a),(5) r(s, a) \\triangleq \\mathbb{E}[R_t \\mid S_{t-1} = s, A_{t-1} = a] = \\sum_{r\n\\in \\mathcal{R}} r \\sum_{s' \\in \\mathcal{S}} p(s', r \\mid s, a),  \\qquad(5)\nthe expected rewards for state-action-next-state triples as a three argument function r(s,a,s′)≜𝔼[Rt∣St−1=s,At−1=a,St=s′]=∑r∈ℛrp(s′,r∣s,a)p(s′∣s,a).(6) r(s, a, s') \\triangleq \\mathbb{E}[R_t \\mid S_{t-1} = s, A_{t-1} = a, S_t =\ns'] = \\sum_{r \\in \\mathcal{R}} r \\; \\frac{p(s', r \\mid s, a)}{p(s' \\mid s, a)}.\n \\qquad(6)\n\n\n\n\nThe time steps can refer to arbitrary successive stages of decision making.\nThe actions can be low-level controls (e.g. voltages), or high-level decisions (e.g. have lunch, go to grad school).\nThey can be determined by low-level sensing (e.g. sensor readings) or they can be more high-level and abstract (e.g. symbolic descriptions of objects in a room).\nStates can be anything we can know that might be useful in making the decisions."
  },
  {
    "objectID": "05_mdp_lp.html#the-agent-environment-interface-2",
    "href": "05_mdp_lp.html#the-agent-environment-interface-2",
    "title": "05_mdp_lp",
    "section": "The Agent – Environment Interface",
    "text": "The Agent – Environment Interface\n\n\n\nRule of thumb\n\n\n\nAnything that cannot be changed arbitrarily by the agent is considered to be outside of it and thus part of its environment.\n\n\n\n\n\nNot everything in the environtment is unknown to the agent.\n\nFor example, the agent often knows how its rewards are computed as a function of its actions and the states.\nReward computation is external to the agent because it defines the task facing the agent and thus is beyond its ability to change arbitrarily.\n\nIn fact, some agents know everything about how its environment.\n\n\n\n\nMDP framework\n\n\nWhatever the details of the sensory, memory, and control apparatus, and whatever objective one is trying to achieve, any problem of learning goal-directed behavior can be reduced to three signals passing back and forth between an agent and its environment:\n\none signal to represent the choices made by the agent (the actions),\none signal to represent the basis on which the choices are made (the states),\none signal to define the agent’s goal (the rewards)."
  },
  {
    "objectID": "05_mdp_lp.html#goals-and-rewards",
    "href": "05_mdp_lp.html#goals-and-rewards",
    "title": "05_mdp_lp",
    "section": "Goals and Rewards",
    "text": "Goals and Rewards\n\nThe agent’s goal is to maximize the total amount of reward it receives; not the immediate reward, but the cumulative reward in the long run.\n\n\n\n\nReward Hypothesis\n\n\nAll of what we mean by goal and purposes can be well thought as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).\n\n\n\n\nFormulating goals in terms of reward signals has proved to be flexible and widely applicable.\n\n\n\n\nExamples\n\n\n\n\n\n\n\n\n\nTask\nReward\n\n\n\n\nRobot walking\n++ve reward on each time step proportional to robot’s forward motion\n\n\nEscape from a maze\n−1-1 reward for every time step that passes prior to escape\n\n\nPlaying chess\n+1+1 for winning, −1-1 for losing, 00 for drawing\n\n\n\n\n\n\n\nThe reward signal is no the place to impart to the agent prior knowledge about how to achieve what we want it to do (better places: initial policy, iniial value function).\n\nOtherwise the agent might find a way to achieve subgoals without achieving the real goal!"
  },
  {
    "objectID": "05_mdp_lp.html#returns-and-episodes",
    "href": "05_mdp_lp.html#returns-and-episodes",
    "title": "05_mdp_lp",
    "section": "Returns and Episodes",
    "text": "Returns and Episodes\n\nIn general, we seek to maximize the expected return, where the return, denoted GtG_t, is defined as some specific function of the reward sequence.\n\nSimplest (bit naïve) case: Gt≜Rt+1+Rt+2+⋯+RT=∑k=t+1TRk,(7) G_t \\triangleq R_{t+1} + R_{t+2} + \\cdots + R_T = \\sum_{k=t+1}^T R_k,   \\qquad(7)\nDiscounted return: Gt≜∑k=0∞γkRt+k+1=∑k=t+1Tγk−t−1Rk,0≤γ≤1.(8) G_t \\triangleq \\sum_{k=0}^\\infty \\gamma^kR_{t+k+1} =\n\\sum_{k=t+1}^T \\gamma^{k-t-1}R_k, \\quad 0 \\leq \\gamma \\leq 1.  \\qquad(8)\n\nThe naïve approach makes sense in applications in which there is a natural notion of a final time step (episodes).\n\nEach episodes ends in a state called the terminal state.\nIn episodic tasks we sometimes need to distinguish the set of all nonterminal states, 𝒮\\mathcal{S} from the set of all states plus the terminal state, 𝒮+\\mathcal{S}^+.\n\nIf the task has no final time, or terminal state, then T=∞T=\\infty.\n\nThe naïve return may easily become infinite. Hence we use the discounted return. Gt=Rt+1+γ(Rt+2+γRt+3+γ2Rt+4+⋯)=Rt+1+γGt+1.(9) G_t = R_{t+1} + \\gamma(R_{t+2} + \\gamma R_{t+3} + \\gamma^2 R_{t+4} + \\cdots)\n= R_{t+1} + \\gamma G_{t+1}.  \\qquad(9)"
  },
  {
    "objectID": "05_mdp_lp.html#how-good-are-a-state-and-an-action",
    "href": "05_mdp_lp.html#how-good-are-a-state-and-an-action",
    "title": "05_mdp_lp",
    "section": "How good are a state and an action?",
    "text": "How good are a state and an action?\n\nThe rewards the agent can expect to receive in the future depend on what actions it will take.\nValue functions are defined with respect to paricular ways of acting, called policies.\n\n\n\n\n\n\nDefinition\n\n\nA policy is a mapping from states o probabilities of selecting each possible action.\n\nIf the agent is following policy π\\pi at time tt, then π(a|s)\\pi(a|s) is the probability that At=aA_t = a given that St=sS_t = s.\nπ(a|s)\\pi(a|s) defines a probability distribution over a∈𝒜(s)a \\in \\mathcal{A}(s) for each s∈𝒮s \\in \\mathcal{S}.\n\n\n\n\n\n\n\n\nDefinition (State-Value Function for Policy π\\pi)\n\n\nThe value function of a state ss under a policy π\\pi, denoted vπ(s)v_\\pi(s), is the expected return when starting in ss and following π\\pi thereafter.\nvπ(s)≜𝔼π[Gt|St=s]=𝔼π[∑k=0∞γkRt+k+1|St=s],∀s∈𝒮.(10) \n\\begin{align}\nv_\\pi(s) &\\triangleq \\mathbb{E}_\\pi[G_t | S_t = s] \\\\ \n&= \\mathbb{E}_\\pi\\left[\\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} | S_t = s \\right],\n\\;\\; \\forall s \\in \\mathcal{S}.\n\\end{align}\n \\qquad(10)\n\nThe value of the terminal state, if any, is always zero.\n\n\n\n\n\n\n\n\n\nDefinition (Action-Value Function for Policy π\\pi)\n\n\nWe define the value of taking action aa in state ss under a policy π\\pi, denoted qπ(s,a)q_\\pi(s, a), as the expected return starting from ss, taking the action aa, and thereafter following policy π\\pi:\nqπ(s,a)≜𝔼π[Gt|St=s,At=a]=𝔼π[∑k=0∞γkRt+k+1|St=s,At=a].(11) \n\\begin{align}\nq_\\pi(s, a) &\\triangleq \\mathbb{E}_\\pi[G_t | S_t=s, A_t=a] \\\\\n&= \\mathbb{E}_\\pi\\left[\\sum_{k=0}^\\infty \\left. \\gamma^k R_{t+k+1}\\right| S_t=s, \nA_t=a \\right].\n\\end{align}\n \\qquad(11)"
  },
  {
    "objectID": "05_mdp_lp.html#bellman-equation",
    "href": "05_mdp_lp.html#bellman-equation",
    "title": "05_mdp_lp",
    "section": "Bellman Equation",
    "text": "Bellman Equation\n\n\n\nFundamental property of Value Functions\n\n\nValue functions saisfy recursive relationships similar that which we established for the return Equation 9.\nFor any policy π\\pi and any state ss, the following consistency condition holds between he value of ss and he value of its possible successor states:\nvπ(s)≜=𝔼π[Gt|St=s]=𝔼π[Rt+1+γGt+1|St=s],by (9)=∑a∈𝒜(s)π(a|s)∑s′,rp(s′,r|s,a)[r+γ𝔼π[Gt+1|St+1=s′]]=∑a∈𝒜(s)π(a|s)∑s′,rp(s′,r|s,a)[r+γvπ(s′)],∀s∈𝒮.(12)\n\\begin{align}\nv_\\pi(s) \\triangleq &= \\mathbb{E}_\\pi[G_t|S_t=s] = \\mathbb{E}_\\pi[R_{t+1} +\n\\gamma G_{t+1}|S_t = s], \\quad {\\text{by (9)} }\\\\\n&= \\sum_{a \\in \\mathcal{A}(s)} \\pi(a|s)\\sum_{s',r}p(s',r|s,a)\\left[r + \\gamma\n\\mathbb{E}_\\pi[G_{t+1}|S_{t+1}=s']\\right] \\\\\n&= \\sum_{a \\in \\mathcal{A}(s)} \\pi(a|s) \\sum_{s', r}p(s',r|s,a)\\left[r + \\gamma\nv_\\pi(s')\\right],\n\\quad \\forall s \\in \\mathcal{S}.\n\\end{align}\n \\qquad(12)\n\n\n\n\n\n\nEach open circle represents a state and each solid circle represents a state-action pair.\nStarting from state ss, the root node at the top, the agent could take any of some set of actions — three are shown — based on its policy π\\pi.\n\n\n\n\n\n\n\n\nFrom each of these, the environment could respond with one of several next states, s′s' (two are shown), along with a reward, rr, depending on its dynamics, given by the function pp.\nBellman equation averages over all the possibilites, weighing each by its probability of occuring: the value of the start state must equal the (discounted) value of the expected next state, plus the reward expected along the way."
  },
  {
    "objectID": "05_mdp_lp.html#optimal-policies-and-optimal-value-functions",
    "href": "05_mdp_lp.html#optimal-policies-and-optimal-value-functions",
    "title": "05_mdp_lp",
    "section": " Optimal Policies and Optimal Value Functions ",
    "text": "Optimal Policies and Optimal Value Functions \n\nWe want to find a policy that achieves a lot of reward over the long run.\nValue functions define a partial ordering over policies: a policy π\\pi is defined to be better than or equal to a policy π′\\pi' if its expected return is greater than or equal to that of π′\\pi' for all states. π≥π′⇔vπ(s)≥vπ′(s),∀s∈𝒮. \\pi \\geq \\pi' \\Longleftrightarrow v_{\\pi}(s) \\geq v_{\\pi'}(s), \\quad\n\\forall s \\in \\mathcal{S}. \n∃\\exists always at least one policy that is better than or equal to all other policies: an optimal policy.\n\nAlthough there may be more than one, let us denot all the optimal policies by π*\\pi_\\ast.\nThey share the same state-value function, called the optimal state-value function:\nv*(s)≜maxπvπ(s),∀s∈𝒮.(13) v_\\ast(s) \\triangleq \\operatorname{max}_\\pi v_{\\pi}(s), \\quad\n\\forall s \\in \\mathcal{S}.  \\qquad(13)\n\nOptimal policies also share the same optimal action-value function:\nq*(s,a)≜maxπqπ(s,a),∀s∈𝒮,a∈𝒜(s).(14) q_\\ast(s, a) \\triangleq \\operatorname{max}_\\pi q_{\\pi}(s, a), \\quad\n  \\forall s \\in \\mathcal{S}, \\; a \\in \\mathcal{A}(s).  \\qquad(14)\nFor the state-action pair (s,a)(s,a), this function gives the expected return for taking action aa in state ss and thereafter following an optimal policy.\n\nThus, we can write q*q_\\ast in terms of v*v_\\ast as follows: q*(s,a)=𝔼[Rt+1+γv*(St+1)∣St=s,At=a].(15) q_\\ast (s, a) = \\mathbb{E}[R_{t+1} + \\gamma v_\\ast(S_{t+1}) \\mid S_t = s, A_t =\na].  \\qquad(15)"
  },
  {
    "objectID": "05_mdp_lp.html#bellman-optimality-equation",
    "href": "05_mdp_lp.html#bellman-optimality-equation",
    "title": "05_mdp_lp",
    "section": "Bellman Optimality Equation",
    "text": "Bellman Optimality Equation\n\nBecause v*v_\\ast is the value function for a policy, it must satisfy the self-consistency condition given by the Bellman equation for state values Equation 12.\nBut it is also the optimal value function so v*v_\\ast’s consistency condition can be written in a special form w/o reference to any specific policy!\n\n\n\n\nThe Bellman optimality equation expresses the fact that the value of a state under an optimal policy must equal the expected return for the best action from that state.\n\n\n\nv*(s)=maxa∈𝒜(s)qπ*(s,a)=maxa𝔼π*[Gt|St=s,At=a]=maxa𝔼π*[Rt+1+γGt+1|St=s,At=a]=maxa𝔼π*[Rt+1+γv*(St+1)|St=s,At=a]=maxa∑s′,rp(s′,r|s,a)[r+γv*(s′)].(16)\n\\begin{align}\nv_\\ast(s) &= \\operatorname{max}_{a \\in \\mathcal{A}(s)} q_{\\pi_\\ast}(s, a) =\n\\operatorname{max}_{a} \\mathbb{E}_{\\pi_\\ast}[G_t | S_t =s, A_t = a] \\\\\n&= \\operatorname{max}_a \\mathbb{E}_{\\pi_\\ast}[R_{t+1} + \\gamma G_{t+1} | S_t =\ns, A_t = a] \\\\\n&= \\operatorname{max}_a \\mathbb{E}_{\\pi_\\ast}[R_{t+1} + \\gamma v_\\ast(S_{t+1}) | S_t =\ns, A_t = a] \\\\\n&= \\operatorname{max}_a \\sum_{s', r} p(s', r|s, a)[r + \\gamma v_\\ast(s')].\n\\end{align}\n \\qquad(16)\n\nThe Bellman optimality equation for q*q_\\ast is\n\nq*(s,a)=𝔼[Rt+1+γmaxa′q*(St+1,a)∣St=s,At=a]=∑s′,rp(s′,r|s,a)[r+γmaxa′q*(s′,a′)].(17)\nq_\\ast(s, a) = \\mathbb{E}\\left[R_{t+1} + \\gamma \\operatorname{max}_{a'}\nq_\\ast(S_{t+1}, a) \\mid S_t = s, A_t = a\\right] = \\sum_{s', r} p(s', r|s, a) \n\\left[ r+ \\gamma \\operatorname{max}_{a'} q_\\ast (s', a') \\right].\n \\qquad(17)"
  }
]