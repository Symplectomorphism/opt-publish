[
  {
    "objectID": "09_aug_lagrangian.html#optimization-theory-and-practice",
    "href": "09_aug_lagrangian.html#optimization-theory-and-practice",
    "title": "09_aug_lagrangian",
    "section": "Optimization Theory and Practice",
    "text": "Optimization Theory and Practice\n\n\nLocal Duality and Dual Methods\n\n\n\n\nInstructor: Aykut Satici, Ph.D.Â   Mechanical and Biomedical Engineering  Electrical and Computer Engineering  Boise State University, Boise, ID, USA\n\n\nTopics:  Local Duality and the Lagrangian Method  The Augmented Lagrangian (M. of Multipliers)  Separable Problems and Their Duals  The Alternating Direction M. of Multipliers"
  },
  {
    "objectID": "09_aug_lagrangian.html#local-duality",
    "href": "09_aug_lagrangian.html#local-duality",
    "title": "09_aug_lagrangian",
    "section": "Local Duality",
    "text": "Local Duality\n\n\n\n\n\nNonlinear Programming Problem\n\n\nminimizef(ğ±),f,ğ¡âˆˆC2,subject toğ¡(ğ±)=ğŸ,ğ±âˆˆâ„n,ğ¡(ğ±)âˆˆâ„m.(1)\n\\begin{align}\n\\operatorname{minimize} & f(\\bm{x}), & f, \\bm{h} \\in C^2, \\\\\n\\text{subject to} & \\bm{h}(\\bm{x}) = \\bm{0}, & \\bm{x} \\in \\mathbb{R}^n, \n\\bm{h}(\\bm{x}) \\in \\mathbb{R}^m.\n\\end{align}\n \\qquad(1)\nEverything we do can be easily extended to problems having inequality as well as equality constraints for the price of a somewhat more involved notation.\n\n\n\n\nAssume that ğ±*\\bm{x}^\\ast is a regular point of the constraints.\n\nThere is then a Lagrange multiplier vector ğ›Œ*\\bm{\\lambda}^\\ast such that\n\n\nâˆ‡f(ğ±*)âˆ’(ğ›Œ*)âŠ¤âˆ‡ğ¡(ğ±*)=ğŸ,(2)\n\\nabla f(\\bm{x}^\\ast) - \\left(\\bm{\\lambda}^\\ast\\right)^\\top \\nabla \\bm{h}\n(\\bm{x}^\\ast) = \\bm{0},\n \\qquad(2)\nand the Hessian of the Lagrangian â„“(ğ±,ğ›Œ*)=f(ğ±)âˆ’(ğ›Œ*)âŠ¤ğ¡(ğ±)\\ell(\\bm{x}, \\bm{\\lambda}^\\ast) = f(\\bm{x}) - \\left(\\bm{\\lambda}^\\ast\\right)^\\top \\bm{h}(\\bm{x})\nğ‹(ğ±*)=ğ…(ğ±*)âˆ’(ğ›Œ*)âŠ¤ğ‡(ğ±*)(3)\n\\bm{L}(\\bm{x}^\\ast) = \\bm{F}(\\bm{x}^\\ast) - \\left(\\bm{\\lambda}^\\ast\\right)^\\top \\bm{H}(\\bm{x}^\\ast)\n \\qquad(3)\nmust be positive semidefinite on the tangent subspace\nM={ğ±:âˆ‡ğ¡(ğ±*)=ğŸ}. M = \\{\\bm{x}: \\nabla \\bm{h}(\\bm{x}^\\ast) = \\bm{0}\\}. \n\n\n\n\nLocal Convexity Assumption\n\n\nWe assume that the Hessian ğ‹(ğ±*)\\bm{L(\\bm{x}^\\ast)} is positive definite. (We mean that ğ‹(ğ±*)\\bm{L(\\bm{x}^\\ast)} on the whole space â„n\\mathbb{R}^n, not just on the subspace MM.)\nThis assumption guarantees that the Lagrangian â„“(ğ±,ğ›Œ*)\\ell(\\bm{x}, \\bm{\\lambda}^\\ast) is locally convex at ğ±*\\bm{x}^\\ast.\n\n\n\n\nWith this assumption, the point ğ±*\\bm{x}^\\ast is not only a local solution to the constrained problem EquationÂ 1; it is also a local solution to the unconstrained problem\n\nminimizeâ„“(ğ±,ğ›Œ*)=f(ğ±)âˆ’(ğ›Œ*)âŠ¤ğ¡(ğ±)(4)\n\\begin{align}\n\\operatorname{minimize} & \\ell(\\bm{x}, \\bm{\\lambda}^\\ast) = f(\\bm{x}) - \n\\left( \\bm{\\lambda}^\\ast \\right)^\\top \\bm{h}(\\bm{x})\n\\end{align}\n \\qquad(4)\n\nFor any ğ›Œ\\bm{\\lambda} sufficiently close to ğ›Œ*\\bm{\\lambda}^\\ast, the function â„“(ğ±,ğ›Œ)\\ell(\\bm{x}, \\bm{\\lambda}) will have a local minimum point at a point ğ±\\bm{x} near ğ±*\\bm{x}^\\ast.\n\nThis follows by noting that by the implicit function theorem, the eqn. âˆ‡f(ğ±)âˆ’ğ›ŒâŠ¤âˆ‡ğ¡(ğ±)=ğŸ\n\\nabla f(\\bm{x}) - \\bm{\\lambda}^\\top \\nabla \\bm{h}\n(\\bm{x}) = \\bm{0}\n\n\n\nhas a solution ğ±\\bm{x} near ğ±*\\bm{x}^\\ast when ğ›Œ\\bm{\\lambda} is near ğ›Œ*\\bm{\\lambda}^\\ast because ğ‹*\\bm{L}^\\ast is positive definite."
  },
  {
    "objectID": "09_aug_lagrangian.html#local-duality-1",
    "href": "09_aug_lagrangian.html#local-duality-1",
    "title": "09_aug_lagrangian",
    "section": "Local Duality",
    "text": "Local Duality\n\n\n\n\n\nThus, locally there is a unique correspondence between ğ›Œ\\bm{\\lambda} and ğ±\\bm{x} through the solution of the unconstrained problem EquationÂ 4. minimizeâ„“(ğ±,ğ›Œ)=f(ğ±)âˆ’ğ›ŒâŠ¤ğ¡(ğ±).(5)\n\\begin{align}\n\\operatorname{minimize} & \\ell(\\bm{x}, \\bm{\\lambda}) = f(\\bm{x}) - \\bm{\\lambda}^\\top \\bm{h}(\\bm{x}).\n\\end{align}\n \\qquad(5)\n\nThis correspondence is continuously differentiable.\n\nNear ğ›Œ*\\bm{\\lambda}^\\ast we define the dual function Ï•\\phi by the equation Ï•(ğ›Œ)â‰œminğ±âˆˆğ’©(ğ±*)[â„“(ğ±,ğ›Œ)=f(ğ±)âˆ’ğ›ŒâŠ¤ğ¡(ğ±)](6)\n\\phi(\\bm{\\lambda}) \\triangleq \\operatorname{min}_{\\bm{x} \\in \\mathcal{N}(\\bm{x}^\\ast)} \n\\left[ \\ell(\\bm{x}, \\bm{\\lambda}) = f(\\bm{x}) - \\bm{\\lambda}^\\top \\bm{h}(\\bm{x}) \\right]\n \\qquad(6)\nWe are then able to show that locally the original constrained problem EquationÂ 4 is equivalent to unconstrained local maximization of of the dual function Ï•\\phi with respect to ğ›Œ\\bm{\\lambda}.\n\nDenote by ğ±(ğ›Œ)\\bm{x}(\\bm{\\lambda}) the unique solution to EquationÂ 5 in the neighborhood of ğ±*\\bm{x}^\\ast.\n\n\n\n\n\n\n\n\nLemma 1\n\n\nThe dual function Ï•\\phi has gradient âˆ‡Ï•(ğ›Œ)=âˆ’ğ¡(ğ±(ğ›Œ))âŠ¤.(7) \\nabla \\phi(\\bm{\\lambda}) = -\\bm{h}(\\bm{x}(\\bm{\\lambda}))^\\top.  \\qquad(7)\n\n\n\n\n\n\nProof\n\n\nWe have explicitly from EquationÂ 6 Ï•(ğ›Œ)=f(ğ±(ğ›Œ))âˆ’ğ›ŒâŠ¤ğ¡(ğ±(ğ›Œ)). \\phi(\\bm{\\lambda}) = f(\\bm{x}(\\bm{\\lambda})) - \\bm{\\lambda}^\\top \\bm{h}(\\bm{x}(\\bm{\\lambda})).  Thus âˆ‡Ï•(ğ›Œ)=[âˆ‡f(ğ±(ğ›Œ))âˆ’ğ›ŒâŠ¤âˆ‡ğ¡(ğ±(ğ›Œ))]âˆ‡ğ±(ğ›Œ)âˆ’ğ¡(ğ±(ğ›Œ))âŠ¤. \n\\nabla \\phi(\\bm{\\lambda}) = \\left[ \\nabla f(\\bm{x}(\\bm{\\lambda})) - \\bm{\\lambda}^\\top \\nabla \\bm{h}(\\bm{x}(\\bm{\\lambda})) \\right] \\nabla \\bm{x}(\\bm{\\lambda}) - \\bm{h}(\\bm{x}(\\bm{\\lambda}))^\\top. \n Since the first term on the right vanishes by the defition of ğ±(ğ›Œ)\\bm{x}(\\bm{\\lambda}) (the unique solution to EquationÂ 5), we obtain EquationÂ 7.\n\n\n\n\n\n\n\nLemma 2\n\n\nThe Hessian of the dual function is ğš½(ğ›Œ)=âˆ’âˆ‡ğ¡(ğ±(ğ›Œ))ğ‹âˆ’1(ğ±(ğ›Œ),ğ›Œ)âˆ‡ğ¡(ğ±(ğ›Œ))âŠ¤.(8)\n\\bm{\\Phi}(\\bm{\\lambda}) = -\\nabla \\bm{h}(\\bm{x}(\\bm{\\lambda}))\\bm{L}^{-1}(\\bm{x}(\\bm{\\lambda}), \\bm{\\lambda}) \\nabla \\bm{h}(\\bm{x}(\\bm{\\lambda}))^\\top.\n \\qquad(8)\n\n\n\n\n\n\nProof\n\n\nBy Lemma 1, ğš½(ğ›Œ)=âˆ’âˆ‡ğ¡(ğ±(ğ›Œ))âˆ‡ğ±(ğ›Œ)\\bm{\\Phi}(\\bm{\\lambda}) = -\\nabla \\bm{h}(\\bm{x}(\\bm{\\lambda})) \\nabla \\bm{x}(\\bm{\\lambda}).\nDifferentiating âˆ‡f(ğ±(ğ›Œ))âˆ’ğ›ŒâŠ¤âˆ‡ğ¡(ğ±(ğ›Œ))=ğŸ\\nabla f(\\bm{x}(\\bm{\\lambda})) - \\bm{\\lambda}^\\top \\nabla \\bm{h}(\\bm{x}(\\bm{\\lambda})) = \\bm{0} with respect to ğ›Œ\\bm{\\lambda}, we obtain\nğ‹(ğ±(ğ›Œ),ğ›Œ)âˆ‡ğ±(ğ›Œ)âˆ’âˆ‡ğ¡(ğ±(ğ›Œ))âŠ¤=ğŸ. \n\\bm{L}(\\bm{x}(\\bm{\\lambda}), \\bm{\\lambda})\\nabla \\bm{x}(\\bm{\\lambda}) - \\nabla \\bm{h}(\\bm{x}(\\bm{\\lambda}))^\\top = \\bm{0}.\n\nSolving for âˆ‡ğ±(ğ›Œ)\\nabla \\bm{x}(\\bm{\\lambda}) and substituting back to the first eqn., we are through."
  },
  {
    "objectID": "09_aug_lagrangian.html#local-duality-theorem",
    "href": "09_aug_lagrangian.html#local-duality-theorem",
    "title": "09_aug_lagrangian",
    "section": "Local Duality Theorem",
    "text": "Local Duality Theorem\n\n\n\n\nLocal Duality Theorem\n\n\nSuppose that the problem minimizef(ğ±)subject toğ¡(ğ±)=ğŸ\n\\begin{align}\n\\operatorname{minimize} & f(\\bm{x}) \\\\\n\\text{subject to} & \\bm{h}(\\bm{x}) = \\bm{0}\n\\end{align}\n has a local solution at ğ±*\\bm{x}^\\ast with corresponding value r*r^\\ast and Lagrange multiplier ğ›Œ*\\bm{\\lambda}^\\ast. Suppose also that ğ±*\\bm{x}^\\ast is a regular point of the constraints and that the corresponding Hessian of the Lagrangian ğ‹*=ğ‹(ğ±*)\\bm{L}^\\ast = \\bm{L}(\\bm{x}^\\ast) is positive definite. Then the dual problem maximizeÏ•(ğ›Œ) \n\\begin{align}\n\\operatorname{maximize} & \\phi(\\bm{\\lambda})\n\\end{align}\n has a local solution at ğ›Œ*\\bm{\\lambda}^\\ast with corresponding value r*r^\\ast and ğ±*\\bm{x}^\\ast as the point corresponding to ğ›Œ*\\bm{\\lambda}^\\ast in the definition of Ï•\\phi.\n\n\n\n\n\n\nProof\n\n\nIt is clear that ğ±*\\bm{x}^\\ast corresponds to ğ›Œ*\\bm{\\lambda}^\\ast in the definition of Ï•\\phi. Now at ğ›Œ*\\bm{\\lambda}^\\ast we have by Lemma 1, âˆ‡Ï•(ğ›Œ*)=âˆ’ğ¡(ğ±*)âŠ¤=ğŸ,\n\\nabla \\phi(\\bm{\\lambda}^\\ast) = -\\bm{h}(\\bm{x}^\\ast)^\\top = \\bm{0}, \n and by Lemma 2, the Hessian of Ï•\\phi is negative definite. Thus ğ›Œ*\\bm{\\lambda}^\\ast satisfies the SOSC for an unconstrained maximum point of Ï•\\phi. The corresponding value Ï•(ğ›Œ*)\\phi(\\bm{\\lambda}^\\ast) is found from the definition of Ï•\\phi to be r*r^\\ast."
  },
  {
    "objectID": "09_aug_lagrangian.html#example",
    "href": "09_aug_lagrangian.html#example",
    "title": "09_aug_lagrangian",
    "section": "Example",
    "text": "Example\nminimizeâˆ’xysubject to(xâˆ’3)2+y2=5.\n\\begin{align}\n\\operatorname{minimize} & -xy \\\\\n\\text{subject to} & (x-3)^2 + y^2 = 5.\n\\end{align}\n\n\n\n\n\n\nFirst-Order Necessary Conditions\n\n\nâˆ’yâˆ’(2xâˆ’6)Î»=0âˆ’xâˆ’2yÎ»=0.\n\\begin{align}\n-y - (2x - 6)\\lambda &= 0 \\\\\n-x - 2y\\lambda &= 0.\n\\end{align}\n together with the constraint. These equations have a solution at x=4,y=2,Î»=âˆ’1.\nx = 4, \\quad y = 2, \\quad \\lambda = -1.\n The Hessian of the corresponding Lagrangian is ğ‹=[2âˆ’1âˆ’12]. \\bm{L} = \\begin{bmatrix} 2 & -1 \\\\ -1 & 2 \\end{bmatrix}.  Since this is positive definite, we conclude that the solution obtained is a local minimum (it is, in fact, a global minimum).\n\n\n\n\n\n\n\nLocal Duality\n\n\nSince ğ‹\\bm{L} is positive definite, we can apply the local duality theory near this solution. Ï•(Î»)=minx,y{âˆ’xyâˆ’Î»[(xâˆ’3)2+y2âˆ’5]},\n\\phi(\\lambda) = \\operatorname{min}_{x,y} \\left\\{-xy - \\lambda \\left[(x-3)^2 + y^2 - 5\\right]\\right\\},\n which leads to Ï•(Î»)=âˆ’4Î»âˆ’4Î»3+80Î»5(4Î»2âˆ’1)2\n\\phi(\\lambda) = \\frac{-4\\lambda - 4\\lambda^3 + 80\\lambda^5}{(4\\lambda^2 - 1)^2}\n valid for Î»&lt;âˆ’12\\lambda &lt; -\\frac{1}{2}. It can be verified that Ï•\\phi has a local maximum at Î»=âˆ’1\\lambda = -1. Plugging this value back in EquationÂ 4 and maximizing (unconstrained) over xx and yy yields the same maximizers as before."
  },
  {
    "objectID": "09_aug_lagrangian.html#inequality-constraints",
    "href": "09_aug_lagrangian.html#inequality-constraints",
    "title": "09_aug_lagrangian",
    "section": "Inequality Constraints",
    "text": "Inequality Constraints\nminimizef(ğ±),fâˆˆC2,ğ±âˆˆâ„nsubject toğ¡(ğ±)=ğŸ,ğ¡âˆˆâ„‚2,ğ¡(ğ±)âˆˆâ„m,ğ (ğ±)â‰¥ğŸ,ğ âˆˆC2,ğ (ğ±)âˆˆâ„p.(9)\n\\begin{align}\n\\operatorname{minimize} & f(\\bm{x}), & f \\in C^2, \\;\\; \\bm{x}\\in \\mathbb{R}^n \\\\ \n\\text{subject to} & \\bm{h}(\\bm{x}) = \\bm{0}, & \\bm{h} \\in \\mathbb{C}^2, \\;\\; \\bm{h}(\\bm{x}) \\in \\mathbb{R}^m, \\\\\\\n& \\bm{g}(\\bm{x}) \\geq \\bm{0}, & \\bm{g} \\in C^2, \\;\\; \\bm{g}(\\bm{x}) \\in \\mathbb{R}^p.\n\\end{align}\n \\qquad(9)\n\nSuppose ğ±*\\bm{x}^\\ast is a local solution of EquationÂ 9 and is a regular point of the constraints.\n\nThen, there are Lagrange multipliers ğ›Œ*\\bm{\\lambda}^\\ast and ğ›*â‰¥ğŸ\\bm{\\mu}^\\ast \\geq \\bm{0} such that âˆ‡f(ğ±*)âˆ’(ğ›Œ*)âŠ¤âˆ‡ğ¡(ğ±*)âˆ’(ğ›*)âŠ¤âˆ‡ğ (ğ±*)=ğŸ,(ğ›*)âŠ¤ğ (ğ±*)=0.\n\\begin{align}\n\\nabla f(\\bm{x}^\\ast) - \\left(\\bm{\\lambda}^\\ast \\right)^\\top \\nabla \n\\bm{h}(\\bm{x}^\\ast) - \\left(\\bm{\\mu}^\\ast\\right)^\\top \\nabla \\bm{g}(\\bm{x}^\\ast) &= \\bm{0}, \\\\\n\\left(\\bm{\\mu}^\\ast\\right)^\\top \\bm{g}(\\bm{x}^\\ast) = 0.\n\\end{align}\n\n\nLocal convexity assumption: Hessian of the Lagrangian is positive definite on the whole space. ğ‹(ğ±*)=ğ…(ğ±*)âˆ’(ğ›Œ*)âŠ¤ğ‡(ğ±*)âˆ’(ğ›*)ğ†(ğ±*)â‰»ğŸ. \\bm{L}(\\bm{x}^\\ast) = \\bm{F}(\\bm{x}^\\ast) - \\left(\\bm{\\lambda}^\\ast\\right)^\\top \n\\bm{H}(\\bm{x}^\\ast) - \\left(\\bm{\\mu}^\\ast\\right)\\bm{G}(\\bm{x}^\\ast) \\succ \\bm{0}.\n\nFor ğ›Œ\\bm{\\lambda} and ğ›â‰¥ğŸ\\bm{\\mu} \\geq \\bm{0} near ğ›Œ*\\bm{\\lambda}^\\ast and ğ›*\\bm{\\mu}^\\ast we can define the dual function Ï•(ğ›Œ,ğ›)â‰œminğ±âˆˆğ’©(ğ±*)[â„“(ğ±,ğ›Œ,ğ›)=f(ğ±)âˆ’ğ›ŒâŠ¤ğ¡(ğ±)âˆ’ğ›âŠ¤ğ (ğ±)],\n\\phi(\\bm{\\lambda}, \\bm{\\mu}) \\triangleq \\operatorname{min}_{\\bm{x} \\in \\mathcal{N}(\\bm{x}^\\ast)} \n\\left[ \\ell(\\bm{x}, \\bm{\\lambda}, \\bm{\\mu}) = f(\\bm{x}) - \\bm{\\lambda}^\\top \n\\bm{h}(\\bm{x}) - \\bm{\\mu}^\\top \\bm{g}(\\bm{x}) \\right],\n where the minimum is taken locally near ğ±*\\bm{x}^\\ast.\nThen it is easy to show, paralleling the devlopment above for equality constraints, that Ï•\\phi achieves a local maximum with respect to ğ›Œ\\bm{\\lambda}, ğ›â‰¥ğŸ\\bm{\\mu} \\geq \\bm{0} at ğ›Œ*\\bm{\\lambda}^\\ast, ğ›*\\bm{\\mu}^\\ast."
  },
  {
    "objectID": "09_aug_lagrangian.html#partial-duality",
    "href": "09_aug_lagrangian.html#partial-duality",
    "title": "09_aug_lagrangian",
    "section": "Partial Duality",
    "text": "Partial Duality\n\n\nIt is not necessary to include the lagrange multipliers of all the cosntraints of a problem in the definition of the dual function.\nIn general, if the local convexity assumption holds, local duality can be defined with respect to any subset of function constraints.\n\nFor example, in problem EquationÂ 9 we might define the dual with respect to only the equality constraints Ï•(ğ›Œ)=minğ (ğ±)â‰¥ğŸ{f(ğ±)âˆ’ğ›ŒâŠ¤ğ¡(ğ±)}, \\phi(\\bm{\\lambda}) = \\operatorname{min}_{\\bm{g}(\\bm{x}) \\geq \\bm{0}} \n\\left\\{f(\\bm{x}) - \\bm{\\lambda}^\\top \\bm{h}(\\bm{x}) \\right\\},  where the minimum is taken locally near the solution ğ±*\\bm{x}^\\ast but constrained by the remaining constraints ğ (ğ±)â‰¥ğŸ\\bm{g}(\\bm{x}) \\geq \\bm{0}.\n\nAgain, the dual function defined in this way will achieve a local maximum at the optimal Lagrange multiplier ğ›Œ*\\bm{\\lambda}^\\ast.\nThe partial dual is especially useful when constraints ğ (ğ±)â‰¥ğŸ\\bm{g}(\\bm{x}) \\geq \\bm{0} are simple such as ğ±â‰¥ğŸ\\bm{x} \\geq \\bm{0} or in a box where many efficient algorithms are available.\n\nSteepest descent projection, interior ellipsoidal-trust region methods, etc."
  },
  {
    "objectID": "09_aug_lagrangian.html#the-lagrangian-method-dual-steepest-ascent",
    "href": "09_aug_lagrangian.html#the-lagrangian-method-dual-steepest-ascent",
    "title": "09_aug_lagrangian",
    "section": "The Lagrangian Method: Dual Steepest Ascent",
    "text": "The Lagrangian Method: Dual Steepest Ascent\n\nAccording to Lemma 1, the gradient of Ï•\\phi is available almost without cost once Ï•\\phi is evaluated.\n\nAny of the standard algorithms discussed for unconstrained optimization can be used for solving the unconstrained Lagrangian problem to evaluate the dual gradient vector.\nThe iterative scheme is simply, starting from any initial pairs (ğ±0,ğ›Œ0,ğ›0(â‰¥ğŸ))\\left(\\bm{x}_0, \\bm{\\lambda}_0, \\bm{\\mu}_0(\\geq \\bm{0})\\right),\n\n\nğ±k+1:=argminğ±â„“(ğ±,ğ›Œk,ğ›ğ¤),ğ›Œk+1:=ğ›Œkâˆ’1cğ¡(ğ±k+1),ğ›k+1:=max{ğŸ,ğ›kâˆ’1cğ (ğ±k+1)}.\n\\begin{align}\n\\bm{x}_{k+1} &:= \\operatorname{arg}\\,\\operatorname{min}_\\bm{x} \\ell(\\bm{x}, \\bm{\\lambda}_k, \\bm{\\mu_k}), \\\\\n\\bm{\\lambda}_{k+1} &:= \\bm{\\lambda}_k - \\frac{1}{c}\\bm{h}(\\bm{x}_{k+1}), \\\\\n\\bm{\\mu}_{k+1} &:= \\operatorname{max} \\left\\{\\bm{0}, \\, \\bm{\\mu}_k - \\frac{1}{c}\\bm{g}(\\bm{x}_{k+1}) \\right\\}.\n\\end{align}\n\nHere, cc is the first-order Lipschitz constant of the dual function Ï•(ğ›Œ,ğ›)\\phi(\\bm{\\lambda}, \\bm{\\mu}).\n\nWithout some special properties, however, the method as a whole can be costly to execute.\n\nEvery evaluation of Ï•\\phi requires the solution of an unconstrained problem in the unknown ğ±\\bm{x}.\n\nConvergence speed: identical to those discussed for solving unconstrained problems.\n\nIf the dual objective is strongly concave, the convergence rate is governed by the eigenvalue structure of the Hessian of the dual function Ï•\\phi: ğš½=âˆ’âˆ‡ğ¡(ğ±*)(ğ‹*)âˆ’1âˆ‡ğ¡(ğ±*)âŠ¤\\bm{\\Phi} = -\\nabla \\bm{h}(\\bm{x}^\\ast)\\left(\\bm{L}^\\ast\\right)^{-1}\\nabla \\bm{h}(\\bm{x}^\\ast)^\\top.\nThe rate of convergence is (Bâˆ’b)2(B+b)2\\frac{(B-b)^2}{(B+b)^2}, where BB and bb are the largest and smallest eigenvalues of ğš½\\bm{\\Phi}."
  },
  {
    "objectID": "09_aug_lagrangian.html#the-augmented-lagrangian",
    "href": "09_aug_lagrangian.html#the-augmented-lagrangian",
    "title": "09_aug_lagrangian",
    "section": "The Augmented Lagrangian",
    "text": "The Augmented Lagrangian\n\nThese methods can be veiwed as a combination of penalty functions and local duality methods.\n\nThe two concepts work together to eliminate many of the disadvantages associated with either method alone.\n\nThe augmented Lagrangian for the equality constrained problem is the function â„“c(ğ±,ğ›Œ)=f(ğ±)âˆ’ğ›ŒâŠ¤ğ¡(ğ±)+c2|ğ¡(ğ±)|2 \\ell_c(\\bm{x}, \\bm{\\lambda}) = f(\\bm{x}) - \\bm{\\lambda}^\\top \\bm{h}(\\bm{x}) + \\frac{c}{2}\\left|\\bm{h}(\\bm{x})\\right|^2  for some positive constant cc.\nFrom a penalty function viewpoint, the augmented Lagrangian, for a fixed value of the vector ğ›Œ\\bm{\\lambda} is simply the Lagrange penalty function for the problem minimizef(ğ±)+12c|ğ¡(ğ±)|2,subject toğ¡(ğ±)=ğŸ,ğ±âˆˆÎ©\n\\begin{align}\n\\operatorname{minimize} & f(\\bm{x}) + \\frac{1}{2}c\\left|\\bm{h}(\\bm{x})\\right|^2, \\\\\n\\text{subject to} & \\bm{h}(\\bm{x}) = \\bm{0}, \\quad \\bm{x} \\in \\Omega\n\\end{align}\n\nThis problem is clearly equivalent to the original equality cosntrained problem since the combinations of the constraints adjoined to f(ğ±)f(\\bm{x}) do not affect the minimum point or the minimum value.\nA typical step of an augmented Lagrangian method starts with a vector ğ›Œk\\bm{\\lambda}_k. Then ğ±(ğ›Œk)\\bm{x}(\\bm{\\lambda}_k) is found as the minimum point of ğ±(ğ›Œk)=argminf(ğ±)âˆ’ğ›ŒkâŠ¤ğ¡(ğ±)+12c|ğ¡(ğ±)|2,subject toğ±âˆˆÎ©.\n\\bm{x}(\\bm{\\lambda}_k) = \\operatorname{arg} \\operatorname{min} f(\\bm{x}) - \n\\bm{\\lambda}_k^\\top \\bm{h}(\\bm{x}) + \\frac{1}{2}c\\left|\\bm{h}(\\bm{x})\\right|^2, \n\\quad \\text{subject to} \\quad \\bm{x} \\in \\Omega.\n\nNext, ğ›Œk\\bm{\\lambda}_k is updated to ğ›Œk+1\\bm{\\lambda}_{k+1}: ğ›Œk+1=ğ›Œkâˆ’cğ¡(ğ±(ğ›Œk)).\\bm{\\lambda}_{k+1} = \\bm{\\lambda}_k - c\\bm{h}(\\bm{x}(\\bm{\\lambda}_k))."
  },
  {
    "objectID": "09_aug_lagrangian.html#the-augmented-lagrangian-1",
    "href": "09_aug_lagrangian.html#the-augmented-lagrangian-1",
    "title": "09_aug_lagrangian",
    "section": "The Augmented Lagrangian",
    "text": "The Augmented Lagrangian\n\nWhereas the original Lagrangian may not be convex near the solution, and hence the standard duality method cannot be applied, the term 12c|ğ¡(ğ±)|2\\frac{1}{2}c\\left|\\bm{h}(\\bm{x})\\right|^2 tends to â€œconvexifyâ€ the Lagrangian.\n\nFor sufficiently large cc, the Lagrangian will indeed be locally convex.\nThus, the duality method can be employed, and the corresponding dual problem can be solved by an iterative process in ğ›Œ\\bm{\\lambda}.\nThis viewpoint leads to the development of additional multiplier adjustment processes.\n\nThe main iteration in augmented Lagrangian methods is with respect to ğ›Œ\\bm{\\lambda}.\n\nThe penalty parameter cc may also be adjusted during the process!\nAs in ordinary penalty function methods, the sequence of ccâ€™s is usually preselected;\n\ncc is either held fixed,\nis increased toward a finite value,\nor tends (slowly) toward infinity.\n\nIn this method, it is not necessary for cc to go to infinity.\n\nIn fact, it may remain of relatively modest value.\nThe ill-conditioning usually associated with the penalty function approach is mediated."
  },
  {
    "objectID": "09_aug_lagrangian.html#the-penalty-viewpoint",
    "href": "09_aug_lagrangian.html#the-penalty-viewpoint",
    "title": "09_aug_lagrangian",
    "section": "The Penalty Viewpoint",
    "text": "The Penalty Viewpoint\n\n\n\nLemma\n\n\nLet ğ€\\bm{A} and ğ\\bm{B} be nn-by-nn symmetric matrices. Suppose that ğ\\bm{B} is positive semidefinite and ğ€\\bm{A} is positive definite on the subspace ğğ±=ğŸ\\bm{Bx} = \\bm{0}. Then there is a c*c^\\ast such that for all câ‰¥c*c \\geq c^\\ast the matrix ğ€+cğ\\bm{A} + c\\bm{B} is positive definite.\n\n\n\n\n\n\nProof\n\n\nSuppose to the contrary that for every kk there were an ğ±k\\bm{x}_k with |ğ±k|=1|\\bm{x}_k| = 1 such that ğ±kâŠ¤(ğ€+kğ)ğ±kâ‰¤0\\bm{x}_k^\\top (\\bm{A} + k\\bm{B})\\bm{x}_k \\leq 0. The sequence {ğ±k}\\left\\{\\bm{x}_k\\right\\} must have a convergent subsequence converging to a limit ğ±â€¾\\bar{\\bm{x}}. Now since ğ±kâŠ¤ğğ±kâ‰¥0\\bm{x}_k^\\top \\bm{B} \\bm{x}_k \\geq 0, it follows that ğ±â€¾âŠ¤ğğ±â€¾=0\\bar{\\bm{x}}^\\top \\bm{B} \\bar{\\bm{x}} = 0. It also follows that ğ±â€¾âŠ¤ğ€ğ±â€¾â‰¤0\\bar{\\bm{x}}^\\top \\bm{A} \\bar{\\bm{x}} \\leq 0. However, this contradicts the hypothesis of the lemma.\n\n\n\n\nThis lemma applies to the Hessian of the augmented Lagrangian, evaluated at the optimal solution pair ğ±*\\bm{x}^\\ast, ğ›Œ*\\bm{\\lambda}^\\ast. ğ‹c(ğ±*,ğ›Œ*)=ğ…(ğ±*)âˆ’(ğ›Œ*)âŠ¤ğ‡(ğ±*)+câˆ‡ğ¡(ğ±*)âŠ¤ğ¡(ğ±*)=ğ‹(ğ±*)+câˆ‡ğ¡(ğ±*)âŠ¤âˆ‡ğ¡(ğ±*).\n\\begin{align}\n\\bm{L}_c(\\bm{x}^\\ast, \\bm{\\lambda}^\\ast) = \\bm{F}(\\bm{x}^\\ast) - \n\\left(\\bm{\\lambda}^\\ast\\right)^\\top\\bm{H}(\\bm{x}^\\ast) + \nc\\nabla \\bm{h}(\\bm{x}^\\ast)^\\top \\bm{h}(\\bm{x}^\\ast) = \\bm{L}(\\bm{x}^\\ast) + \nc\\nabla \\bm{h}(\\bm{x}^\\ast)^\\top \\nabla \\bm{h}(\\bm{x}^\\ast).\n\\end{align}\n\n\nThe first term, the Hessian of the normal Lagrangian, is positive definite on the subspece âˆ‡ğ¡(ğ±*)=ğŸ\\nabla \\bm{h}(\\bm{x}^\\ast) = \\bm{0}. This corresponds to the matrix ğ€\\bm{A} in the lemma.\nThe matrix âˆ‡ğ¡(ğ±*)âŠ¤âˆ‡ğ¡(ğ±*)\\nabla \\bm{h}(\\bm{x}^\\ast)^\\top \\nabla \\bm{h}(\\bm{x}^\\ast) is positive semidefinite and corresponds to ğ\\bm{B} in the lemma.\n\nIt follows that there is a c*c^\\ast such that for all c&gt;c*c &gt; c^\\ast, ğ‹c(ğ±*,ğ›Œ*)\\bm{L}_c(\\bm{x}^\\ast, \\bm{\\lambda}^\\ast) is positive definite.\n\n\nThis leads directly to the first basic result concerning augmented Lagrangian."
  },
  {
    "objectID": "09_aug_lagrangian.html#the-penalty-viewpoint-1",
    "href": "09_aug_lagrangian.html#the-penalty-viewpoint-1",
    "title": "09_aug_lagrangian",
    "section": "The Penalty Viewpoint",
    "text": "The Penalty Viewpoint\n\n\n\nProposition\n\n\nAssume that the second-order sufficiency conditions for a local minimum are satisfied at ğ±*\\bm{x}^\\ast, ğ›Œ*\\bm{\\lambda}^\\ast. Then there is a c*c^\\ast such that for all câ‰¥c*c \\geq c^\\ast, the augmented Lagrangian â„“c(ğ±,ğ›Œ*)\\ell_c(\\bm{x}, \\bm{\\lambda}^\\ast) has a local minimum point at ğ±*\\bm{x}^\\ast.\n\n\n\n\nBy continuity, for any ğ›Œ\\bm{\\lambda} near ğ›Œ*\\bm{\\lambda}^\\ast, the augmented Lagrangian has a unique local minimum point near ğ±*\\bm{x}^\\ast.\nThis correspondence defines a continuous function.\n\nIf a value of ğ›Œ\\bm{\\lambda} can be found such that ğ¡(ğ±(ğ›Œ))=ğŸ\\bm{h}(\\bm{x}(\\bm{\\lambda})) = \\bm{0}, then that ğ›Œ\\bm{\\lambda} must in fact be ğ›Œ*\\bm{\\lambda}^\\ast.\n\nThis is because ğ±(ğ›Œ)\\bm{x}(\\bm{\\lambda}) satisfies the necessary conditions of the original problem.\n\n\nTherefore, the problem of determining the proper value of ğ›Œ\\bm{\\lambda} can be viewed as one of solving the equation ğ¡(ğ±(ğ›Œ))=ğŸ. \\bm{h}(\\bm{x}(\\bm{\\lambda})) = \\bm{0}. \nFor this purpose the iterative process ğ›Œk+1=ğ›Œkâˆ’cğ¡(ğ±(ğ›Œk)), \\bm{\\lambda}_{k+1} = \\bm{\\lambda}_k - c \\bm{h}(\\bm{x}(\\bm{\\lambda}_k)),  is a method of successive approximation (such as fixed-point iteration).\n\nThis process will converge linearly in a neighborhood around ğ›Œ*\\bm{\\lambda}^\\ast although a rigorous proof is somewhat complex."
  },
  {
    "objectID": "09_aug_lagrangian.html#example-1",
    "href": "09_aug_lagrangian.html#example-1",
    "title": "09_aug_lagrangian",
    "section": "Example",
    "text": "Example\nminimize2x2+2xy+y2âˆ’2y,subject tox=0.\n\\begin{align}\n\\operatorname{minimize} & 2x^2 + 2xy + y^2 - 2y, \\\\\n\\text{subject to} & x = 0.\n\\end{align}\n\n\nThe augmented Lagrangian for this problem is\n\nâ„“c(x,y,Î»)=2x2+2xy+y2âˆ’2yâˆ’Î»x+12cx2. \\ell_c(x, y, \\lambda) = 2x^2 + 2xy + y^2 - 2y - \\lambda x + \\frac{1}{2}cx^2. \n\nThe minimum can be found analytically to be\n\nx=âˆ’(2âˆ’Î»)2+c,y=4+câˆ’Î»2+c.\nx = \\frac{-(2-\\lambda)}{2+c}, \\quad y = \\frac{4+c-\\lambda}{2+c}.\n\n\nSince h(x,y)=xh(x, y) = x in this example, it follows that the iterative process for Î»k\\lambda_k is\n\nÎ»k+1=Î»k+c(2âˆ’Î»k)2+c=(22+c)Î»k+2c2+c.\n\\lambda_{k+1} = \\lambda_k + \\frac{c(2-\\lambda_k)}{2+c} = \\left(\\frac{2}{2+c}\\right)\\lambda_k + \\frac{2c}{2+c}.\n\n\nThis converges to Î»=2\\lambda = 2 for any c&gt;0c &gt; 0.\nThe coefficient 22+c\\frac{2}{2+c} governs the rate of convergence.\n\nThe rate improves as cc is increased."
  },
  {
    "objectID": "09_aug_lagrangian.html#geometric-interpretation",
    "href": "09_aug_lagrangian.html#geometric-interpretation",
    "title": "09_aug_lagrangian",
    "section": "Geometric Interpretation",
    "text": "Geometric Interpretation\n\n\n\nThe minimum of the augmented Lagrangian at step kk can be expressed in terms of the primal function as follows:\n\nminâ„“c(ğ±,ğ›Œk)=minğ±{f(ğ±)âˆ’ğ›ŒkâŠ¤ğ¡(ğ±)+12c|ğ¡(ğ±)|2}=minğ±,ğ²{f(ğ±)âˆ’ğ›ŒkâŠ¤ğ²+12c|ğ²|2:ğ¡(ğ±)=ğ²}=minğ²{Ï‰(ğ²)âˆ’ğ›ŒkâŠ¤ğ²+12c|ğ²|2},\n\\begin{align}\n\\operatorname{min} \\ell_c(\\bm{x}, \\bm{\\lambda}_k) &= \\operatorname{min}_\\bm{x} \n\\left\\{ f(\\bm{x}) - \\bm{\\lambda}_k^\\top \\bm{h}(\\bm{x}) + \\frac{1}{2}c\\left|\\bm{h}(\\bm{x})\\right|^2 \\right\\} \\\\\n&= \\operatorname{min}_{\\bm{x}, \\bm{y}} \\left\\{ f(\\bm{x}) - \\bm{\\lambda}_k^\\top \\bm{y} + \\frac{1}{2}c|\\bm{y}|^2: \\, \\bm{h}(\\bm{x}) = \\bm{y} \\right\\} \\\\\n&= \\operatorname{min}_\\bm{y} \\left\\{ \\omega(\\bm{y}) - \\bm{\\lambda}_k^\\top \\bm{y} + \\frac{1}{2}c|\\bm{y}|^2 \\right\\},\n\\end{align}\n\nwhere the minimization w.r.t. ğ²\\bm{y} is taken to be locally near ğ²=ğŸ\\bm{y} = \\bm{0}.\n\nIn general, if ğ±(ğ›Œğ¤)\\bm{x}(\\bm{\\lambda_k}) minimizes â„“c(ğ±,ğ›Œk)\\ell_c(\\bm{x}, \\bm{\\lambda}_k), then ğ²k=ğ¡(ğ±(ğ›Œğ¤))\\bm{y}_k = \\bm{h}(\\bm{x}(\\bm{\\lambda_k})) is the minimum of Ï‰(ğ²)âˆ’ğ›ŒkâŠ¤ğ²+12c|ğ²|2\\omega(\\bm{y}) - \\bm{\\lambda}_k^\\top \\bm{y} + \\frac{1}{2}c|\\bm{y}|^2.\n\nAt that point we have âˆ‡Ï‰(ğ²k)âŠ¤+cğ²k=ğ›Œkâˆ‡Ï‰(ğ²k)âŠ¤=ğ›Œkâˆ’cğ²k=ğ›Œkâˆ’cğ¡(ğ±(ğ²k)). \n\\begin{align} \n&\\nabla \\omega (\\bm{y}_k)^\\top + c\\bm{y}_k = \\bm{\\lambda}_k  \\\\\n&\\nabla \\omega(\\bm{y}_k)^\\top = \\bm{\\lambda}_k - c \\bm{y}_k = \\bm{\\lambda}_k - c \\bm{h}(\\bm{x}(\\bm{y}_k)). \n\\end{align}\n\n\nIt follows that for the next multiplier we have ğ›Œk+1=ğ›Œkâˆ’cğ¡(ğ±(ğ›Œk))=âˆ‡Ï‰(ğ²k)âŠ¤.\n\\bm{\\lambda}_{k+1} = \\bm{\\lambda}_k - c\\bm{h}(\\bm{x}(\\bm{\\lambda}_k)) = \\nabla \\omega (\\bm{y}_k)^\\top.\n\n\n\n\n\n\n\n\n\nPrimal function\n\n\nÏ‰(ğ²)â‰œmin{f(ğ±):ğ¡(ğ±)=ğ²}, \\omega(\\bm{y}) \\triangleq \\operatorname{min}\\left\\{ f(\\bm{x}): \\bm{h}(\\bm{x}) = \\bm{y} \\right\\},  where the minimum is understood to be taken locally near ğ±*\\bm{x}^\\ast.\n\nÏ‰(ğŸ)=f(ğ±*)\\omega(\\bm{0}) = f(\\bm{x}^\\ast).\nâˆ‡Ï‰(ğŸ)âŠ¤=ğ›Œ*\\nabla \\omega(\\bm{0})^\\top = \\bm{\\lambda}^\\ast."
  },
  {
    "objectID": "09_aug_lagrangian.html#separable-problems",
    "href": "09_aug_lagrangian.html#separable-problems",
    "title": "09_aug_lagrangian",
    "section": "Separable Problems",
    "text": "Separable Problems\n\nA structure that arises frequently in mathematical programming applications is that of the separable problem:\n\nminimizeâˆ‘i=1qfi(ğ±i),subject toâˆ‘i=1qğ¡i(ğ±i)=ğŸ,âˆ‘i=1qğ i(ğ±i)â‰¥ğŸ.(10)\n\\begin{align}\n\\operatorname{minimize} & \\sum_{i=1}^q f_i(\\bm{x}_i), \\\\\n\\text{subject to} & \\sum_{i=1}^q \\bm{h}_i(\\bm{x}_i) = \\bm{0}, \\\\\n& \\sum_{i=1}^q \\bm{g}_i(\\bm{x}_i) \\geq \\bm{0}.\n\\end{align}\n \\qquad(10)\n\nIn this formulation, the components of the nn-vector ğ±\\bm{x} are partitioned into qq disjoint groups, ğ±=(ğ±1,ğ±2,â€¦,ğ±q)\\bm{x} = \\left(\\bm{x}_1, \\bm{x}_2, \\ldots, \\bm{x}_q \\right).\n\nEach group may or may not have the same number of components.\n\n\n\n\n\nExample\n\n\nProblems involving a series of decision made at distinct times are often separable. maximizey(t),u(t)âˆ‘t=1Tf(y(t),u(t)),subject toy(t)=y(tâˆ’1)âˆ’u(t)+s(t),t=1,â€¦,T,câ‰¤y(t)â‰¤d,t=1,â€¦,T,0â‰¤u(t),t=1,â€¦,T.\n\\begin{align}\n\\operatorname{maximize}_{y(t), u(t)} & \\sum_{t=1}^T f(y(t), u(t)), & \\\\\n\\text{subject to} & y(t) = y(t-1) - u(t) + s(t), & t = 1, \\ldots, T, \\\\\n& c \\leq y(t) \\leq d, & t = 1, \\ldots, T, \\\\\n& 0 \\leq u(t), & t = 1, \\ldots, T.\n\\end{align}\n Here, the state variable y(t)y(t) represents the water volume behind the dam at the end of period tt, control variable u(t)u(t) represents the volume flow through the dam during period tt, and data s(t)s(t) is the volume flowing into the lake behind the dam during period tt from upper streams. The function ff gives the power generation, and cc and dd are bounds on lake volume.\nPartition into pairs: ğ±t=(y(t),u(t))\\bm{x}_t = (y(t), u(t)), t=1,â€¦,Tt = 1, \\ldots, T."
  },
  {
    "objectID": "09_aug_lagrangian.html#decomposition",
    "href": "09_aug_lagrangian.html#decomposition",
    "title": "09_aug_lagrangian",
    "section": "Decomposition",
    "text": "Decomposition\n\nSeparable problems are ideally suited to dual methods, because the required unconstrained minimization decomposes into small subproblems.\n\nGenerally, the most difficult aspect of a dual method is the evaluation of the dual function.\nFor a separable problem, if we associate ğ›Œ\\bm{\\lambda} with the equality constraints and ğ›â‰¥ğŸ\\bm{\\mu} \\geq \\bm{0} with the inequality constraints of EquationÂ 10, the required dual function is Ï•(ğ›Œ,ğ›)=minâˆ‘i=1q(fi(ğ±i)âˆ’ğ›ŒâŠ¤ğ¡i(ğ±i)âˆ’ğ›âŠ¤ğ i(ğ±i)).\n\\phi(\\bm{\\lambda}, \\bm{\\mu}) = \\operatorname{min} \\sum_{i=1}^q \\left(\nf_i(\\bm{x}_i) - \\bm{\\lambda}^\\top \\bm{h}_i(\\bm{x}_i) - \\bm{\\mu}^\\top \\bm{g}_i(\\bm{x}_i)\n\\right).\n\n\nThis minimization decomposes into the qq separate problems minğ±ifi(ğ±i)âˆ’ğ›ŒâŠ¤ğ¡i(ğ±i)âˆ’ğ›âŠ¤ğ i(ğ±i).\n\\operatorname{min}_{\\bm{x}_i} f_i(\\bm{x}_i) - \\bm{\\lambda}^\\top \\bm{h}_i(\\bm{x}_i) - \\bm{\\mu}^\\top \\bm{g}_i(\\bm{x}_i).\n\nThe solution of these subproblems can usually be accomplished relatively efficiently, since they are of smaller dimension than the original problem.\n\n\n\n\nExample\n\n\nusing duality with respect to the equality constraints, we denote the dual variables by Î»(t)\\lambda(t), t=1,â€¦,Tt = 1, \\ldots, T. The ttht^{\\text{th}} subproblem becomes maxcâ‰¤y(t)â‰¤d,0â‰¤u(t){f(y(t),u(t))+[Î»(t+1)âˆ’Î»(t)]y(t)âˆ’Î»(t)[u(t)âˆ’s(t)]}\n\\operatorname{max}_{c \\leq y(t) \\leq d, 0 \\leq u(t)} \\left\\{ \nf(y(t), u(t)) + \\left[ \\lambda(t+1) - \\lambda(t) \\right]y(t) - \\lambda(t) \\left[u(t) - s(t) \\right]\n\\right\\}\n which is a two-dimensional optimization problem.\n\nThe selection of Î»âˆˆâ„N\\lambda \\in \\mathbb{R}^N decomposes the problem into separate problems for each time period."
  },
  {
    "objectID": "09_aug_lagrangian.html#problem-set-up",
    "href": "09_aug_lagrangian.html#problem-set-up",
    "title": "09_aug_lagrangian",
    "section": "Problem Set-Up",
    "text": "Problem Set-Up\n\nConsider the convex minimization model with linear/affine constraints and an objective function that is the sum of two seaparable functions with two blocks of variables:\n\nminimizef1(ğ±1)+f2(ğ±2),fi:â„niâ†’â„,subject toğ€1ğ±1+ğ€2ğ±2=ğ›,ğ›âˆˆâ„mğ±1âˆˆÎ©1,ğ±2âˆˆÎ©2Î©iâŠ†â„ni(11)\n\\begin{align}\n\\operatorname{minimize} & f_1(\\bm{x}^1) + f_2(\\bm{x}^2), &  f_i: \\mathbb{R}^{n_i} \\rightarrow \\mathbb{R}, \\\\\n\\text{subject to} & \\bm{A}_1 \\bm{x}^1 + \\bm{A}_2 \\bm{x}^2 = \\bm{b}, & \\bm{b} \\in \\mathbb{R}^m \\\\\n& \\bm{x}^1 \\in \\Omega_1, \\; \\bm{x}^2 \\in \\Omega_2 & \\Omega_i \\subseteq \\mathbb{R}^{n_i}\n\\end{align} \n \\qquad(11)\n\nThen, the augmented Lagrangian function for EquationÂ 11 would be â„“c(ğ±1,ğ±2,ğ›Œ)=f1(ğ±1)+f2(ğ±2)âˆ’ğ›ŒâŠ¤(ğ€1ğ±1+ğ€2ğ±2âˆ’ğ›)+c2|ğ€1ğ±1+ğ€2ğ±2âˆ’ğ›|2.\n\\ell_c(\\bm{x}^1, \\bm{x}^2, \\bm{\\lambda}) = f_1(\\bm{x}^1) + f_2(\\bm{x}^2) - \\bm{\\lambda}^\\top\n\\left(\\bm{A}_1\\bm{x}^1 + \\bm{A}_2\\bm{x}^2 - \\bm{b} \\right) + \\frac{c}{2}\n\\left| \\bm{A}_1\\bm{x}^1 + \\bm{A}_2\\bm{x}^2 - \\bm{b} \\right|^2.\n\nIn contrast to the method of multipliers that we previously covered, the alternating direction method of multipliers (ADMM) is to (approximately) minimize â„“c(ğ±1,ğ±2,ğ›Œ)\\ell_c(\\bm{x}^1, \\bm{x}^2, \\bm{\\lambda}) in an alternative order:\n\nğ±k+11:=argminğ±1âˆˆÎ©1â„“c(ğ±1,ğ±k2,ğ›Œk),ğ±k+12:=argminğ±2âˆˆÎ©2â„“c(ğ±k+11,ğ±2,ğ›Œk),ğ›Œk+1:=ğ›Œkâˆ’c(ğ€1ğ±k+11+ğ€2ğ±k+12âˆ’ğ›).\n\\begin{align}\n\\bm{x}_{k+1}^1 &:= \\operatorname{arg}\\operatorname{min}_{\\bm{x}^1 \\in \\Omega_1} \n\\ell_c(\\bm{x}^1, \\bm{x}_k^2, \\bm{\\lambda}_k), \\\\\n\\bm{x}_{k+1}^2 &:= \\operatorname{arg}\\operatorname{min}_{\\bm{x}^2 \\in \\Omega_2} \n\\ell_c(\\bm{x}_{k+1}^1, \\bm{x}^2, \\bm{\\lambda}_k), \\\\\n\\bm{\\lambda}_{k+1} &:= \\bm{\\lambda}_k - c\\left( \\bm{A}_1\\bm{x}_{k+1}^1 + \\bm{A}_2\\bm{x}_{k+1}^2 - \\bm{b} \\right).\n\\end{align}\n\n\nThe idea is that each of the smaller-block minimization problems can be solved more efficiently or even in closed-forms for certain cases.\n\n\n\nOptimization Theory and Practice â€¢ Aykut C. Satici"
  }
]