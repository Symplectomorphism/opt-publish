[
  {
    "objectID": "02_basic_lp.html#optimization-theory-and-practice",
    "href": "02_basic_lp.html#optimization-theory-and-practice",
    "title": "02_basic_lp",
    "section": "Optimization Theory and Practice",
    "text": "Optimization Theory and Practice\n\n\nBasic Properties of Linear Programs\n\n\n\n\nInstructor: Aykut Satici, Ph.D.Â   Mechanical and Biomedical Engineering  Electrical and Computer Engineering  Boise State University, Boise, ID, USA\n\n\nTopics:  Introduction  Examples  Basic Feasible Solutions  Fundamental Theorem  Convex Geometry  Farkasâ€™s Lemma"
  },
  {
    "objectID": "02_basic_lp.html#standard-form",
    "href": "02_basic_lp.html#standard-form",
    "title": "02_basic_lp",
    "section": "Standard Form",
    "text": "Standard Form\n\n\n\n\n\n\nLinear Program (LP)\n\n\nAn LP is an optimization problem in which the objective function is linear in the unknowns and the constraints consist of linear (in)equalities.\n\n\n\n\n\n\n\n\n\nStandard form\n\n\nminimizeğœâŠ¤ğ±=c1x1+c2x2+â‹¯+cnxnsubject toğ€ğ±=ğ›andğ±â‰¥ğŸ.\n\\begin{align}\n\\operatorname{minimize} & \\bm{c}^\\top \\bm{x} = c_1x_1 + c_2x_2 + \\cdots + c_nx_n\n\\\\\n\\text{subject to} & \\bm{A}\\bm{x} = \\bm{b} \\quad \\text{and} \\quad \\bm{x} \\geq\n\\bm{0}.\n\\end{align}\n\n\nğœ,ğ±âˆˆâ„n\\bm{c}, \\bm{x} \\in \\mathbb{R}^n are column vectors, ğ€âˆˆâ„mÃ—n\\bm{A} \\in \\mathbb{R}^{m \\times n} a fat matrix (m&lt;nm &lt; n), ğ›âˆˆâ„m\\bm{b} \\in \\mathbb{R}^m a column vector.\nbib_iâ€™s, cic_iâ€™s and aija_{ij}â€™s are fixed real constants, and the xix_iâ€™s are real numbers to be determined.\nWe assume that each equation has been multiplied by minus unity, if necessary, so that each biâ‰¥0b_i \\geq 0."
  },
  {
    "objectID": "02_basic_lp.html#slack-variables",
    "href": "02_basic_lp.html#slack-variables",
    "title": "02_basic_lp",
    "section": "Slack Variables",
    "text": "Slack Variables\n\n\n\n\n\nmaximizec1x2+c2x2+â‹¯+cnxnsubject toa11x1+a12x2+â‹¯+a1nxnâ‰¤b1,a21x1+a22x2+â‹¯+a2nxnâ‰¤b2,â‹®â‹®am1x1+am2x2+â‹¯+amnxnâ‰¤bm,x1,x2,â€¦,xnâ‰¥0.\n\\begin{align}\n\\operatorname{maximize} & c_1x_2 + c_2x_2 + \\cdots + c_nx_n \\\\\n\\text{subject to} & a_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n &\\leq b_1, \\\\\n& a_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n &\\leq b_2, \\\\\n& \\vdots & \\vdots \\\\\n& a_{m1}x_1 + a_{m2}x_2 + \\cdots + a_{mn}x_n &\\leq b_m, \\\\\n& x_1, x_2, \\ldots, x_n \\geq 0.\n\\end{align}\n\n\n\n\nThis problem may be alternatively expressed as\n\n\n\nminimizeâˆ’c1x2âˆ’c2x2âˆ’â‹¯âˆ’cnxnsubject toa11x1+a12x2+â‹¯+a1nxn+xn+1=b1,a21x1+a22x2+â‹¯+a2nxn12+xn+2=b2,â‹®â‹®am1x1+am2x2+â‹¯+amnxn12345+xn+m=bm,x1,x2,â€¦,xn+1,â€¦,xn+mâ‰¥0.\n\\begin{align}\n\\operatorname{minimize} & -c_1x_2 - c_2x_2 - \\cdots - c_nx_n \\\\\n\\text{subject to} & a_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n + x_{n+1} &= b_1, \\\\\n& a_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n \\phantom{12} + x_{n+2} &= b_2, \\\\\n& \\vdots & \\vdots \\\\\n& a_{m1}x_1 + a_{m2}x_2 + \\cdots + a_{mn}x_n \\phantom{12345} + x_{n+m} &= b_m, \\\\\n& x_1, x_2, \\ldots, x_{n+1}, \\ldots, x_{n+m} \\geq 0.\n\\end{align}\n\n\n\n\n\n\nThe new nonnegative variables xn+ix_{n+i}, i=1,â€¦,mi=1, \\ldots, m convert inequalities to equalities are called slack variables.\n\nÂ \n\nThe constrant matrix now is transformed to ğ€â†’[ğ€ğˆ]\\bm{A} \\rightarrow \\begin{bmatrix} \\bm{A} & \\bm{I} \\end{bmatrix}."
  },
  {
    "objectID": "02_basic_lp.html#surplus-variables",
    "href": "02_basic_lp.html#surplus-variables",
    "title": "02_basic_lp",
    "section": "Surplus Variables",
    "text": "Surplus Variables\n\nIf the linear equalities are reversed so that a typical inequality is\n\nai1x1+ai2x2+â‹¯+ainxnâ‰¥bi, a_{i1}x_1 + a_{i2}x_2 + \\cdots + a_{in} x_n \\geq b_i, \nit is clear that this is equivalent to\nai1x1+ai2x2+â‹¯+ainxnâˆ’yi=bi, a_{i1}x_1 + a_{i2}x_2 + \\cdots + a_{in} x_n - y_i = b_i, \nwith yiâ‰¥0y_i \\geq 0.\n\nVariables, such as yiy_i, adjoined in this fashion to convert a â€œgreater than or equal toâ€ inequality to equality are called surplus variables.\nBy suitably multiplying by minus unity, and adjoining slack and surplus variables, any set of linear inequalities can be converted to standard form."
  },
  {
    "objectID": "02_basic_lp.html#free-variables-first-method",
    "href": "02_basic_lp.html#free-variables-first-method",
    "title": "02_basic_lp",
    "section": "Free variables â€“ first method",
    "text": "Free variables â€“ first method\n\n\n\nSuppose one or more of the unknown variables is not required to be nonnegative, say, x1â‰¥0x_1 \\geq 0 is not present so that x1x_1 is free to take on either positive or negative values.\n\n\n\n\nWe then write x1=u1âˆ’v1x_1 = u_1 - v_1, and require that u1,v1â‰¥0u_1, v_1 \\geq 0.\nWe substitute u1âˆ’v1u_1 - v_1 for x1x_1 everwhere.\nThe problem is then expressed in terms of the n+1n+1 variables u1,v1,x2,x3,â€¦,xnu_1, v_1, x_2, x_3, \\ldots, x_n.\nThere is a certaind egree of redundancy introduced in this technique since a constant added to u1u_1 and v1v_1 does not change x1x_1.\nNevertheless, the simplex method can still be used to find the solution."
  },
  {
    "objectID": "02_basic_lp.html#free-variables-second-method",
    "href": "02_basic_lp.html#free-variables-second-method",
    "title": "02_basic_lp",
    "section": "Free variables â€“ second method",
    "text": "Free variables â€“ second method\n\nTake the same free variable situation, i.e, x1x_1 is free to take on positive or negative values.\nOne can take any one of the mm equality constraints which has a nonzero coefficient for x1x_1, say, for example,\n\nai1x1+ai2x2+â‹¯+ainxn=bi, a_{i1}x_1 + a_{i2}x_2 + \\cdots + a_{in}x_n = b_i, \nwhere ai1â‰ 0a_{i1} \\neq 0.\n\nThen x1x_1 can be expressed as a linear combination of the other variables, plus a constant.\nThis expression can be substituted everywhere for x1x_1 and we are led to a new problem of exacly the same form but expressed in terms of the variables x2,x3,â€¦,xnx_2, x_3, \\ldots, x_n only.\nFurthermore, the ithi^{\\text{th}} equation, used to determine x1x_1 is now identically zero and it too can be eliminated.\nWe obtain a linear program having n1n_1 variables and mâˆ’1m-1 constraint equations."
  },
  {
    "objectID": "02_basic_lp.html#example-specific-case",
    "href": "02_basic_lp.html#example-specific-case",
    "title": "02_basic_lp",
    "section": "Example â€“ specific case",
    "text": "Example â€“ specific case\n\n\n\n\n\nminimizex1+3x2+4x3subject tox1+2x2+3x3=5,2x1+3x2+x3=6,x2,x3â‰¥0.\n\\begin{align}\n\\operatorname{minimize} & x_1 + 3x_2 + 4x_3 \\\\\n\\text{subject to} & x_1 + 2x_2 + 3x_3 = 5, \\\\\n& 2x_1 + 3x_2 + x_3 = 6, \\\\\n& x_2, x_3 \\geq 0.\n\\end{align}\n\n\n\n\n\nx1x_1 is free, so we can solve for it from the first constraint, obtaining\nx1=5âˆ’2x2âˆ’x3.(1) x_1 = 5 - 2x_2 - x_3. \\qquad(1)\n\n\nSubstituting this into the objective and the second constraint, we obtain the equivalent problem\nminimizex2+3x3subject tox2+x3=4,x2,x3â‰¥0.\n\\begin{align}\n\\operatorname{minimize} & x_2 + 3x_3 \\\\\n\\text{subject to}& x_2 + x_3 = 4, \\\\\n& x_2, x_3 \\geq 0.\n\\end{align}\n\nwhich is a problem in standard form.\nAfter the smaller problem is solved (what is the answer?) the value for x1=âˆ’3x_1 = -3 can be found from EquationÂ 1."
  },
  {
    "objectID": "02_basic_lp.html#example-1-the-diet-problem",
    "href": "02_basic_lp.html#example-1-the-diet-problem",
    "title": "02_basic_lp",
    "section": "Example 1 â€“ The Diet Problem",
    "text": "Example 1 â€“ The Diet Problem\n\n\n\nDetermine the most economical die that satisfies the basic minimum nutritional requirements for good health\n\n\n\n\n\n\n\n\n\nThere are available nn different foods.\nThere are mm basic nutritional ingredients,\nEach unit of food jj contains aija_{ij} units of the ithi^{\\text{th}} nutrient.\n\n\n\njthj^{\\text{th}} food sells at a price cjc_j per unit.\nEach individual must receive at least bib_i units of the ithi^{\\text{th}} nutrient per day.\n\n\n\n\nIf we denote by xjx_j the number of units of food jj in the diet, the problem is to select xjx_jâ€™s to minimize the total cost c1x1+c2x2+â‹¯+cnxn c_1x_1 + c_2x_2 + \\cdots + c_nx_n \n\n\nsubject to the nutritional constraints ai1x1+ai2x2+â‹¯+ainxnâ‰¥bi,i=1,â€¦,m, a_{i1}x_1 + a_{i2}x_2 + \\cdots + a_{in}x_n \\geq b_i, \\; i=1, \\ldots, m, \n\n\nand the nonnegative constraints x1â‰¥0,x2â‰¥0,â€¦,xnâ‰¥0, x_1 \\geq 0, x_2 \\geq 0, \\ldots, x_n \\geq 0,  on the food quantities.\n\n\n\n\n\nThis problem can be converted to standard form by subtrating a nonnegative surplus variable from the left side of each of the mm linear inequalities."
  },
  {
    "objectID": "02_basic_lp.html#example-2-the-resource-allocation-problem",
    "href": "02_basic_lp.html#example-2-the-resource-allocation-problem",
    "title": "02_basic_lp",
    "section": "Example 2â€“ The Resource-Allocation Problem",
    "text": "Example 2â€“ The Resource-Allocation Problem\n\n\n\nA facility is capable of manufacturing nn different products.\nEach product can be produced at any level xjâ‰¥0x_j \\geq 0, j=1,2,â€¦,nj=1, 2, \\ldots, n.\nEach unit of the jthj^{\\text{th}} product needs aija_{ij} units of the ithi^{\\text{th}} resource, i=1,2,â€¦,mi = 1, 2, \\ldots, m.\n\n\n\nEach product may require various amounts of mm different resources.\nEach unit of the jthj^{\\text{th}} product can sell for Ï€j\\pi_j dollars.\nEach bib_i, i=1,2,â€¦,mi = 1, 2, \\ldots, m describe the available quantities of the mm resources.\n\n\n\n\nWe wish to manufacture products at maximum revenue\nmaximizeÏ€1x1+Ï€2x2+â‹¯+Ï€nxn\n\\begin{align}\n\\operatorname{maximize} & \\pi_1x_1 + \\pi_2x_2 + \\cdots + \\pi_nx_n\n\\end{align}\n\n\n\nsubject to the resource constraints\nsubject toai1x1+ai2x2+â‹¯+ainxnâ‰¤bi,i=1,â€¦,m\n\\begin{align}\n\\text{subject to} & a_{i1}x_1 + a_{i2}x_2 + \\cdots + a_{in}x_n \\leq b_i, \\; i=1,\n\\ldots, m\n\\end{align}\n\nand the nonnegativity consraints on all production variables.\n\n\n\nThe problem can also be interpreted as\n\nfund nn different activities, where\nÏ€j\\pi_j is the full reward from the jthj^{\\text{th}} activity,\nxjx_j is restricted to 0â‰¤xjâ‰¤10 \\leq x_j \\leq 1, representing the funding level from 0%0\\% to 100%100\\%."
  },
  {
    "objectID": "02_basic_lp.html#example-3-the-transportation-problem",
    "href": "02_basic_lp.html#example-3-the-transportation-problem",
    "title": "02_basic_lp",
    "section": "Example 3 â€“ The Transportation Problem",
    "text": "Example 3 â€“ The Transportation Problem\n\n\n\nQuantities a1,a2,â€¦,ama_1, a_2, \\ldots, a_m of a certain product are to be shipped from mm locations.\nShipping a unit of product from origin ii to destination jj costs cijc_{ij}.\n\n\n\nThese products will be received in amounts of b1,b2,â€¦,bnb_1, b_2, \\ldots, b_n at each of nn destinations.\nWe want to determine the amounts xijx_{ij} to be shipped between each origin-destination pair i=1,2,â€¦,mi = 1, 2, \\ldots, m; j=1,2,â€¦,nj=1, 2, \\ldots, n.\n\n\n\n\n\n\n\nx11x_{11}\nx12x_{12}\nâ‹¯\\cdots\nx1nx_{1n}\n|\na1a_1\n\n\nx21x_{21}\nx22x_{22}\nâ‹¯\\cdots\nx2nx_{2n}\n|\na2a_2\n\n\nâ‹®\\vdots\nâ‹®\\vdots\nâ‹®\\vdots\nâ‹®\\vdots\n|\nâ‹®\\vdots\n\n\nxm1x_{m1}\nxm2x_{m2}\nâ‹¯\\cdots\nxmnx_{mn}\n|\nama_m\n\n\nâ€”â€”\nâ€”â€”\nâ€”â€”\nâ€”â€”\n\n\n\n\nb1b_{1}\nb2b_{2}\nâ‹¯\\cdots\nbnb_{n}\n\n\n\n\n\n\n\nThe ithi^{\\text{th}} row in this array defines the variables assoc. w/ the ithi^{\\text{th}} origin.\nThe jthj^{\\text{th}} column defines the variables assoc. w/ the jthj^{\\text{th}} destination.\nProblem: find the nonnegative variables xijx_{ij} so that the sum across the ithi^{\\text{th}} row is aja_j, the sum down the jthj^{\\text{th}} column is bjb_j, and the weighted sum âˆ‘j=1nâˆ‘i=1mcijxij\\sum_{j=1}^n\\sum_{i=1}^m c_{ij}x_{ij} is minimized."
  },
  {
    "objectID": "02_basic_lp.html#example-4-the-maximal-flow-problem",
    "href": "02_basic_lp.html#example-4-the-maximal-flow-problem",
    "title": "02_basic_lp",
    "section": "Example 4 â€“ The Maximal Flow Problem",
    "text": "Example 4 â€“ The Maximal Flow Problem\n\n\n\n\n\nMaximal flow problem\n\n\nDetermine the maximal flow that can be established in such a network.\n\n\n\nmaximizefsubject toâˆ‘j=1nx1jâˆ’âˆ‘j=1nxj1âˆ’f=0,âˆ‘j=1nxijâˆ’âˆ‘j=1nxji=0,iâ‰ 1,m,âˆ‘j=1nxmjâˆ’âˆ‘j=1nxjm+f=0,0â‰¤xijâ‰¤kij,âˆ€i,j,\n\\begin{align}\n\\operatorname{maximize} & f \\\\\n\\text{subject to} & \\sum_{j=1}^n x_{1j} - \\sum_{j=1}^n x_{j1} - f = 0, \\\\\n& \\sum_{j=1}^n x_{ij} - \\sum_{j=1}^n x_{ji}  = 0, \\quad i \\neq 1, m, \\\\\n& \\sum_{j=1}^n x_{mj} - \\sum_{j=1}^n x_{jm} + f = 0, \\\\\n& 0 \\leq x_{ij} \\leq k_{ij}, \\quad \\forall i, j,\n\\end{align}\n\nwhere kij=0k_{ij} = 0 for those no-arc pairs (i,j)(i,j).\n\n\n\n\n\n\n\n\nCapacitated network in which two special nodes, called the source (node 1); and the sink (node mm) are distinguished.\nAll other nodes must satisfy the conservation requirement: net flow into these nodes must be zero.\n\nthe source may have a net outflow,\nthe sink may have a net inflow.\n\nThe outlow ff of the source will equal the inflow of the sink."
  },
  {
    "objectID": "02_basic_lp.html#example-5-a-supply-chain-problem",
    "href": "02_basic_lp.html#example-5-a-supply-chain-problem",
    "title": "02_basic_lp",
    "section": "Example 5 â€“ A Supply-Chain Problem",
    "text": "Example 5 â€“ A Supply-Chain Problem\n\n\n\nA warehouse is buying and selling stock of a certain commodity in order to maximize profit over a certain length of time.\n\n\n\n\n\n\nWarehouse has a fixed capacity CC.\nThe price, pip_i, of the commodity is known to fluctuate over a number of time periods, say months, indexed by ii.\nThe warehouse is originally empty and is required to be empty at the end of the last period.\n\n\n\nThere is a cost rr per unit of holding stock for one period.\nIn any period the same price holds for both purchase and sale.\nxix_i: level of stock in the warehouse at the beginning of period ii, uiu_i: amount bought during this period, sis_i: amount sold during this period.\n\n\n\nmaximizeâˆ‘i=1n(pi(siâˆ’ui)âˆ’rxi)subject toxi+1=xi+uiâˆ’si,i=1,2,â€¦,nâˆ’1,0=xn+unâˆ’sn,xi+zi=C,i=2,â€¦,n,x1=0,xiâ‰¥0,uiâ‰¥0,siâ‰¥0,ziâ‰¥0,\n\\begin{align}\n\\operatorname{maximize} & \\sum_{i=1}^n \\left(p_i(s_i - u_i) - rx_i \\right) \\\\\n\\text{subject to} & x_{i+1} = x_i + u_i - s_i, \\quad i = 1, 2, \\ldots, n-1, \\\\\n& 0 = x_n + u_n - s_n, \\\\\n& x_i + z_i = C, \\quad i = 2, \\ldots, n, \\\\\n& x_1 = 0, x_i \\geq 0, u_i \\geq 0, s_i \\geq 0, z_i \\geq 0,\n\\end{align}\n\nwhere ziz_i is a slack variable."
  },
  {
    "objectID": "02_basic_lp.html#example-6-linear-classifier-and-support-vector-machine",
    "href": "02_basic_lp.html#example-6-linear-classifier-and-support-vector-machine",
    "title": "02_basic_lp",
    "section": "Example 6 â€“ Linear Classifier and Support Vector Machine",
    "text": "Example 6 â€“ Linear Classifier and Support Vector Machine\n\n\n\n\n\ndd-dimensional data points are to be classified into two distinct classes.\n\n\n\n\nIn general, we have vectors ğšiâˆˆâ„d\\bm{a}_i \\in \\mathbb{R}^d for i=1,2,â€¦,n1i=1, 2, \\ldots, n_1 and vector ğ›jâˆˆâ„d\\bm{b}_j \\in \\mathbb{R}^d for j=1,2,â€¦,n2j = 1, 2, \\ldots, n_2.\nWe wish to find a hyperplane that separates ğši\\bm{a}_iâ€™s from the ğ›j\\bm{b}_jâ€™s, i.e., find a slope-vector yâˆˆâ„dy \\in \\mathbb{R}^d and an intercept Î²\\beta such that\n\nğšiâŠ¤ğ²+Î²â‰¥1,âˆ€i,ğ›jâŠ¤ğ²+Î²â‰¤1,âˆ€j,\n\\begin{align}\n  \\bm{a}_i^\\top \\bm{y} + \\beta &\\geq 1, \\quad \\forall i, \\\\\n  \\bm{b}_j^\\top \\bm{y} + \\beta &\\leq 1, \\quad \\forall j, \\\\\n\\end{align}\n\nwhere {ğ±:ğ±ğ²+Î²=0}\\{\\bm{x}: \\bm{x}^\\bm{y} + \\beta = 0\\} is the desired hyperplane.\n\nThe separation is defined by the fixed margins +1+1 and âˆ’1-1, which could be made soft or variable later.\n\n\n\n\n\n\n\n\nExample\n\n\n\nTwo-dimensional data points may be grade averages in science and humanities for different students.\nWe also know the academic major of each student, as being science or humanities, which serves as the classification."
  },
  {
    "objectID": "02_basic_lp.html#example-7-markov-decision-process-mdp",
    "href": "02_basic_lp.html#example-7-markov-decision-process-mdp",
    "title": "02_basic_lp",
    "section": "Example 7 â€“ Markov Decision Process (MDP)",
    "text": "Example 7 â€“ Markov Decision Process (MDP)\n\n\n\n\n\nMarkov Decision Process\n\n\nAn MDP problem is defined by a finite number of states, indexed by i=1,â€¦,mi = 1, \\ldots, m, where each state has a set of a finite number of actions, ğ’œi\\mathcal{A}_i, to take.\n\n\n\n\nEach action, say jâˆˆğ’œij \\in \\mathcal{A}_i, is associated with\n\nan immediate cost cjc_j of taking,\na probability distribution ğ©jâˆˆâ„m\\bm{p}_j \\in \\mathbb{R}^m to transfer to all possible states at the next time period.\n\nA stationary policy for the decision maker is a function Ï€={Ï€1,Ï€2,â‹¯,Ï€m}\\pi = \\{\\pi_1, \\pi_2, \\cdots, \\pi_m\\} that specifies a single action in every state, Ï€iâˆˆğ’œi\\pi_i \\in \\mathcal{A}_i.\n\n\n\n\n\n\n\n\nMDP Problem\n\n\nFind a stationary policy to minimize or maximize the discounted sum of expected costs or rewards over the infinite time horizon with a discount factor 0â‰¤Î³&lt;10 \\leq \\gamma &lt; 1, when the process starts from state i0i^0:\nâˆ‘t=0âˆÎ³tğ”¼[cÏ€it]\n\\sum_{t=0}^\\infty \\gamma^t \\mathbb{E}[c_{\\pi_{i^t}}]"
  },
  {
    "objectID": "02_basic_lp.html#example-7-markov-decision-process-mdp-1",
    "href": "02_basic_lp.html#example-7-markov-decision-process-mdp-1",
    "title": "02_basic_lp",
    "section": "Example 7 â€“ Markov Decision Process (MDP)",
    "text": "Example 7 â€“ Markov Decision Process (MDP)\n\n\n\n\n\nMaze Runner Game\n\n\nEach square represents a state, where each of states {1,2,3,4}\\{1, 2, 3, 4\\} has two possible actions to take:\n\nred action: moves to the next state at the next time period,\nblue action: shortcut move, with a probability distribution, to a state at the next time period.\n\nEach state of {5,6}\\{5, 6\\} has only one action\n\nmoving to state 66 (Exit state),\nmoving to state 11 (Start state).\n\nAll actions have zero cost, except state 55â€™s (Trap state) action, which has a 11-unit cost to get out.\nSuppose that the game is played infinitely, what is the optimal policy? That is, which action is best to take for every state at any time, to minimize the present-expected total cost?\n\n\n\n\n\n\n\n\n\n\nMDP Problem\n\n\nTwo constraints for the two actions of State 11\ny1âˆ’Î³y2â‰¤0,y1âˆ’Î³(14y3+14y5+14y6)â‰¤0.\ny_1 - \\gamma y_2 \\leq 0, \\;\\; y_1 - \\gamma(\\frac{1}{4}y_3 +\n\\frac{1}{4}y_5 + \\frac{1}{4}y_6) \\leq 0.\n\nOnly constraint for the single action of State 55\ny5âˆ’Î³y6â‰¤1.\ny_5 - \\gamma y_6 \\leq 1."
  },
  {
    "objectID": "02_basic_lp.html#example-7-markov-decision-process-mdp-2",
    "href": "02_basic_lp.html#example-7-markov-decision-process-mdp-2",
    "title": "02_basic_lp",
    "section": "Example 7 â€“ Markov Decision Process (MDP)",
    "text": "Example 7 â€“ Markov Decision Process (MDP)\n\nLet yi*y_i^\\ast, i=1,â€¦,mi = 1, \\ldots, m represent the optimal present-expected cost when the process starts at state ii and time 00\n\nalso called the cost-to_go value of state ii.\nThe yi*y_i^\\astâ€™s follow Bellmanâ€™s principle of optimality:\n\n\nyi*=minjâˆˆğ’œi(cj+Î³ğ©jâŠ¤ğ²*),\ny_i^\\ast = \\operatorname{min}_{j \\in \\mathcal{A}_i} (c_j + \\gamma \\bm{p}_j^\\top\n\\bm{y}^\\ast),\n\nwhere cjc_j is the immediate cost of taking action jâˆˆğ’œij \\in \\mathcal{A}_i at the current time period, and ğ©jâŠ¤ğ²*\\bm{p}_j^\\top \\bm{y}^\\ast is the optimal expected cost from the next time period, and then on.\n\nWhen yi*y_i^\\ast is known for every state, the optimal action in each state would be\n\nÏ€i*=argminjâˆˆğ’œi(cj+Î³ğ©jâŠ¤ğ²*),âˆ€i.\n\\pi_i^\\ast = \\operatorname{arg min}_{j \\in \\mathcal{A}_i} (c_j + \\gamma\n\\bm{p}_j^\\top \\bm{y}^\\ast), \\quad \\forall i."
  },
  {
    "objectID": "02_basic_lp.html#example-7-markov-decision-process-mdp-3",
    "href": "02_basic_lp.html#example-7-markov-decision-process-mdp-3",
    "title": "02_basic_lp",
    "section": "Example 7 â€“ Markov Decision Process (MDP)",
    "text": "Example 7 â€“ Markov Decision Process (MDP)\n\nOne observes that ğ²*âˆˆâ„m\\bm{y}^\\ast \\in \\mathbb{R}^m is a fixed-point of the Bellman operator, and it can be computed by the following linear program.\n\nmaximizeâˆ‘i=1myisubject toy1âˆ’Î³ğ©jâŠ¤ğ²â‰¤cjâˆ€jâˆˆğ’œ1,â‹®yiâˆ’Î³ğ©jâŠ¤ğ²â‰¤cjâˆ€jâˆˆğ’œi,â‹®ymâˆ’Î³ğ©jâŠ¤ğ²â‰¤cjâˆ€jâˆˆğ’œm.\n\\begin{align}\n\\operatorname{maximize} & \\sum_{i=1}^m y_i \\\\\n\\text{subject to} & y_1 - \\gamma \\bm{p}_j^\\top \\bm{y} \\leq c_j \\quad \\forall j \\in \\mathcal{A}_1, \\\\\n& \\vdots \\\\\n& y_i - \\gamma \\bm{p}_j^\\top \\bm{y} \\leq c_j \\quad \\forall j \\in \\mathcal{A}_i, \\\\\n& \\vdots \\\\\n& y_m - \\gamma \\bm{p}_j^\\top \\bm{y} \\leq c_j \\quad \\forall j \\in \\mathcal{A}_m. \\\\\n\\end{align}\n\n\nBasically, we relax the â€œmin\\operatorname{min}â€ operator to â€œâ‰¤\\leqâ€ from Bellmanâ€™s principle and make them into the constraints and then maximize the sum of yiy_iâ€™s as the objective.\nWhen the objective is maximized, at least one ineqality constraint in ğ’œi\\mathcal{A}_i must become equal for every state ii so that ğ²\\bm{y} is a fixed point solution of the Bellman operator."
  },
  {
    "objectID": "02_basic_lp.html#basic-solutions",
    "href": "02_basic_lp.html#basic-solutions",
    "title": "02_basic_lp",
    "section": "Basic Solutions",
    "text": "Basic Solutions\n\n\n\n\n\nSystem of equalities\n\n\nğ€ğ±=ğ›(2) \\bm{Ax} = \\bm{b}  \\qquad(2)\n\nFrom the nn columns of ğ€\\bm{A}, we select a set of mm linearly independent columns.\nWLOG, assume that the first mm columns of ğ€\\bm{A} are selected: denote the mÃ—mm \\times m matrix determined by these columns by ğ\\bm{B}.\nThe matrix ğ\\bm{B} is nonsingular and we may uniquely solve the equation ğğ±ğ=ğ›orğ±ğ=ğâˆ’1ğ› \\bm{Bx_B} = \\bm{b} \\quad \\text{or} \\quad \\bm{x_B} = \\bm{B}^{-1}\\bm{b} \n\n\n\n\n\nWe refer to ğ\\bm{B} as a basis, since ğ\\bm{B} consists of mm linearly independent columns that can be regarded as a basis for â„m\\mathbb{R}^m.\n\n\n\n\n\nDefinition\n\n\nGiven the set of mm simultaneous linear equations nn unknowns EquationÂ 2, let ğ\\bm{B} be any nonsingular mÃ—mm \\times m submatrix made up of columns of ğ€\\bm{A}. Then, if all nâˆ’mn-m components of ğ±\\bm{x} not associated with columns of ğ\\bm{B} are set equal to zero, the solution to the resulting set of equations is said to be a basic solution to EquationÂ 2 with respect to basis ğ\\bm{B}.\n\n\n\n\n\n\nDefinition\n\n\nThe components of ğ±\\bm{x} associated with the columns of ğ\\bm{B}, denoted by the subvector ğ±B\\bm{x}_B according to the same column index order in ğ\\bm{B}, are called basic variables.\n\n\n\n\n\n\n\n\nFull-Rank Assumption\n\n\nThe mÃ—nm \\times n matrix ğ€\\bm{A} has m&lt;nm &lt; n, and the mm rows of ğ€\\bm{A} are linearly independent."
  },
  {
    "objectID": "02_basic_lp.html#basic-feasible-solutions-1",
    "href": "02_basic_lp.html#basic-feasible-solutions-1",
    "title": "02_basic_lp",
    "section": "Basic Feasible Solutions",
    "text": "Basic Feasible Solutions\n\n\n\nDefinition\n\n\nIf one or more of the basic variables in a basic solution has value zero, that solution is said to be a degenerate basic solution.\n\n\n\n\nIn a nondegenerate basic solution, the basic variables, and hence the basis ğ\\bm{B}, can be immediately identified from the positive components of the solution.\n\nThere is ambiguity associated with a degenerate basic solution - some of the nonbasic variables can be interchanged!\n\n\n\n\n\nDefinition\n\n\nA vector ğ±\\bm{x} satisfying\nğ€ğ±=ğ›,ğ±â‰¥ğŸ,(3)\n\\bm{Ax} = \\bm{b}, \\quad \\bm{x} \\geq \\bm{0},\n \\qquad(3)\nis said to be feasible for these constraints. A feasible solution to the constraints EquationÂ 3 that is also basic is said to be a basic feasible solution; if this solution is also a degenerate basic solution, it is called a degenerate basic feasible solution."
  },
  {
    "objectID": "02_basic_lp.html#the-fundamental-theorem-of-lp",
    "href": "02_basic_lp.html#the-fundamental-theorem-of-lp",
    "title": "02_basic_lp",
    "section": "The Fundamental Theorem of LP",
    "text": "The Fundamental Theorem of LP\n\nCorresponding to a linear program in standard form\nminimizeğœâŠ¤ğ±subject toğ€ğ±=ğ›,ğ±â‰¥0(4)\n\\begin{align}\n\\operatorname{minimize} & \\bm{c}^\\top \\bm{x}  \\\\\n\\text{subject to} & \\bm{Ax} = \\bm{b}, \\;\\; \\bm{x} \\geq 0\n\\end{align} \n \\qquad(4)\na feasible solution to the constraints that achieves the minimum value of the objective function subject to those constraints is said to be an optimal feasible solution. If this solution is basic, it is an optimal basic feasible solution.\n\n\n\n\n\nFundamental Theorem\n\n\nGiven a linear program in standard form EquationÂ 4 where ğ€\\bm{A} is an mÃ—nm \\times n matrix of rank mm,\n\nif there is a feasible solution, there is a basic feasible solution;\nif there is an optimal feasible solution, there is an optimal basic feasible solution.\n\n\n\n\n\n\n\n\n\nProof (1)\n\n\nDenote the columns of ğ€\\bm{A} by ğš1,ğš2,â€¦,ğšn\\bm{a}_1, \\bm{a}_2, \\ldots, \\bm{a}_n. Suppose ğ±=(x1,x2,â€¦,xn)\\bm{x} = (x_1, x_2, \\ldots, x_n) is a feasible solution. Then, in terms of the columns of ğ€\\bm{A}, this solution satisfies\nx1ğš1+x2ğš2+â‹¯+xnğšn=ğ›. x_1\\bm{a}_1 + x_2\\bm{a}_2 + \\cdots + x_n\\bm{a}_n = \\bm{b}."
  },
  {
    "objectID": "02_basic_lp.html#the-fundamental-theorem-of-lp-1",
    "href": "02_basic_lp.html#the-fundamental-theorem-of-lp-1",
    "title": "02_basic_lp",
    "section": "The Fundamental Theorem of LP",
    "text": "The Fundamental Theorem of LP\n\n\n\nProof (1) - Continued -\n\n\nAssume that exactly pp of the variables xix_i are greater than zero, and wlog, that they are the first pp variables. Thus\nx1ğš1+x2ğš2+â‹¯+xpğšp=ğ›.(5) x_1 \\bm{a}_1 + x_2 \\bm{a}_2 + \\cdots + x_p \\bm{a}_p = \\bm{b}.  \\qquad(5)\nThere are now two cases, corresponding as to whether the set ğš1,ğš2,â€¦,ğšp\\bm{a}_1, \\bm{a}_2, \\ldots, \\bm{a}_p is linearly independent or linearly dependent.\nCase 1: Assume ğš1,ğš2,â€¦,ğšp\\bm{a}_1, \\bm{a}_2, \\ldots, \\bm{a}_p are linearly independent. Then clearly, pâ‰¤mp \\leq m. If p=mp = m, the solution is basic and the proof is complete. If p&lt;mp &lt; m, then, since ğ€\\bm{A} has rank mm, mâˆ’pm-p vectors can be found from the remaining nâˆ’pn-p vectors so that the resulting set of mm vectors is lienarly independent. Assigning the value zero to the corresponding mâˆ’pm-p variables yields a (degenerate) basic feasible solution.\nCase 2: Assume ğš1,ğš2,â€¦,ğšp\\bm{a}_1, \\bm{a}_2, \\ldots, \\bm{a}_p are linearly dependent. Then there is a nontrivial linear combination of these vectors that is zero. Thus there are constants y1,y2,â€¦,ypy_1, y_2, \\ldots, y_p, at least one of which can be assumed to be positive, such that\ny1ğš1+y2ğš2+â‹¯+ypğšp=ğŸ.(6) y_1\\bm{a}_1 + y_2\\bm{a}_2 + \\cdots + y_p\\bm{a}_p = \\bm{0}.  \\qquad(6)\nMultiplying this equation by a scalar Îµ\\varepsilon and subtracting it from EquationÂ 5, we obtain\n(x1âˆ’Îµy1)ğš1+(x2âˆ’Îµy2)ğš2+â‹¯+(xpâˆ’Îµyp)ğšp=ğ›.(7) (x_1 - \\varepsilon y_1)\\bm{a}_1 + (x_2 - \\varepsilon y_2)\\bm{a}_2 + \\cdots + \n   (x_p - \\varepsilon y_p)\\bm{a}_p = \\bm{b}.  \\qquad(7)\nThis equation hods for every Îµ\\varepsilon, and for each Îµ\\varepsilon the components xjâˆ’Îµyjx_j - \\varepsilon y_j correspond to a solution of the linear equalities â€” although they may violate xiâˆ’Îµyiâ‰¥0x_i - \\varepsilon y_i \\geq 0. Denoting ğ²=(y1,y2,â€¦,yp,0,0,â€¦,0)\\bm{y} = (y_1, y_2, \\ldots, y_p, 0, 0, \\ldots, 0), we see that for any Îµ\\varepsilon\nğ±âˆ’Îµğ²(8) \\bm{x} - \\varepsilon \\bm{y}  \\qquad(8)\nis a solution to the equalities. For Îµ=0\\varepsilon = 0, this reduces to the original feasible solution."
  },
  {
    "objectID": "02_basic_lp.html#the-fundamental-theorem-of-lp-2",
    "href": "02_basic_lp.html#the-fundamental-theorem-of-lp-2",
    "title": "02_basic_lp",
    "section": "The Fundamental Theorem of LP",
    "text": "The Fundamental Theorem of LP\n\n\n\nProof (1) - Continued -\n\n\nAs Îµ\\varepsilon is increased from zero, the various components increase, decrease, or remain constant, depending upon whether the correspoding yiy_i is negative, positive, or zero. Since we assume at least one yiy_i is positive, at least one component will decrease as Îµ\\varepsilon is increased. we increase Îµ\\varepsilon to the first point where one or more components become zero. Specifically, we set\nÎµ=min{xiyi:yi&gt;0}. \\varepsilon = \\operatorname{min}\\left\\{\\frac{x_i}{y_i}: y_i &gt; 0 \\right\\}. \nFor this value of Îµ\\varepsilon, the solution given by EquationÂ 8 is feasible and has at most pâˆ’1p-1 positive variables. Repeating this process as necessary, we can eliminate positive variables until we have a feasible solution with corresponding columns that are linearly independent. At that point Case 1 applies.\n\n\n\n\n\n\n\n\nProof (2)\n\n\nLet ğ±=(x1,x2,â€¦,xn)\\bm{x} = (x_1, x_2, \\ldots, x_n) be an optimal feasible solution and, as in the proof of (1) above, suppose there are exactly pp positive variables x1,x2,â€¦,xpx_1, x_2, \\ldots, x_p. Agan, there are two cases; and Case 1, corresponding to the linear independence is exactly the same as before.\nCase 2 also goes exactly the same as before, but it must be shown that for any Îµ\\varepsilon the solution EquationÂ 8 is optimal. To show this, note that the value of the solution ğ±âˆ’Îµğ²\\bm{x} - \\varepsilon\\bm{y} is\nğœâŠ¤ğ±âˆ’ÎµğœâŠ¤ğ².(9) \\bm{c}^\\top \\bm{x} - \\varepsilon \\bm{c}^\\top \\bm{y}.  \\qquad(9)\nFor Îµ\\varepsilon sufficiently small in magnitude, ğ±âˆ’Îµğ²\\bm{x} - \\varepsilon \\bm{y} is a feasible solution for positive or negative values of Îµ\\varepsilon. Thus, we conclude that ğœâŠ¤ğ²=ğŸ\\bm{c}^\\top \\bm{y} = \\bm{0} (why?).\n\n\n\n\n\n\n\n\nProof (2)\n\n\nLet ğ±=(x1,x2,â€¦,xn)\\bm{x} = (x_1, x_2, \\ldots, x_n) be an optimal feasible solution and, as in the proof of (1) above, suppose there are exactly pp positive variables x1,x2,â€¦,xpx_1, x_2, \\ldots, x_p. Agan, there are two cases; and Case 1, corresponding to the linear independence is exactly the same as before.\nCase 2 also goes exactly the same as before, but it must be shown that for any Îµ\\varepsilon the solution EquationÂ 8 is optimal. To show this, note that the value of the solution ğ±âˆ’Îµğ²\\bm{x} - \\varepsilon\\bm{y} is\nğœâŠ¤ğ±âˆ’ÎµğœâŠ¤ğ².(10) \\bm{c}^\\top \\bm{x} - \\varepsilon \\bm{c}^\\top \\bm{y}.  \\qquad(10)\nFor Îµ\\varepsilon sufficiently small in magnitude, ğ±âˆ’Îµğ²\\bm{x} - \\varepsilon \\bm{y} is a feasible solution for positive or negative values of Îµ\\varepsilon. Thus, we conclude that ğœâŠ¤ğ²=ğŸ\\bm{c}^\\top \\bm{y} = \\bm{0}. For, if ğœâŠ¤ğ²â‰ ğŸ\\bm{c}^\\top \\bm{y} \\neq \\bm{0}, an Îµ\\varepsilon of small magnitude and proper sign could be determined so as to render EquationÂ 10 smaller than ğœâŠ¤ğ²\\bm{c}^\\top \\bm{y} while maintaining feasibility."
  },
  {
    "objectID": "02_basic_lp.html#the-fundamental-theorem-of-lp-3",
    "href": "02_basic_lp.html#the-fundamental-theorem-of-lp-3",
    "title": "02_basic_lp",
    "section": "The Fundamental Theorem of LP",
    "text": "The Fundamental Theorem of LP\n\nPart (1) of the theorem is commonly referred to as CarathÃ©odoryâ€™s theorem.\nPart (2) of the theorem reduces the task of solving a linear program to that of searching over basic feasible solutions.\nSince for a problem having nn variables and mm constraints there are at most\n\n(nm)=n!m!(nâˆ’m)!\n\\binom{n}{m} = \\frac{n!}{m!(n-m)!}\n\nbasic solutions (corresponding to the number of ways of selecting mm of nn columns), there are only a finite number of possibilities.\n\nThus the fundamental theorem yields an obvious, but terribly inefficient, finite search technique.\nBy expanding upon the technique of proof as well as the statement of the fundamental theorem, the efficient simplex procedure is derived."
  }
]