[
  {
    "objectID": "02_basic_lp.html#optimization-theory-and-practice",
    "href": "02_basic_lp.html#optimization-theory-and-practice",
    "title": "02_basic_lp",
    "section": "Optimization Theory and Practice",
    "text": "Optimization Theory and Practice\n\n\nBasic Properties of Linear Programs\n\n\n\n\nInstructor: Aykut Satici, Ph.D.   Mechanical and Biomedical Engineering  Electrical and Computer Engineering  Boise State University, Boise, ID, USA\n\n\nTopics:  Introduction  Examples  Basic Feasible Solutions  Fundamental Theorem  Convex Geometry  Farkas’s Lemma"
  },
  {
    "objectID": "02_basic_lp.html#standard-form",
    "href": "02_basic_lp.html#standard-form",
    "title": "02_basic_lp",
    "section": "Standard Form",
    "text": "Standard Form\n\n\n\n\n\n\nLinear Program (LP)\n\n\nAn LP is an optimization problem in which the objective function is linear in the unknowns and the constraints consist of linear (in)equalities.\n\n\n\n\n\n\n\n\n\nStandard form\n\n\nminimize𝐜⊤𝐱=c1x1+c2x2+⋯+cnxnsubject to𝐀𝐱=𝐛and𝐱≥𝟎.\n\\begin{align}\n\\operatorname{minimize} & \\bm{c}^\\top \\bm{x} = c_1x_1 + c_2x_2 + \\cdots + c_nx_n\n\\\\\n\\text{subject to} & \\bm{A}\\bm{x} = \\bm{b} \\quad \\text{and} \\quad \\bm{x} \\geq\n\\bm{0}.\n\\end{align}\n\n\n𝐜,𝐱∈ℝn\\bm{c}, \\bm{x} \\in \\mathbb{R}^n are column vectors, 𝐀∈ℝm×n\\bm{A} \\in \\mathbb{R}^{m \\times n} a fat matrix (m&lt;nm &lt; n), 𝐛∈ℝm\\bm{b} \\in \\mathbb{R}^m a column vector.\nbib_i’s, cic_i’s and aija_{ij}’s are fixed real constants, and the xix_i’s are real numbers to be determined.\nWe assume that each equation has been multiplied by minus unity, if necessary, so that each bi≥0b_i \\geq 0."
  },
  {
    "objectID": "02_basic_lp.html#slack-variables",
    "href": "02_basic_lp.html#slack-variables",
    "title": "02_basic_lp",
    "section": "Slack Variables",
    "text": "Slack Variables\n\n\n\n\n\nmaximizec1x2+c2x2+⋯+cnxnsubject toa11x1+a12x2+⋯+a1nxn≤b1,a21x1+a22x2+⋯+a2nxn≤b2,⋮⋮am1x1+am2x2+⋯+amnxn≤bm,x1,x2,…,xn≥0.\n\\begin{align}\n\\operatorname{maximize} & c_1x_2 + c_2x_2 + \\cdots + c_nx_n \\\\\n\\text{subject to} & a_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n &\\leq b_1, \\\\\n& a_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n &\\leq b_2, \\\\\n& \\vdots & \\vdots \\\\\n& a_{m1}x_1 + a_{m2}x_2 + \\cdots + a_{mn}x_n &\\leq b_m, \\\\\n& x_1, x_2, \\ldots, x_n \\geq 0.\n\\end{align}\n\n\n\n\nThis problem may be alternatively expressed as\n\n\n\nminimize−c1x2−c2x2−⋯−cnxnsubject toa11x1+a12x2+⋯+a1nxn+xn+1=b1,a21x1+a22x2+⋯+a2nxn12+xn+2=b2,⋮⋮am1x1+am2x2+⋯+amnxn12345+xn+m=bm,x1,x2,…,xn+1,…,xn+m≥0.\n\\begin{align}\n\\operatorname{minimize} & -c_1x_2 - c_2x_2 - \\cdots - c_nx_n \\\\\n\\text{subject to} & a_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n + x_{n+1} &= b_1, \\\\\n& a_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n \\phantom{12} + x_{n+2} &= b_2, \\\\\n& \\vdots & \\vdots \\\\\n& a_{m1}x_1 + a_{m2}x_2 + \\cdots + a_{mn}x_n \\phantom{12345} + x_{n+m} &= b_m, \\\\\n& x_1, x_2, \\ldots, x_{n+1}, \\ldots, x_{n+m} \\geq 0.\n\\end{align}\n\n\n\n\n\n\nThe new nonnegative variables xn+ix_{n+i}, i=1,…,mi=1, \\ldots, m convert inequalities to equalities are called slack variables.\n\n \n\nThe constrant matrix now is transformed to 𝐀→[𝐀𝐈]\\bm{A} \\rightarrow \\begin{bmatrix} \\bm{A} & \\bm{I} \\end{bmatrix}."
  },
  {
    "objectID": "02_basic_lp.html#surplus-variables",
    "href": "02_basic_lp.html#surplus-variables",
    "title": "02_basic_lp",
    "section": "Surplus Variables",
    "text": "Surplus Variables\n\nIf the linear equalities are reversed so that a typical inequality is\n\nai1x1+ai2x2+⋯+ainxn≥bi, a_{i1}x_1 + a_{i2}x_2 + \\cdots + a_{in} x_n \\geq b_i, \nit is clear that this is equivalent to\nai1x1+ai2x2+⋯+ainxn−yi=bi, a_{i1}x_1 + a_{i2}x_2 + \\cdots + a_{in} x_n - y_i = b_i, \nwith yi≥0y_i \\geq 0.\n\nVariables, such as yiy_i, adjoined in this fashion to convert a “greater than or equal to” inequality to equality are called surplus variables.\nBy suitably multiplying by minus unity, and adjoining slack and surplus variables, any set of linear inequalities can be converted to standard form."
  },
  {
    "objectID": "02_basic_lp.html#free-variables-first-method",
    "href": "02_basic_lp.html#free-variables-first-method",
    "title": "02_basic_lp",
    "section": "Free variables – First Method",
    "text": "Free variables – First Method\n\n\n\nSuppose one or more of the unknown variables is not required to be nonnegative, say, x1≥0x_1 \\geq 0 is not present so that x1x_1 is free to take on either positive or negative values.\n\n\n\n\nWe then write x1=u1−v1x_1 = u_1 - v_1, and require that u1,v1≥0u_1, v_1 \\geq 0.\nWe substitute u1−v1u_1 - v_1 for x1x_1 everywhere.\nThe problem is then expressed in terms of the n+1n+1 variables u1,v1,x2,x3,…,xnu_1, v_1, x_2, x_3, \\ldots, x_n.\nThere is a certain degree of redundancy introduced in this technique since a constant added to u1u_1 and v1v_1 does not change x1x_1.\nNevertheless, the simplex method can still be used to find the solution."
  },
  {
    "objectID": "02_basic_lp.html#free-variables-second-method",
    "href": "02_basic_lp.html#free-variables-second-method",
    "title": "02_basic_lp",
    "section": "Free variables – Second Method",
    "text": "Free variables – Second Method\n\nTake the same free variable situation, i.e, x1x_1 is free to take on positive or negative values.\nOne can take any one of the mm equality constraints which has a nonzero coefficient for x1x_1, say, for example,\n\nai1x1+ai2x2+⋯+ainxn=bi, a_{i1}x_1 + a_{i2}x_2 + \\cdots + a_{in}x_n = b_i, \nwhere ai1≠0a_{i1} \\neq 0.\n\nThen x1x_1 can be expressed as a linear combination of the other variables, plus a constant.\nThis expression can be substituted everywhere for x1x_1 and we are led to a new problem of exactly the same form but expressed in terms of the variables x2,x3,…,xnx_2, x_3, \\ldots, x_n only.\nFurthermore, the ithi^{\\text{th}} equation, used to determine x1x_1 is now identically zero and it too can be eliminated.\nWe obtain a linear program having n−1n-1 variables and m−1m-1 constraint equations."
  },
  {
    "objectID": "02_basic_lp.html#example-specific-case",
    "href": "02_basic_lp.html#example-specific-case",
    "title": "02_basic_lp",
    "section": "Example – Specific Case",
    "text": "Example – Specific Case\n\n\n\n\n\nminimizex1+3x2+4x3subject tox1+2x2+3x3=5,2x1+3x2+x3=6,x2,x3≥0.\n\\begin{align}\n\\operatorname{minimize} & x_1 + 3x_2 + 4x_3 \\\\\n\\text{subject to} & x_1 + 2x_2 + 3x_3 = 5, \\\\\n& 2x_1 + 3x_2 + x_3 = 6, \\\\\n& x_2, x_3 \\geq 0.\n\\end{align}\n\n\n\n\n\nx1x_1 is free, so we can solve for it from the first constraint, obtaining\nx1=5−2x2−x3.(1) x_1 = 5 - 2x_2 - x_3. \\qquad(1)\n\n\nSubstituting this into the objective and the second constraint, we obtain the equivalent problem\nminimizex2+3x3subject tox2+x3=4,x2,x3≥0.\n\\begin{align}\n\\operatorname{minimize} & x_2 + 3x_3 \\\\\n\\text{subject to}& x_2 + x_3 = 4, \\\\\n& x_2, x_3 \\geq 0.\n\\end{align}\n\nwhich is a problem in standard form.\nAfter the smaller problem is solved (what is the answer?) the value for x1=−3x_1 = -3 can be found from Equation 1."
  },
  {
    "objectID": "02_basic_lp.html#example-1-the-diet-problem",
    "href": "02_basic_lp.html#example-1-the-diet-problem",
    "title": "02_basic_lp",
    "section": "Example 1 – The Diet Problem",
    "text": "Example 1 – The Diet Problem\n\n\n\nDetermine the most economical diet that satisfies the basic minimum nutritional requirements for good health\n\n\n\n\n\n\n\n\n\nThere are available nn different foods.\nThere are mm basic nutritional ingredients,\nEach unit of food jj contains aija_{ij} units of the ithi^{\\text{th}} nutrient.\n\n\n\njthj^{\\text{th}} food sells at a price cjc_j per unit.\nEach individual must receive at least bib_i units of the ithi^{\\text{th}} nutrient per day.\n\n\n\n\nIf we denote by xjx_j the number of units of food jj in the diet, the problem is to select xjx_j’s to minimize the total cost c1x1+c2x2+⋯+cnxn c_1x_1 + c_2x_2 + \\cdots + c_nx_n \n\n\nsubject to the nutritional constraints ai1x1+ai2x2+⋯+ainxn≥bi,i=1,…,m, a_{i1}x_1 + a_{i2}x_2 + \\cdots + a_{in}x_n \\geq b_i, \\; i=1, \\ldots, m, \n\n\nand the nonnegative constraints x1≥0,x2≥0,…,xn≥0, x_1 \\geq 0, x_2 \\geq 0, \\ldots, x_n \\geq 0,  on the food quantities.\n\n\n\n\n\nThis problem can be converted to standard form by subtracting a nonnegative surplus variable from the left side of each of the mm linear inequalities."
  },
  {
    "objectID": "02_basic_lp.html#example-2-the-resource-allocation-problem",
    "href": "02_basic_lp.html#example-2-the-resource-allocation-problem",
    "title": "02_basic_lp",
    "section": "Example 2– The Resource-Allocation Problem",
    "text": "Example 2– The Resource-Allocation Problem\n\n\n\nA facility is capable of manufacturing nn different products.\nEach product can be produced at any level xj≥0x_j \\geq 0, j=1,2,…,nj=1, 2, \\ldots, n.\nEach unit of the jthj^{\\text{th}} product needs aija_{ij} units of the ithi^{\\text{th}} resource, i=1,2,…,mi = 1, 2, \\ldots, m.\n\n\n\nEach product may require various amounts of mm different resources.\nEach unit of the jthj^{\\text{th}} product can sell for πj\\pi_j dollars.\nEach bib_i, i=1,2,…,mi = 1, 2, \\ldots, m describe the available quantities of the mm resources.\n\n\n\n\nWe wish to manufacture products at maximum revenue\nmaximizeπ1x1+π2x2+⋯+πnxn\n\\begin{align}\n\\operatorname{maximize} & \\pi_1x_1 + \\pi_2x_2 + \\cdots + \\pi_nx_n\n\\end{align}\n\n\n\nsubject to the resource constraints\nsubject toai1x1+ai2x2+⋯+ainxn≤bi,i=1,…,m\n\\begin{align}\n\\text{subject to} & a_{i1}x_1 + a_{i2}x_2 + \\cdots + a_{in}x_n \\leq b_i, \\; i=1,\n\\ldots, m\n\\end{align}\n\nand the nonnegativity consraints on all production variables.\n\n\n\nThe problem can also be interpreted as\n\nfund nn different activities, where\nπj\\pi_j is the full reward from the jthj^{\\text{th}} activity,\nxjx_j is restricted to 0≤xj≤10 \\leq x_j \\leq 1, representing the funding level from 0%0\\% to 100%100\\%."
  },
  {
    "objectID": "02_basic_lp.html#example-3-the-transportation-problem",
    "href": "02_basic_lp.html#example-3-the-transportation-problem",
    "title": "02_basic_lp",
    "section": "Example 3 – The Transportation Problem",
    "text": "Example 3 – The Transportation Problem\n\n\n\nQuantities a1,a2,…,ama_1, a_2, \\ldots, a_m of a certain product are to be shipped from mm locations.\nShipping a unit of product from origin ii to destination jj costs cijc_{ij}.\n\n\n\nThese products will be received in amounts of b1,b2,…,bnb_1, b_2, \\ldots, b_n at each of nn destinations.\nWe want to determine the amounts xijx_{ij} to be shipped between each origin-destination pair i=1,2,…,mi = 1, 2, \\ldots, m; j=1,2,…,nj=1, 2, \\ldots, n.\n\n\n\n\n\n\n\nx11x_{11}\nx12x_{12}\n⋯\\cdots\nx1nx_{1n}\n|\na1a_1\n\n\nx21x_{21}\nx22x_{22}\n⋯\\cdots\nx2nx_{2n}\n|\na2a_2\n\n\n⋮\\vdots\n⋮\\vdots\n⋮\\vdots\n⋮\\vdots\n|\n⋮\\vdots\n\n\nxm1x_{m1}\nxm2x_{m2}\n⋯\\cdots\nxmnx_{mn}\n|\nama_m\n\n\n——\n——\n——\n——\n\n\n\n\nb1b_{1}\nb2b_{2}\n⋯\\cdots\nbnb_{n}\n\n\n\n\n\n\n\nThe ithi^{\\text{th}} row in this array defines the variables associated with the ithi^{\\text{th}} origin.\nThe jthj^{\\text{th}} column defines the variables associated with the jthj^{\\text{th}} destination.\nProblem: find the nonnegative variables xijx_{ij} so that the sum across the ithi^{\\text{th}} row is aja_j, the sum down the jthj^{\\text{th}} column is bjb_j, and the weighted sum ∑j=1n∑i=1mcijxij\\sum_{j=1}^n\\sum_{i=1}^m c_{ij}x_{ij} is minimized."
  },
  {
    "objectID": "02_basic_lp.html#example-4-the-maximal-flow-problem",
    "href": "02_basic_lp.html#example-4-the-maximal-flow-problem",
    "title": "02_basic_lp",
    "section": "Example 4 – The Maximal Flow Problem",
    "text": "Example 4 – The Maximal Flow Problem\n\n\n\n\n\nMaximal flow problem\n\n\nDetermine the maximal flow that can be established in such a network.\n\n\n\nmaximizefsubject to∑j=1nx1j−∑j=1nxj1−f=0,∑j=1nxij−∑j=1nxji=0,i≠1,m,∑j=1nxmj−∑j=1nxjm+f=0,0≤xij≤kij,∀i,j,\n\\begin{align}\n\\operatorname{maximize} & f \\\\\n\\text{subject to} & \\sum_{j=1}^n x_{1j} - \\sum_{j=1}^n x_{j1} - f = 0, \\\\\n& \\sum_{j=1}^n x_{ij} - \\sum_{j=1}^n x_{ji}  = 0, \\quad i \\neq 1, m, \\\\\n& \\sum_{j=1}^n x_{mj} - \\sum_{j=1}^n x_{jm} + f = 0, \\\\\n& 0 \\leq x_{ij} \\leq k_{ij}, \\quad \\forall i, j,\n\\end{align}\n\nwhere kij=0k_{ij} = 0 for those no-arc pairs (i,j)(i,j).\n\n\n\n\n\n\n\n\nCapacitated network in which two special nodes, called the source (node 1); and the sink (node mm) are distinguished.\nAll other nodes must satisfy the conservation requirement: net flow into these nodes must be zero.\n\nthe source may have a net outflow,\nthe sink may have a net inflow.\n\nThe outlow ff of the source will equal the inflow of the sink."
  },
  {
    "objectID": "02_basic_lp.html#example-5-a-supply-chain-problem",
    "href": "02_basic_lp.html#example-5-a-supply-chain-problem",
    "title": "02_basic_lp",
    "section": "Example 5 – A Supply-Chain Problem",
    "text": "Example 5 – A Supply-Chain Problem\n\n\n\nA warehouse is buying and selling stock of a certain commodity in order to maximize profit over a certain length of time.\n\n\n\n\n\n\nWarehouse has a fixed capacity CC.\nThe price, pip_i, of the commodity is known to fluctuate over a number of time periods, say months, indexed by ii.\nThe warehouse is originally empty and is required to be empty at the end of the last period.\n\n\n\nThere is a cost rr per unit of holding stock for one period.\nIn any period the same price holds for both purchase and sale.\nxix_i: level of stock in the warehouse at the beginning of period ii, uiu_i: amount bought during this period, sis_i: amount sold during this period.\n\n\n\nmaximize∑i=1n(pi(si−ui)−rxi)subject toxi+1=xi+ui−si,i=1,2,…,n−1,0=xn+un−sn,xi+zi=C,i=2,…,n,x1=0,xi≥0,ui≥0,si≥0,zi≥0,\n\\begin{align}\n\\operatorname{maximize} & \\sum_{i=1}^n \\left(p_i(s_i - u_i) - rx_i \\right) \\\\\n\\text{subject to} & x_{i+1} = x_i + u_i - s_i, \\quad i = 1, 2, \\ldots, n-1, \\\\\n& 0 = x_n + u_n - s_n, \\\\\n& x_i + z_i = C, \\quad i = 2, \\ldots, n, \\\\\n& x_1 = 0, x_i \\geq 0, u_i \\geq 0, s_i \\geq 0, z_i \\geq 0,\n\\end{align}\n\nwhere ziz_i is a slack variable."
  },
  {
    "objectID": "02_basic_lp.html#example-6-linear-classifier-and-support-vector-machine",
    "href": "02_basic_lp.html#example-6-linear-classifier-and-support-vector-machine",
    "title": "02_basic_lp",
    "section": "Example 6 – Linear Classifier and Support Vector Machine",
    "text": "Example 6 – Linear Classifier and Support Vector Machine\n\n\n\n\n\ndd-dimensional data points are to be classified into two distinct classes.\n\n\n\n\nIn general, we have vectors 𝐚i∈ℝd\\bm{a}_i \\in \\mathbb{R}^d for i=1,2,…,n1i=1, 2, \\ldots, n_1 and vector 𝐛j∈ℝd\\bm{b}_j \\in \\mathbb{R}^d for j=1,2,…,n2j = 1, 2, \\ldots, n_2.\nWe wish to find a hyperplane that separates 𝐚i\\bm{a}_i’s from the 𝐛j\\bm{b}_j’s, i.e., find a slope-vector y∈ℝdy \\in \\mathbb{R}^d and an intercept β\\beta such that\n\n𝐚i⊤𝐲+β≥1,∀i,𝐛j⊤𝐲+β≤1,∀j,\n\\begin{align}\n  \\bm{a}_i^\\top \\bm{y} + \\beta &\\geq 1, \\quad \\forall i, \\\\\n  \\bm{b}_j^\\top \\bm{y} + \\beta &\\leq 1, \\quad \\forall j, \\\\\n\\end{align}\n\nwhere {𝐱:𝐱𝐲+β=0}\\{\\bm{x}: \\bm{x}^\\bm{y} + \\beta = 0\\} is the desired hyperplane.\n\nThe separation is defined by the fixed margins +1+1 and −1-1, which could be made soft or variable later.\n\n\n\n\n\n\n\n\nExample\n\n\n\nTwo-dimensional data points may be grade averages in science and humanities for different students.\nWe also know the academic major of each student, as being science or humanities, which serves as the classification."
  },
  {
    "objectID": "02_basic_lp.html#example-7-markov-decision-process-mdp",
    "href": "02_basic_lp.html#example-7-markov-decision-process-mdp",
    "title": "02_basic_lp",
    "section": "Example 7 – Markov Decision Process (MDP)",
    "text": "Example 7 – Markov Decision Process (MDP)\n\n\n\n\n\nMarkov Decision Process\n\n\nAn MDP problem is defined by a finite number of states, indexed by i=1,…,mi = 1, \\ldots, m, where each state has a set of a finite number of actions, 𝒜i\\mathcal{A}_i, to take.\n\n\n\n\nEach action, say j∈𝒜ij \\in \\mathcal{A}_i, is associated with\n\nan immediate cost cjc_j of taking,\na probability distribution 𝐩j∈ℝm\\bm{p}_j \\in \\mathbb{R}^m to transfer to all possible states at the next time period.\n\nA stationary policy for the decision maker is a function π={π1,π2,⋯,πm}\\pi = \\{\\pi_1, \\pi_2, \\cdots, \\pi_m\\} that specifies a single action in every state, πi∈𝒜i\\pi_i \\in \\mathcal{A}_i.\n\n\n\n\n\n\n\n\nMDP Problem\n\n\nFind a stationary policy to minimize or maximize the discounted sum of expected costs or rewards over the infinite time horizon with a discount factor 0≤γ&lt;10 \\leq \\gamma &lt; 1, when the process starts from state i0i^0:\n∑t=0∞γt𝔼[cπit]\n\\sum_{t=0}^\\infty \\gamma^t \\mathbb{E}[c_{\\pi_{i^t}}]"
  },
  {
    "objectID": "02_basic_lp.html#example-7-markov-decision-process-mdp-1",
    "href": "02_basic_lp.html#example-7-markov-decision-process-mdp-1",
    "title": "02_basic_lp",
    "section": "Example 7 – Markov Decision Process (MDP)",
    "text": "Example 7 – Markov Decision Process (MDP)\n\n\n\n\n\nMaze Runner Game\n\n\nEach square represents a state, where each of states {1,2,3,4}\\{1, 2, 3, 4\\} has two possible actions to take:\n\nred action: moves to the next state at the next time period,\nblue action: shortcut move, with a probability distribution, to a state at the next time period.\n\nEach state of {5,6}\\{5, 6\\} has only one action\n\nmoving to state 66 (Exit state),\nmoving to state 11 (Start state).\n\nAll actions have zero cost, except state 55’s (Trap state) action, which has a 11-unit cost to get out.\nSuppose that the game is played infinitely, what is the optimal policy? That is, which action is best to take for every state at any time, to minimize the present-expected total cost?\n\n\n\n\n\n\n\n\n\n\nMDP Problem\n\n\nTwo constraints for the two actions of State 11\ny1−γy2≤0,y1−γ(14y3+14y5+14y6)≤0.\ny_1 - \\gamma y_2 \\leq 0, \\;\\; y_1 - \\gamma(\\frac{1}{4}y_3 +\n\\frac{1}{4}y_5 + \\frac{1}{4}y_6) \\leq 0.\n\nOnly constraint for the single action of State 55\ny5−γy6≤1.\ny_5 - \\gamma y_6 \\leq 1."
  },
  {
    "objectID": "02_basic_lp.html#example-7-markov-decision-process-mdp-2",
    "href": "02_basic_lp.html#example-7-markov-decision-process-mdp-2",
    "title": "02_basic_lp",
    "section": "Example 7 – Markov Decision Process (MDP)",
    "text": "Example 7 – Markov Decision Process (MDP)\n\nLet yi*y_i^\\ast, i=1,…,mi = 1, \\ldots, m represent the optimal present-expected cost when the process starts at state ii and time 00\n\nalso called the cost-to-go value of state ii.\nThe yi*y_i^\\ast’s follow Bellman’s principle of optimality:\n\n\nyi*=minj∈𝒜i(cj+γ𝐩j⊤𝐲*),\ny_i^\\ast = \\operatorname{min}_{j \\in \\mathcal{A}_i} (c_j + \\gamma \\bm{p}_j^\\top\n\\bm{y}^\\ast),\n\nwhere cjc_j is the immediate cost of taking action j∈𝒜ij \\in \\mathcal{A}_i at the current time period, and 𝐩j⊤𝐲*\\bm{p}_j^\\top \\bm{y}^\\ast is the optimal expected cost from the next time period, and then on.\n\nWhen yi*y_i^\\ast is known for every state, the optimal action in each state would be\n\nπi*=argminj∈𝒜i(cj+γ𝐩j⊤𝐲*),∀i.\n\\pi_i^\\ast = \\operatorname{arg min}_{j \\in \\mathcal{A}_i} (c_j + \\gamma\n\\bm{p}_j^\\top \\bm{y}^\\ast), \\quad \\forall i."
  },
  {
    "objectID": "02_basic_lp.html#example-7-markov-decision-process-mdp-3",
    "href": "02_basic_lp.html#example-7-markov-decision-process-mdp-3",
    "title": "02_basic_lp",
    "section": "Example 7 – Markov Decision Process (MDP)",
    "text": "Example 7 – Markov Decision Process (MDP)\n\nOne observes that 𝐲*∈ℝm\\bm{y}^\\ast \\in \\mathbb{R}^m is a fixed-point of the Bellman operator, and it can be computed by the following linear program.\n\nmaximize∑i=1myisubject toy1−γ𝐩j⊤𝐲≤cj∀j∈𝒜1,⋮yi−γ𝐩j⊤𝐲≤cj∀j∈𝒜i,⋮ym−γ𝐩j⊤𝐲≤cj∀j∈𝒜m.\n\\begin{align}\n\\operatorname{maximize} & \\sum_{i=1}^m y_i \\\\\n\\text{subject to} & y_1 - \\gamma \\bm{p}_j^\\top \\bm{y} \\leq c_j \\quad \\forall j \\in \\mathcal{A}_1, \\\\\n& \\vdots \\\\\n& y_i - \\gamma \\bm{p}_j^\\top \\bm{y} \\leq c_j \\quad \\forall j \\in \\mathcal{A}_i, \\\\\n& \\vdots \\\\\n& y_m - \\gamma \\bm{p}_j^\\top \\bm{y} \\leq c_j \\quad \\forall j \\in \\mathcal{A}_m. \\\\\n\\end{align}\n\n\nBasically, we relax the “min\\operatorname{min}” operator to “≤\\leq” from Bellman’s principle and make them into the constraints and then maximize the sum of yiy_i’s as the objective.\nWhen the objective is maximized, at least one ineqality constraint in 𝒜i\\mathcal{A}_i must become equal for every state ii so that 𝐲\\bm{y} is a fixed point solution of the Bellman operator."
  },
  {
    "objectID": "02_basic_lp.html#basic-solutions",
    "href": "02_basic_lp.html#basic-solutions",
    "title": "02_basic_lp",
    "section": "Basic Solutions",
    "text": "Basic Solutions\n\n\n\n\n\nSystem of equalities\n\n\n𝐀𝐱=𝐛(2) \\bm{Ax} = \\bm{b}  \\qquad(2)\n\nFrom the nn columns of 𝐀\\bm{A}, we select a set of mm linearly independent columns.\nWLOG, assume that the first mm columns of 𝐀\\bm{A} are selected: denote the m×mm \\times m matrix determined by these columns by 𝐁\\bm{B}.\nThe matrix 𝐁\\bm{B} is nonsingular and we may uniquely solve the equation 𝐁𝐱𝐁=𝐛or𝐱𝐁=𝐁−1𝐛 \\bm{Bx_B} = \\bm{b} \\quad \\text{or} \\quad \\bm{x_B} = \\bm{B}^{-1}\\bm{b} \n\n\n\n\n\nWe refer to 𝐁\\bm{B} as a basis, since 𝐁\\bm{B} consists of mm linearly independent columns that can be regarded as a basis for ℝm\\mathbb{R}^m.\n\n\n\n\n\nDefinition\n\n\nGiven the set of mm simultaneous linear equations nn unknowns Equation 2, let 𝐁\\bm{B} be any nonsingular m×mm \\times m submatrix made up of columns of 𝐀\\bm{A}. Then, if all n−mn-m components of 𝐱\\bm{x} not associated with columns of 𝐁\\bm{B} are set equal to zero, the solution to the resulting set of equations is said to be a basic solution to Equation 2 with respect to basis 𝐁\\bm{B}.\n\n\n\n\n\n\nDefinition\n\n\nThe components of 𝐱\\bm{x} associated with the columns of 𝐁\\bm{B}, denoted by the subvector 𝐱B\\bm{x}_B according to the same column index order in 𝐁\\bm{B}, are called basic variables.\n\n\n\n\n\n\n\n\nFull-Rank Assumption\n\n\nThe m×nm \\times n matrix 𝐀\\bm{A} has m&lt;nm &lt; n, and the mm rows of 𝐀\\bm{A} are linearly independent."
  },
  {
    "objectID": "02_basic_lp.html#basic-feasible-solutions-1",
    "href": "02_basic_lp.html#basic-feasible-solutions-1",
    "title": "02_basic_lp",
    "section": "Basic Feasible Solutions",
    "text": "Basic Feasible Solutions\n\n\n\nDefinition\n\n\nIf one or more of the basic variables in a basic solution has value zero, that solution is said to be a degenerate basic solution.\n\n\n\n\nIn a nondegenerate basic solution, the basic variables, and hence the basis 𝐁\\bm{B}, can be immediately identified from the positive components of the solution.\n\nThere is ambiguity associated with a degenerate basic solution – some of the nonbasic variables can be interchanged!\n\n\n\n\n\nDefinition\n\n\nA vector 𝐱\\bm{x} satisfying\n𝐀𝐱=𝐛,𝐱≥𝟎,(3)\n\\bm{Ax} = \\bm{b}, \\quad \\bm{x} \\geq \\bm{0},\n \\qquad(3)\nis said to be feasible for these constraints. A feasible solution to the constraints Equation 3 that is also basic is said to be a basic feasible solution; if this solution is also a degenerate basic solution, it is called a degenerate basic feasible solution."
  },
  {
    "objectID": "02_basic_lp.html#the-fundamental-theorem-of-lp",
    "href": "02_basic_lp.html#the-fundamental-theorem-of-lp",
    "title": "02_basic_lp",
    "section": "The Fundamental Theorem of LP",
    "text": "The Fundamental Theorem of LP\n\nCorresponding to a linear program in standard form\nminimize𝐜⊤𝐱subject to𝐀𝐱=𝐛,𝐱≥0(4)\n\\begin{align}\n\\operatorname{minimize} & \\bm{c}^\\top \\bm{x}  \\\\\n\\text{subject to} & \\bm{Ax} = \\bm{b}, \\;\\; \\bm{x} \\geq 0\n\\end{align} \n \\qquad(4)\na feasible solution to the constraints that achieves the minimum value of the objective function subject to those constraints is said to be an optimal feasible solution. If this solution is basic, it is an optimal basic feasible solution.\n\n\n\n\n\nFundamental Theorem\n\n\nGiven a linear program in standard form Equation 4 where 𝐀\\bm{A} is an m×nm \\times n matrix of rank mm,\n\nif there is a feasible solution, there is a basic feasible solution;\nif there is an optimal feasible solution, there is an optimal basic feasible solution.\n\n\n\n\n\n\n\n\n\nProof (1)\n\n\nDenote the columns of 𝐀\\bm{A} by 𝐚1,𝐚2,…,𝐚n\\bm{a}_1, \\bm{a}_2, \\ldots, \\bm{a}_n. Suppose 𝐱=(x1,x2,…,xn)\\bm{x} = (x_1, x_2, \\ldots, x_n) is a feasible solution. Then, in terms of the columns of 𝐀\\bm{A}, this solution satisfies\nx1𝐚1+x2𝐚2+⋯+xn𝐚n=𝐛. x_1\\bm{a}_1 + x_2\\bm{a}_2 + \\cdots + x_n\\bm{a}_n = \\bm{b}."
  },
  {
    "objectID": "02_basic_lp.html#the-fundamental-theorem-of-lp-1",
    "href": "02_basic_lp.html#the-fundamental-theorem-of-lp-1",
    "title": "02_basic_lp",
    "section": "The Fundamental Theorem of LP",
    "text": "The Fundamental Theorem of LP\n\n\n\nProof (1) - Continued -\n\n\nAssume that exactly pp of the variables xix_i are greater than zero, and wlog, that they are the first pp variables. Thus\nx1𝐚1+x2𝐚2+⋯+xp𝐚p=𝐛.(5) x_1 \\bm{a}_1 + x_2 \\bm{a}_2 + \\cdots + x_p \\bm{a}_p = \\bm{b}.  \\qquad(5)\nThere are now two cases, corresponding as to whether the set 𝐚1,𝐚2,…,𝐚p\\bm{a}_1, \\bm{a}_2, \\ldots, \\bm{a}_p is linearly independent or linearly dependent.\nCase 1: Assume 𝐚1,𝐚2,…,𝐚p\\bm{a}_1, \\bm{a}_2, \\ldots, \\bm{a}_p are linearly independent. Then clearly, p≤mp \\leq m. If p=mp = m, the solution is basic and the proof is complete. If p&lt;mp &lt; m, then, since 𝐀\\bm{A} has rank mm, m−pm-p vectors can be found from the remaining n−pn-p vectors so that the resulting set of mm vectors is linearly independent. Assigning the value zero to the corresponding m−pm-p variables yields a (degenerate) basic feasible solution.\nCase 2: Assume 𝐚1,𝐚2,…,𝐚p\\bm{a}_1, \\bm{a}_2, \\ldots, \\bm{a}_p are linearly dependent. Then there is a nontrivial linear combination of these vectors that is zero. Thus there are constants y1,y2,…,ypy_1, y_2, \\ldots, y_p, at least one of which can be assumed to be positive, such that\ny1𝐚1+y2𝐚2+⋯+yp𝐚p=𝟎.(6) y_1\\bm{a}_1 + y_2\\bm{a}_2 + \\cdots + y_p\\bm{a}_p = \\bm{0}.  \\qquad(6)\nMultiplying this equation by a scalar ε\\varepsilon and subtracting it from Equation 5, we obtain\n(x1−εy1)𝐚1+(x2−εy2)𝐚2+⋯+(xp−εyp)𝐚p=𝐛.(7) (x_1 - \\varepsilon y_1)\\bm{a}_1 + (x_2 - \\varepsilon y_2)\\bm{a}_2 + \\cdots + \n   (x_p - \\varepsilon y_p)\\bm{a}_p = \\bm{b}.  \\qquad(7)\nThis equation hods for every ε\\varepsilon, and for each ε\\varepsilon the components xj−εyjx_j - \\varepsilon y_j correspond to a solution of the linear equalities — although they may violate xi−εyi≥0x_i - \\varepsilon y_i \\geq 0. Denoting 𝐲=(y1,y2,…,yp,0,0,…,0)\\bm{y} = (y_1, y_2, \\ldots, y_p, 0, 0, \\ldots, 0), we see that for any ε\\varepsilon\n𝐱−ε𝐲(8) \\bm{x} - \\varepsilon \\bm{y}  \\qquad(8)\nis a solution to the equalities. For ε=0\\varepsilon = 0, this reduces to the original feasible solution."
  },
  {
    "objectID": "02_basic_lp.html#the-fundamental-theorem-of-lp-2",
    "href": "02_basic_lp.html#the-fundamental-theorem-of-lp-2",
    "title": "02_basic_lp",
    "section": "The Fundamental Theorem of LP",
    "text": "The Fundamental Theorem of LP\n\n\n\nProof (1) - Continued -\n\n\nAs ε\\varepsilon is increased from zero, the various components increase, decrease, or remain constant, depending upon whether the correspoding yiy_i is negative, positive, or zero. Since we assume at least one yiy_i is positive, at least one component will decrease as ε\\varepsilon is increased. we increase ε\\varepsilon to the first point where one or more components become zero. Specifically, we set\nε=min{xiyi:yi&gt;0}. \\varepsilon = \\operatorname{min}\\left\\{\\frac{x_i}{y_i}: y_i &gt; 0 \\right\\}. \nFor this value of ε\\varepsilon, the solution given by Equation 8 is feasible and has at most p−1p-1 positive variables. Repeating this process as necessary, we can eliminate positive variables until we have a feasible solution with corresponding columns that are linearly independent. At that point Case 1 applies.\n\n\n\n\n\n\n\n\nProof (2)\n\n\nLet 𝐱=(x1,x2,…,xn)\\bm{x} = (x_1, x_2, \\ldots, x_n) be an optimal feasible solution and, as in the proof of (1) above, suppose there are exactly pp positive variables x1,x2,…,xpx_1, x_2, \\ldots, x_p. Again, there are two cases; and Case 1, corresponding to the linear independence is exactly the same as before.\nCase 2 also goes exactly the same as before, but it must be shown that for any ε\\varepsilon the solution Equation 8 is optimal. To show this, note that the value of the solution 𝐱−ε𝐲\\bm{x} - \\varepsilon\\bm{y} is\n𝐜⊤𝐱−ε𝐜⊤𝐲.(9) \\bm{c}^\\top \\bm{x} - \\varepsilon \\bm{c}^\\top \\bm{y}.  \\qquad(9)\nFor ε\\varepsilon sufficiently small in magnitude, 𝐱−ε𝐲\\bm{x} - \\varepsilon \\bm{y} is a feasible solution for positive or negative values of ε\\varepsilon. Thus, we conclude that 𝐜⊤𝐲=𝟎\\bm{c}^\\top \\bm{y} = \\bm{0} (why?).\n\n\n\n\n\n\n\n\nProof (2)\n\n\nLet 𝐱=(x1,x2,…,xn)\\bm{x} = (x_1, x_2, \\ldots, x_n) be an optimal feasible solution and, as in the proof of (1) above, suppose there are exactly pp positive variables x1,x2,…,xpx_1, x_2, \\ldots, x_p. Again, there are two cases; and Case 1, corresponding to the linear independence is exactly the same as before.\nCase 2 also goes exactly the same as before, but it must be shown that for any ε\\varepsilon the solution Equation 8 is optimal. To show this, note that the value of the solution 𝐱−ε𝐲\\bm{x} - \\varepsilon\\bm{y} is\n𝐜⊤𝐱−ε𝐜⊤𝐲.(10) \\bm{c}^\\top \\bm{x} - \\varepsilon \\bm{c}^\\top \\bm{y}.  \\qquad(10)\nFor ε\\varepsilon sufficiently small in magnitude, 𝐱−ε𝐲\\bm{x} - \\varepsilon \\bm{y} is a feasible solution for positive or negative values of ε\\varepsilon. Thus, we conclude that 𝐜⊤𝐲=𝟎\\bm{c}^\\top \\bm{y} = \\bm{0}. For, if 𝐜⊤𝐲≠𝟎\\bm{c}^\\top \\bm{y} \\neq \\bm{0}, an ε\\varepsilon of small magnitude and proper sign could be determined so as to render Equation 10 smaller than 𝐜⊤𝐲\\bm{c}^\\top \\bm{y} while maintaining feasibility."
  },
  {
    "objectID": "02_basic_lp.html#the-fundamental-theorem-of-lp-3",
    "href": "02_basic_lp.html#the-fundamental-theorem-of-lp-3",
    "title": "02_basic_lp",
    "section": "The Fundamental Theorem of LP",
    "text": "The Fundamental Theorem of LP\n\nPart (1) of the theorem is commonly referred to as Carathéodory’s theorem.\nPart (2) of the theorem reduces the task of solving a linear program to that of searching over basic feasible solutions.\nSince for a problem having nn variables and mm constraints there are at most\n\n(nm)=n!m!(n−m)!\n\\binom{n}{m} = \\frac{n!}{m!(n-m)!}\n\nbasic solutions (corresponding to the number of ways of selecting mm of nn columns), there are only a finite number of possibilities.\n\nThus the fundamental theorem yields an obvious, but terribly inefficient, finite search technique.\nBy expanding upon the technique of proof as well as the statement of the fundamental theorem, the efficient simplex procedure is derived."
  },
  {
    "objectID": "02_basic_lp.html#extreme-points",
    "href": "02_basic_lp.html#extreme-points",
    "title": "02_basic_lp",
    "section": "Extreme Points",
    "text": "Extreme Points\n\n\n\nDefinition\n\n\nA point 𝐱\\bm{x} in a convex set CC is said to be an extreme point of CC if there are no two distinct points 𝐱1\\bm{x}_1 and 𝐱2\\bm{x}_2 in CC such that 𝐱=α𝐱1+(1−α)𝐱2\\bm{x} = \\alpha \\bm{x}_1 + (1-\\alpha)\\bm{x}_2 for some α\\alpha, 0&lt;α&lt;10 &lt; \\alpha &lt; 1.\n\n\n\n\nAn extreme point is thus a point that does not lie strictly within a line segment connecting two other points of the set.\n\n\n\n\nTheorem (Equivalence of Extreme Points and Basic Solutions).\n\n\nLet 𝐀\\bm{A} be an m×nm \\times n matrix of rank mm and 𝐛\\bm{b} an mm-vector. Let KK be the convex polytope consisting of all nn-vectors 𝐱\\bm{x} satisfying\n𝐀𝐱=𝐛,𝐱≥𝟎.(11) \\bm{Ax} = \\bm{b}, \\;\\; \\bm{x} \\geq \\bm{0}.  \\qquad(11)\nA vector 𝐱\\bm{x} is an extreme point of KK if and only if 𝐱\\bm{x} is a basic feasible solution to Equation 11.\n\n\n\n\n\n\nProof\n\n\nSuppose first that 𝐱=(x1,x2,…,xm,0,0,…,0)\\bm{x} = (x_1, x_2, \\ldots, x_m, 0, 0, \\ldots, 0) is a basic feasible solution to Equation 11. Then\nx1𝐚1+x2𝐚2+⋯+xm𝐚m=𝐛, x_1 \\bm{a}_1 + x_2\\bm{a}_2 + \\cdots + x_m\\bm{a}_m = \\bm{b}, \nwhere 𝐚1,𝐚2,…,𝐚m\\bm{a}_1, \\bm{a}_2, \\ldots, \\bm{a}_m, the first mm columns of 𝐀\\bm{A} are linearly independent. Suppose that 𝐱\\bm{x} could be expressed as a convex combination of two other points in K; say, 𝐱=α𝐲+(1−α)𝐳\\bm{x} = \\alpha \\bm{y} + (1-\\alpha)\\bm{z}, 0&lt;α&lt;10 &lt; \\alpha &lt; 1, 𝐲≠𝐳\\bm{y} \\neq \\bm{z}."
  },
  {
    "objectID": "02_basic_lp.html#extreme-points-1",
    "href": "02_basic_lp.html#extreme-points-1",
    "title": "02_basic_lp",
    "section": "Extreme Points",
    "text": "Extreme Points\n\n\n\nProof\n\n\nSince all components of 𝐱\\bm{x}, 𝐲\\bm{y}, 𝐳\\bm{z} are nonnegative and since 0&lt;α&lt;10 &lt; \\alpha &lt; 1, it follows immediately that the last n−mn-m components of 𝐲\\bm{y} and 𝐳\\bm{z} are zero. Thus, in particular, we have\ny1𝐚1+y2𝐚2+⋯+ym𝐚m=𝐛 y_1\\bm{a}_1 + y_2\\bm{a}_2 + \\cdots + y_m\\bm{a}_m = \\bm{b} \nand\nz1𝐚1+z2𝐚2+⋯+zm𝐚m=𝐛. z_1\\bm{a}_1 + z_2\\bm{a}_2 + \\cdots + z_m\\bm{a}_m = \\bm{b}. \nSince the vectors 𝐚1,𝐚2,…,𝐚m\\bm{a}_1, \\bm{a}_2, \\ldots, \\bm{a}_m are linearly independent, however, it follows that 𝐱=𝐲=𝐳\\bm{x} = \\bm{y} = \\bm{z} and hence xx is an extreme point of KK.\nConversely, assume that 𝐱\\bm{x} is an extreme point of KK. Let us assume that the nonzero components of 𝐱\\bm{x} are the first kk components. Then\nx1𝐚1+x2𝐚2+⋯+xk𝐚k=𝐛,xi&gt;0,i=1,2,…,k. x_1\\bm{a}_1 + x_2\\bm{a}_2 + \\cdots + x_k\\bm{a}_k = \\bm{b}, \\quad x_i &gt; 0,\n\\quad i = 1, 2, \\ldots, k. \nTo show that 𝐱\\bm{x} is a bsic feasible solution (BFS) it must be shown that the vectors 𝐚1,𝐚2,…,𝐚k\\bm{a}_1, \\bm{a}_2, \\ldots, \\bm{a}_k are linearly independent. We do this by contradiction. Suppose they are linearly dependent. Then there is a nontrivial linear combination that is zero:\ny1𝐚1+y2𝐚2+⋯yk𝐚k=𝟎. y_1\\bm{a}_1 + y_2\\bm{a}_2 + \\cdots y_k\\bm{a}_k = \\bm{0}. \nDefine the nn-vector 𝐲=(y1,y2,…,yk,0,0,…,0)\\bm{y} = (y_1, y_2, \\ldots, y_k, 0, 0, \\ldots, 0). Since xi&gt;0x_i &gt; 0, 1≤i≤k1 \\leq i \\leq k, it is possible to select ε\\varepsilon such that\n𝐱+ε𝐲≥𝟎,𝐱−ε𝐲≥𝟎. \\bm{x} + \\varepsilon \\bm{y} \\geq \\bm{0}, \\quad \\bm{x} - \\varepsilon \\bm{y}\n\\geq \\bm{0}. \nWe then have 𝐱=12(𝐱+ε𝐲)+12(𝐱−ε𝐲)\\bm{x} = \\frac{1}{2}(\\bm{x}+\\varepsilon\\bm{y}) + \\frac{1}{2}(\\bm{x}-\\varepsilon\\bm{y}) which expresses 𝐱\\bm{x} as a convex combination of two distinct vectors in KK."
  },
  {
    "objectID": "02_basic_lp.html#extreme-points-2",
    "href": "02_basic_lp.html#extreme-points-2",
    "title": "02_basic_lp",
    "section": "Extreme Points",
    "text": "Extreme Points\n\n\n\nCorollary\n\n\nIf the convex set KK corresponding to Equation 11 is nonempty, it has at least one extreme point.\n\n\n\n\n\n\nCorollary\n\n\nIf there is a finite optimal solution to a linear programming problem, there is a finite optimal solution which is an extreme point of the constraint set.\n\n\n\n\n\n\nCorollary\n\n\nThe constraint set KK corresponding to Equation 11 possesses at most a finite number of extreme points and each of them is finite.\n\n\n\n\n\n\nCorollary\n\n\nIf the convex polytope KK corresponding to Equation 11 is bounded, then KK is a convex polyhedron, that is, KK consists of points that are convex combinations of a finite number of points."
  },
  {
    "objectID": "02_basic_lp.html#convex-geometry-examples",
    "href": "02_basic_lp.html#convex-geometry-examples",
    "title": "02_basic_lp",
    "section": "Convex Geometry – Examples",
    "text": "Convex Geometry – Examples\n\n\n\n\n\n\nConsider the constraint set in ℝ3\\mathbb{R}^3 defined by\nx1+x2+x3=1,x1,x2,x3≥0.\n\\begin{align}\nx_1 + x_2 + x_3 &= 1, \\\\\nx_1, x_2, x_3 &\\geq 0.\n\\end{align}\n\n\nThis set is illustrated in the figure.\nIt has three extreme points, corresponding to the three basic solutions to x1+x2+x3=1x_1 + x_2 + x_3 = 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConsider the constraint set in ℝ3\\mathbb{R}^3 defined by\nx1+x2+x3=1,2x1+3x2=1,x1,x2,x3≥0.\n\\begin{align}\nx_1 + x_2 + x_3 &= 1, \\\\\n2x_1 + 3x_2 &= 1, \\\\\nx_1, x_2, x_3 &\\geq 0.\n\\end{align}\n\n\nThis set is illustrated in the figure.\nIt has two extreme points, corresponding to the two basic feasible solutions.\nNote that the system of equations itself has three basic solutions: (2,−1,0),(12,0,12),(0,13,23), (2, -1, 0), (\\frac{1}{2}, 0, \\frac{1}{2}), (0, \\frac{1}{3}, \\frac{2}{3}),  the first of which is not feasible."
  },
  {
    "objectID": "02_basic_lp.html#convex-geometry-examples-1",
    "href": "02_basic_lp.html#convex-geometry-examples-1",
    "title": "02_basic_lp",
    "section": "Convex Geometry – Examples",
    "text": "Convex Geometry – Examples\n\n\n\n\n\n\nConsider the constraint set in ℝ2\\mathbb{R}^2 defined by\nx1+83x2≤4,x1+x2≤2,2x1≤3,x1,x2≥0.\n\\begin{align}\nx_1 + \\frac{8}{3}x_2 &\\leq 4, \\\\\nx_1 + x_2 &\\leq 2, \\\\\n2x_1 &\\leq 3, \\\\\nx_1, x_2 &\\geq 0.\n\\end{align}\n\n\nThe set has five extreme points.\nIn order to compare this example, we must introduce slack variables to yield the equivalent set in ℝ5\\mathbb{R}^5.\n\nx1+83x2+x3=4,x1+x2+x4=2,2x1+x5=3x1,x2,x3,x4,x5≥0.\n\\begin{align}\nx_1 + \\frac{8}{3}x_2 + x_3 &= 4, \\\\\nx_1 + x_2 + x_4 &= 2, \\\\\n2_x1 + x_5 &= 3 \\\\\nx_1, x_2, x_3, x_4, x_5 \\geq 0.\n\\end{align}\n\n\n\n\n\n\n\n\nA basic solution for this system is obtained by setting any two variables to zero and solving for the remaining three.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA basic solution for this system is obtained by setting any two variables to zero and solving for the remaining three.\nAs indicated in the figure, each edge of the figure corresponds to one variable being zero, and the extreme points are the points where two variables are zero.\n\n\n\n\n\n\n\n\nEven when not expressed in the standard form, the extreme points of the set defined by the constraints of a lienar program correspond to the possible solution points.\nThe level sets of an objective function −2x1−x2-2x_1 - x_2 is included in the bottom figure.\n\nAs the level varies, different parallel lines are obtained.\nThe optimal value of the linear program is the smallest value of this level for which the corresponding line has a point in common with the feasible set.\nIn the figure, this occurs at the point (32,12)(\\frac{3}{2}, \\frac{1}{2}) with the level z=−72z = -\\frac{7}{2}."
  },
  {
    "objectID": "02_basic_lp.html#infeasibility-certificates",
    "href": "02_basic_lp.html#infeasibility-certificates",
    "title": "02_basic_lp",
    "section": "(In)feasibility Certificates",
    "text": "(In)feasibility Certificates\n\n\n\nTheorem (Farkas’s Lemma).\n\n\nLet 𝐀\\bm{A} be an m×nm \\times n matrix and 𝐛\\bm{b} be an mm-vector. The system of constraints 𝐀𝐱=𝐛,𝐱≥𝟎(12) \\bm{Ax} = \\bm{b}, \\quad \\bm{x} \\geq \\bm{0}  \\qquad(12) has a feasible solution 𝐱\\bm{x} if and only if the system of constraints −𝐲⊤𝐀≥𝟎,𝐲⊤𝐛=1(or&gt;0)(13) -\\bm{y}^\\top \\bm{A} \\geq \\bm{0}, \\quad \\bm{y}^\\top \\bm{b} = 1 (\\text{or} &gt; 0)\n \\qquad(13) has no feasible solution 𝐲\\bm{y}. Therefore a single feasible solution 𝐲\\bm{y} for system Equation 13 establishes an infeasibility certificate for the system Equation 12.\n\n\n\n\nThe two systems, Equation 12 and Equation 13, are called alternative systems: one of them is feasible and the other is infeasible.\n\n\n\n\nExample 1\n\n\nSuppose 𝐀=[11],𝐛=−1\\bm{A} = \\begin{bmatrix} 1 & 1 \\end{bmatrix}, \\quad \\bm{b} = -1. Then, y=−1y = -1 is feasible for system Equation 13, which proves that the system Equation 12 is infeasible."
  },
  {
    "objectID": "02_basic_lp.html#infeasibility-certificates-1",
    "href": "02_basic_lp.html#infeasibility-certificates-1",
    "title": "02_basic_lp",
    "section": "(In)feasibility Certificates",
    "text": "(In)feasibility Certificates\n\n\n\nLemma\n\n\nLet CC be the cone generated by the columns of matrix 𝐀\\bm{A}, that is C={𝐀𝐱∈ℝm:𝐱≥𝟎}. C = \\{\\bm{Ax} \\in \\mathbb{R}^m: \\bm{x} \\geq \\bm{0}\\}.  Then C is a closed and convex set.\n\n\n\n\n\n\nProof (of Farkas’s Lemma).\n\n\nLet the system Equation 12 have a feasible solution, say 𝐱‾\\bar{\\bm{x}}. Then, the system Equation 13 must be infeasible, since, otherwise, we have a contradiction\n0&lt;𝐲⊤𝐛=𝐲⊤(𝐀𝐱‾)=(𝐲⊤𝐀)𝐱‾≤0, 0 &lt; \\bm{y}^\\top \\bm{b} = \\bm{y}^\\top(\\bm{A}\\bar{\\bm{x}}) = (\\bm{y}^\\top\n\\bm{A})\\bar{\\bm{x}} \\leq 0, \nfrom 𝐱‾≥𝟎\\bar{\\bm{x}} \\geq \\bm{0} and 𝐲⊤𝐀≤𝟎\\bm{y}^\\top \\bm{A} \\leq \\bm{0}.\nNow, let the system Equation 12 have no feasible solution, that is, 𝐛∉C:={𝐀𝐱:𝐱≥0}\\bm{b} \\notin C := \\{\\bm{Ax}: \\bm{x} \\geq 0\\}. We now prove that its alternative system Equation 13 must have a feasible solution.\nSince points 𝐛\\bm{b} is not in CC and CC is a closed convex set, by the separating hyperplane theorem, there is a 𝐲\\bm{y} such that 𝐲⊤𝐛&gt;sup𝐜∈C𝐲⊤𝐜. \\bm{y}^\\top\n\\bm{b} &gt; \\operatorname{sup}_{\\bm{c} \\in C} \\bm{y}^\\top \\bm{c}.  But we know that 𝐜=𝐀𝐱\\bm{c} = \\bm{Ax} for some 𝐱≥𝟎\\bm{x} \\geq \\bm{0}, so we have 𝐲⊤𝐛&gt;sup𝐱≥𝟎𝐲⊤𝐀𝐱=sup𝐱≥𝟎(𝐲⊤𝐀)𝐱.(14) \\bm{y}^\\top \\bm{b} &gt; \\operatorname{sup}_{\\bm{x}\\geq\\bm{0}} \\bm{y}^\\top\n\\bm{Ax} = \\operatorname{sup}_{\\bm{x} \\geq \\bm{0}} (\\bm{y}^\\top \\bm{A})\\bm{x}.  \\qquad(14) Setting 𝐱=𝟎\\bm{x} = \\bm{0}, we have 𝐲⊤𝐛&gt;0\\bm{y}^\\top \\bm{b} &gt; 0 from inequality Equation 14."
  },
  {
    "objectID": "02_basic_lp.html#infeasibility-certificates-2",
    "href": "02_basic_lp.html#infeasibility-certificates-2",
    "title": "02_basic_lp",
    "section": "(In)feasibility Certificates",
    "text": "(In)feasibility Certificates\n\n\n\nProof (of Farkas’s Lemma) - Continued -\n\n\nFurthermore, inequality Equation 14 also implies 𝐲⊤𝐀≤𝟎\\bm{y}^\\top \\bm{A} \\leq \\bm{0}. Since otherwise, say the first entry of 𝐲⊤𝐀\\bm{y}^\\top \\bm{A}, (𝐲⊤𝐀)1(\\bm{y}^\\top \\bm{A})_1, is positive. We can then choose a vector 𝐱‾≥𝟎\\bar{\\bm{x}} \\geq \\bm{0} such that\nx‾1=α&gt;0,x‾2=⋯=x‾n=0. \\bar{x}_1 = \\alpha &gt; 0, \\bar{x}_2 = \\cdots = \\bar{x}_n = 0. \nThen, from this choice, we have\nsup𝐱≥𝟎(𝐲⊤𝐀)𝐱≥(𝐲⊤𝐀)𝐱‾=α(𝐲⊤𝐀)1. \\operatorname{sup}_{\\bm{x} \\geq \\bm{0}} (\\bm{y}^\\top\\bm{A})\\bm{x} \\geq\n(\\bm{y}^\\top \\bm{A})\\bar{\\bm{x}} = \\alpha(\\bm{y}^\\top \\bm{A})_1. \nThis tends to ∞\\infty as α→∞\\alpha \\rightarrow \\infty. This is a contradiction because (𝐲⊤𝐀)𝐱‾(\\bm{y}^\\top\\bm{A})\\bar{\\bm{x}} should be bounded from above by inequality Equation 14. Therefore, 𝐲\\bm{y} identified in the separating hyperplane theorem is a feasible solution to system Equation 13. Finally, we can always scale 𝐲\\bm{y} such that 𝐲⊤𝐛=1\\bm{y}^\\top \\bm{b} = 1.\n\n\n\n\n\n\nGeometric Interpretation\n\n\nIf 𝐛\\bm{b} is not in the closed and convex cone generated by the columns of the matrix 𝐀\\bm{A}, then there must be a hyperplane separating 𝐛\\bm{b} and the cone, and the feasible solution 𝐲\\bm{y} to the alternative system is the slope-vector of the hyperplane."
  },
  {
    "objectID": "02_basic_lp.html#variant-of-farkass-lemma",
    "href": "02_basic_lp.html#variant-of-farkass-lemma",
    "title": "02_basic_lp",
    "section": "Variant of Farkas’s Lemma",
    "text": "Variant of Farkas’s Lemma\n\n\n\nCorollary\n\n\nLet 𝐀\\bm{A} be an m×nm \\times n matrix and 𝐜\\bm{c} an nn-vector. The system of constraints\n𝐀⊤𝐲≤c(15) \\bm{A}^\\top \\bm{y} \\leq c  \\qquad(15)\nhas a feasible solution 𝐲\\bm{y} if and only if the system of constraints\n𝐀𝐱=𝟎,𝐱≥𝟎,𝐜⊤𝐱=−1(or&lt;0)(16) \\bm{Ax} = \\bm{0}, \\quad \\bm{x} \\geq \\bm{0}, \\quad \\bm{c}^\\top \\bm{x} = -1 \\;\n(\\text{or} &lt; 0)  \\qquad(16)\nhas no feasible solution 𝐱\\bm{x}. Therefore a single feasible solution 𝐱\\bm{x} for system Equation 16 establishes an infeasibility certificate for the system Equation 15.\n\n\n\n\n\nOptimization Theory and Practice • Aykut C. Satici"
  }
]