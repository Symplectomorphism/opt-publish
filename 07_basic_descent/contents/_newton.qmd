# Newton's Method: <br> Second-Order

<br>

$$ f(\bm{x}) \simeq f(\bm{x}_k) + \nabla f(\bm{x}_k)(\bm{x} - \bm{x}_k) +
\frac{1}{2} (\bm{x} - \bm{x}_k)^\top \bm{F}(\bm{x}_k)(\bm{x} - \bm{x}_k). $$

The rhs is minimized at 

$$ \boxed{\bm{x}_{k+1} = \bm{x}_k - [\bm{F}(\bm{x}_k)]^{-1} \nabla f(\bm{x}_k)^\top.} $$ {#eq-newton-method}



## Order Two Convergence {.smaller}

<br>

::: {.callout-tip icon=false}
## Theorem (Newton's Method)
Let $f \in C^3$ on $\mathbb{R}^n$ and assume that at the local minimum point
$\bm{x}^\ast$, the Hessian $\bm{F}(\bm{x}^\ast)$ is positive definite. Then if
started sufficiently close to $\bm{x}^\ast$, the points generated by Newton's
method converge to $\bm{x}^\ast$. The order of convergence is at least two.
:::

::: {.callout-note icon=false}
## Proof
There are $\rho, \beta_1, \beta_2 > 0$ such that for all $\bm{x}$ with $|\bm{x}
- \bm{x}^\ast < \rho$, there holds $|\bm{F}(\bm{x})^{-1}| < \beta_1$ and
$|\nabla f(\bm{x^\ast})^\top - \nabla f(\bm{x})^\top -
\bm{F}(\bm{x})(\bm{x}^\ast - \bm{x})| \leq \beta_2 |\bm{x} - \bm{x}^\ast|^2$.
now suppose $\bm{x}_k$ is selected with $\beta_1\beta_2|\bm{x}_k - \bm{x}^\ast|
< 1$ and $|\bm{x}_k - \bm{x}^\ast| < \rho$. Then

$$
\begin{align}
|\bm{x}_{k+1} - \bm{x}^\ast| &= |\bm{x}_k - \bm{x}^\ast -
\bm{F}(\bm{x}_k)^{-1}\nabla f(\bm{x}_k)^\top| = |\bm{F}(\bm{x}_k)^{-1}[\nabla
f(\bm{x}^\ast)^\top - \nabla f(\bm{x}_k) - \bm{F}(\bm{x}_k)(\bm{x}^\ast -
\bm{x}_k)]| \\ &\leq |\bm{f}(\bm{x}_k)^{-1}|\beta_2|\bm{x}_k - \bm{x}^\ast|^2
\leq \beta_1\beta_2|\bm{x}_k - \bm{x}^\ast|^2 < |\bm{x}_k - \bm{x}^\ast|.
\end{align}
$$

The final inequality shows that the new point is closer to $\bm{x}^\ast$ than
the old point, and hence all conditions apply again to $\bm{x}_{k+1}$.
The previous inequality establishes that the convergence is second order.
:::


## Modifications {.smaller}

::: {.callout-important appearance="minimal"}
Although Newton's method is very attractive in terms of its convergence
properties near the solution, it requires modification before it can be used at
points that are remote from the solution.
:::

:::: {.columns}

::: {.column width="50%"}

::: {.callout-warning icon=false}
## Damping
A search parameter $\alpha$ is introduced 
$$ \bm{x}_{k+1} = \bm{x}_k - \alpha_k \bm{F}(\bm{x}_k)^{-1}\nabla
f(\bm{x}_k)^\top, $$
where $\alpha_k$ is selected to minimize $f$.
:::

::: {.callout-warning icon=false}
## Positive Definiteness and Scaling
General class of algorithms is given by 
$$ \bm{x}_{k+1} = \bm{x}_k + \alpha \bm{d}_k = \bm{x}_k - \alpha \bm{M}_k \bm{g}_k, $$ {#eq-general}

* SD: $\bm{M}_k = \bm{I}$, Newton: $\bm{M}_k = \bm{F}(\bm{x}_k)^{-1}$.

For small $\alpha$, it can be shown that $$ f(\bm{x}_{k+1}) = f(\bm{x}_k) -
\alpha \bm{g}_k^\top \bm{M}_k \bm{g}_k + O(\alpha^2). $$

* As $\alpha \rightarrow 0$, the second term on the rhs dominates the third.
* To guarantee a descrese in $f$, we must have $\bm{g}_k^\top \bm{M}_k \bm{g}_k
> 0$.
  - Simplest way to ensure this is to require $\bm{M}_k \succ \bm{0}$.
:::

:::

::: {.column width="50%"}


::: {.callout-warning icon=false}
## General Problems
* In practice, Newton's method must be modified to accommodate the possible
nonpositive definiteness at regions remote from the solution.

* Common approach: $\bm{M}_k = [\mu_k\bm{I} +
\bm{F}(\bm{x}_k)]^{-1}$ for some $\mu_k > 0$.

* This can be regarded as a compromise between SD ($\mu_k$ very large) and
Newton's method ($\mu_k = 0$).

__Levenberg-Marquardt__ performs Cholesky factorization for a given value of
$\mu_k$ as follows $$\mu_k \bm{I} + \bm{F}(\bm{x}_k) = \bm{G}\bm{G}^\top. $$

This checks for positive definiteness (not positive definite if factorization
fails).

If the factorization breaks down $\mu_k$ is increased.

Step direction is found by solving $\bm{G}\bm{G}^\top \bm{d}_k = -\bm{g}_k$.

:::


:::

::::
