# The Method of Steepest Descent: First-Order

Often the first tried method on a new problem.


## The Method and Convergence {.smaller}

:::: {.columns}

::: {.column width="50%"}

::: {.callout-important appearance="minimal"}
* The gradient $\nabla f(\bm{x})$ is defined as a $n$-dim. _row_ vector.
* We define the $n$-dim. _column_ vector $g(\bm{x}) = \nabla f(\bm{x})^\top$.
:::

::: {.callout-tip icon=false}
## The Method of Steepest Descent (SDM)

The iterative algorithm is 

$$ \bm{x}_{k+1} = \bm{x}_k - \alpha_k \bm{g}_k, $$

where the stepsize $\alpha_k$ is a nonnegative scalar possible minimizing
$f(\bm{x}_k - \alpha \bm{g}_k)$.
:::

::: {.callout-warning icon=false}
## The Algorithm
Define the mapping $\phantom{1} \bm{S}: \mathbb{R}^{2n} \rightarrow
\mathbb{R}^n$ by $$ \bm{S}(\bm{x}, \bm{d}) = \{\bm{y}: \bm{y} = \bm{x} + \alpha
\bm{d}, \;\; \alpha \geq 0, \; f(\bm{y}) =
\operatorname{min}_{0 \leq \alpha < \infty} f(\bm{x} + \alpha \bm{d}) \}. $$

This is a closed map if $\bm{d} \neq \bm{0}$. 

The overall algorithm is $\; \bm{A}: \mathbb{R}^n \rightarrow
\mathbb{R}^n$ which gives $\bm{x}_{k+1} \in \bm{A}(\bm{x}_k)$ can be decomposed
in the form 

$$ \bm{A} = \bm{SG}, \quad \bm{G}(\bm{x}) = (\bm{x}, -\bm{g}(\bm{x})). $$
:::


:::

::: {.column width="50%"}

::: {.callout-important icon=false}
## Global Convergence

* Define the solution set $\Gamma = \{\bm{x} \in \mathbb{R}^n: \nabla f(\bm{x})
= \bm{0}\}$.
* $Z(\bm{x}) = f(\bm{x})$ is a descent function for $\bm{A}$, since for $\nabla
f(\bm{x}) \neq \bm{0}$
$$ \operatorname{min}_{0 \leq \alpha < \infty} f(\bm{x} - \alpha \bm{g}(\bm{x}))
< f(\bm{x}). $$

* Thus by the Global Convergence Theorem, if the sequence $\{\bm{x}_k\}$ is
bounded, it will have limit points and each of these is a solution.
:::

::: {.callout-tip icon=false}
## Definition (First-order $\beta$-Lipschitz Function)
For any two points $\bm{x}$ and $\bm{y}$, $|\nabla f(\bm{y}) - \nabla f(\bm{x})|
\leq \beta |\bm{y} - \bm{x}|$, for a positive real number $\beta$.
:::

::: {.callout-important icon=false}
## Convergence Speed for fixed stepsize $\alpha_k = \frac{1}{\beta}$ is arithmetic.
We may not know $\frac{1}{\beta}$ so we employ a _backtracking
line search_: start with a guess of $\beta$; if sufficient obj. reduction is
achieved, halve $\beta$; otherwise double $\beta$. Stop when the process is
reversed.
:::

:::

::::


## Steepest Descent Analysis {.smaller}

:::: {.columns}

::: {.column width="50%"}
::: {.callout-tip appearance="minimal"}
__Lemma__.
Let $f(\bm{x})$ be differentiable everywhere and satisfy the (first-order)
$\beta$-Lipschitz condition. Then, for any two points $\bm{x}$ and $\bm{y}$ 
$$ f(\bm{y}) - f(\bm{x}) - \nabla f(\bm{x})(\bm{y} - \bm{x}) \leq
\frac{\beta}{2}|\bm{y} - \bm{x}|^2. $$
:::

::: {.callout-tip icon=false}
## Steepest Descent &mdash; Lipschitz Convex Case.
Let $f(\bm{x})$ be convex and differentiable everywhere, satisfy the
(first-order) $\beta$-Lipschitz condition, and admit a minimizer $\bm{x}^\ast$.
Then, the method of steepest descent
$$ \bm{x}_{k+1} = \bm{x}_k - \frac{1}{\beta}\bm{g}_k = \bm{x}_k -
\frac{1}{\beta}\nabla f(\bm{x}_k)^\top $$
generates a sequence of solutions $\bm{x}_k$ such that 
$$ |\nabla f(\bm{x}_k)| \leq \frac{\beta}{\sqrt{k(k+1)}}|\bm{x}_0 -
\bm{x}^\ast|, $$ and 
$$ f(\bm{x}_k) - f^\ast \leq \frac{\beta}{2(k+1)}|\bm{x}_0 - \bm{x}^\ast|^2. $$
:::

::: {.callout-note icon=false}
## Proof
Consider the function $g_\bm{x}(\bm{y}) = f(\bm{y}) - \nabla f(\bm{x})\bm{y}$ for any
given $\bm{x}$.
:::

:::

::: {.column width="50%"}

::: {.callout-note appearance="minimal"}
Note that $g_\bm{x}$ is convex and satisfies the $\beta$-Lipschitz condition.
Moreover $\bm{x}$ is the minimizer of $g_\bm{x}(\bm{y})$ and $\nabla
g_\bm{x}(\bm{y}) = \nabla f(\bm{y}) - \nabla f(\bm{x})$.

Applying the Lemma to $g_\bm{x}$ and noting the relations of $g_\bm{x}$ and
$f(\bm{x})$, we have

$$
\begin{align}
&f(\bm{x}) - f(\bm{y}) - \nabla f(\bm{x})(\bm{x} - \bm{y}) = g_\bm{x}(\bm{x}) -
g_\bm{x}(\bm{y}) \\
&\phantom{123} \leq g_\bm{x}(\bm{y} - \frac{1}{\beta}\nabla g_\bm{x}(\bm{y})) -
g_\bm{x}(\bm{y}) \\
&\phantom{123}\leq \nabla g_{\bm{x}}(\bm{y})(-\frac{1}{\beta}\nabla g_\bm{x}(\bm{y})^\top) +
\frac{\beta}{2}\frac{1}{\beta^2}|\nabla g_\bm{x}(\bm{y})|^2 \\
&\phantom{123} = -\frac{1}{2\beta}|\nabla g_{\bm{x}}(y)|^2 = -\frac{1}{2\beta}|\nabla
f(\bm{x}) - \nabla f(\bm{y})|^2.
\end{align}
$$ {#eq-8.20}

Similarly, we have 

$$ f(\bm{y}) - f(\bm{x}) - \nabla f(\bm{y})(\bm{y} - \bm{x}) \leq
-\frac{1}{2\beta}|\nabla f(\bm{x}) - \nabla f(\bm{y})|^2. $$

Adding the above two derived inequalities, we have for any $\bm{x}$ and $\bm{y}$

$$
(\nabla f(\bm{x}) - \nabla f(\bm{y}))(\bm{x} - \bm{y}) \geq
\frac{1}{\beta}|\nabla f(\bm{x}) - \nabla f(\bm{y})|^2.
$$ {#eq-8.21}

For simplicity, let $\bm{d}_k = \bm{x}_k - \bm{x}^\ast$ and $\delta_k =
f(\bm{x}_k) - f(\bm{x}^\ast) \geq 0$.
:::

:::

::::


## Steepest Descent Analysis {.smaller}

:::: {.columns}

::: {.column width="50%"}

::: {.callout-note icon=false}
## Proof -- Continued --
Now let $\bm{x} = \bm{x}_{k+1}$ and $\bm{y} = \bm{x}_k$ in @eq-8.21. Then

<span style="font-size:95%">
$$ -\frac{1}{\beta} \bm{g}_k^\top (\bm{g}_{k+1} - \bm{g}_k) = (\bm{x}_{k+1} -
\bm{x}_k)^\top (\bm{g}_{k+1} - \bm{g}_k) \geq \frac{1}{\beta}|\bm{g}_{k+1} -
\bm{g}_k|^2, $$
</span>

which leads to 

$$
|\bm{g}_{k+1}|^2 \leq \bm{g}_{k+1}^\top \bm{g}_k \leq |\bm{g}_{k+1}||\bm{g}_k|,
\;\; \text{i.e.,} \;\; |\bm{g}_{k+1}| \leq |\bm{g}_k|.
$$ {#eq-8.22}

This inequality implies that $|\bm{g}_k| = |\nabla f(\bm{x}_k)|$ is monotonically
decreasing.

Applying inequality @eq-8.20 for $\bm{x} = \bm{x}_k$ and $\bm{y} = \bm{x}^\ast$
and noting $\bm{g}^\ast = \bm{0}$ we have

$$
\begin{align}
\delta_k &\leq \bm{g}_k^\top \bm{d}_k - \frac{1}{2\beta}|\bm{g}_k|^2 \\
&= -\beta(\bm{x}_{k+1} - \bm{x}_k)\bm{d}_k - \frac{\beta}{2}|\bm{x}_{k+1} -
\bm{x}_k|^2 \\
&= -\frac{\beta}{2}(|\bm{x}_{k+1} - \bm{x}_k|^2 + 2(\bm{x}_{k+1} -
\bm{x}_k)^\top \bm{d}_k) \\
&= -\frac{\beta}{2}(|\bm{d}_{k+1} - \bm{d}_k|^2 + 2(\bm{d}_{k+1} - \bm{d}_k)^\top
\bm{d}_k) \\
&= \frac{\beta}{2}(|\bm{d}_k|^2 - |\bm{d}_{k+1}|^2).
\end{align}
$$ {#eq-8.23}
:::

:::

::: {.column width="50%"}
::: {.callout-note appearance="minimal"}
Summing up @eq-8.23 from $0$ to $k$, we have

$$
\sum_{l=0}^k \delta_l \leq \frac{\beta}{2}(|\bm{d}_0|^2 - |\bm{d}_{k+1}|^2) \leq
\frac{\beta}{2}|\bm{d}_0|^2.
$$ {#eq-8.24}

Using @eq-8.20 again for $\bm{x} = \bm{x}_{k+1}$ and $\bm{y} = \bm{x}_k$ and noting
the SD rule we have

$$
\begin{align}
\delta_{k+1} - \delta_k &= f(\bm{x}_{k+1}) - f(\bm{x}_k) \\
&\leq \bm{g}_{k+1}^\top (-\frac{1}{\beta}\bm{g}_k) -
\frac{1}{2\beta}|\bm{g}_{k+1} - \bm{g}_k|^2 \\
&= -\frac{1}{2\beta}(|\bm{g}_{k+1}|^2 + |\bm{g}_k|^2).
\end{align}
$$ {#eq-8.25}

Noting that @eq-8.25 holds for all $k$, we have

<span style="font-size:95%">
$$
\begin{align}
\sum_{l=0}^k \delta_l &= \sum_{l=0}^k \delta_l (l + 1 - l) = \sum_{l=0}^k
\delta_l(l+1) - \sum_{l=0}^k \delta_l l \\
&= \sum_{l=1}^{k+1} \delta_{l-1}l - \sum_{l=1}^k \delta_ll = \delta_k (k+1) +
\sum_{l=1}^k (\delta_{l-1} - \delta_l)l \\
&\geq \delta_k(k+1) + \sum_{l=1}^k \frac{l}{2\beta}(|\bm{g}_l^2| + |\bm{g}_{l-1}|^2) \\
&\geq \delta_k (k+1) + \frac{k(k+1)}{2\beta}|\bm{g}_k|^2,
\end{align}
$$
</span>

where the last inequality comes from the fact that $|\bm{g}_k| = |\nabla
f(\bm{x}_k)|$ is monotonically decreasing.

:::
:::

::::


## Steepest Descent Analysis 

::: {.callout-note icon=false}
## Proof -- Continued --
Using @eq-8.24 we finally have 

$$
(k+1)\delta_k + \frac{k(k+1)}{2\beta} |\bm{g}_k|^2 \leq
\frac{\beta}{2}|\bm{d}_0|^2
$$ {#eq-8.26}

Inequality @eq-8.26, from $\delta_k = f(\bm{x}_k) - f(\bm{x}^\ast) \geq 0$ and
$\bm{d}_0 = \bm{x}_0 - \bm{x}^\ast$, proves the desired bounds.
:::



## The Quadratic Case {.smaller}

::: {.callout-important appearance="minimal"}
When $f(\bm{x})$ is strongly convex, the convergence speed can be increased from
arithmetic to geometric or linear convergence.
:::


:::: {.columns}

::: {.column width="50%"}

::: {.callout-warning appearance="minimal"}
We focus on the quadratic problem 

$$ f(\bm{x}) = \frac{1}{2}\bm{x}^\top \bm{Q} \bm{x} - \bm{x}^\top \bm{b}, \;\;
\bm{Q} \succ \bm{0}. $$ {#eq-quadratic}

* The unique minimum of $f$ can be found directly by setting the gradient to
zero: $$ \bm{Q}\bm{x}^\ast = \bm{b}. $$
* Let's introduce the function $$ E(\bm{x}) = \frac{1}{2}(\bm{x} -
\bm{x}^\ast)\bm{Q}(\bm{x} - \bm{x}^\ast), $$ so that we have $E(\bm{x}) =
f(\bm{x}) + \frac{1}{2}\bm{x}^\ast \bm{Q} \bm{x}^\ast$. Hence
$\operatorname{min} f \; \Leftrightarrow \; \operatorname{min} E$.

* The method of SD can be expressed as $\bm{x}_{k+1} = \bm{x}_k - \alpha_k
\bm{g}_k$, where $\bm{g}_k = \bm{Q}\bm{x}_k - \bm{b}$ and $\alpha_k$ minimizes
$f(\bm{x}_k - \alpha \bm{g}_k)$.
* We have by definition @eq-quadratic
$$ f(\bm{x}_k -\alpha \bm{g}_k) = \frac{1}{2}(\bm{x}_k - \alpha \bm{g}_k)^\top
\bm{Q} (\bm{x}_k - \alpha \bm{g}_k) - (\bm{x}_k - \alpha \bm{g}_k)^\top \bm{b}.
$$

This is minimized at $$ \alpha_k = \frac{\bm{g}_k^\top \bm{g}_k}{\bm{g}_k^\top
\bm{Q} \bm{g}_k}. $$
:::

:::

::: {.column width="50%"}
<center>
<img src="./contents/assets/steepestdesc.png" width="70%" /img>
</center>

::: {.callout-note appearance="minimal"}
* The function $f$ and the SD process is illustrated by showing the contours of
constant values of $f$ and a typical sequence developed by the process.

* The contours of $f$ are $n$-dim. ellipsoids with axes in the directions of the
$n$-mutually orthogonal eigenvectors of $\bm{Q}$.

* The axis corresponding to the $i^{\text{th}}$ eigenvector has length
proportional to $\frac{1}{\lambda_i}$.
:::

:::

::::


## Nonquadratic Case {.smaller}

::: {.callout-important appearance="minimal"}
For nonquadratic functions, SD still does well if the condition number is
modest. To establish estimates, assume that the Hessian matrix is positive
definite: $\alpha \bm{I} \leq \bm{F}(\bar{\bm{x}}) \leq A\bm{I}$.
:::

:::: {.columns}

::: {.column width="50%"}

::: {.callout-warning icon=false}
## Exact Line Search
$$ f(\bm{x}_k - \alpha g(\bm{x}_k)) \leq f(\bm{x}_k) - \alpha
\bm{g}(\bm{x}_k)^\top \bm{g}(\bm{x}_k) +
\frac{A\alpha^2}{2}\bm{g}(\bm{x}_k)^\top \bm{g}(\bm{x}_k). $$

Minimizing both sides w.r.t. $\alpha$ yields
$$ f(\bm{x}_{k+1}) \leq f(\bm{x}_k) - \frac{1}{2A}|\bm{g}(\bm{x}_k)|^2. $$

Subtracting the optimal value $f^\ast = f(\bm{x}^\ast)$ from both sides
$$ f(\bm{x}_{k+1}) - f^\ast \leq f(\bm{x}_k) - f^\ast -
\frac{1}{2A}|\bm{g}(\bm{x}_k)|^2. $$ {#eq-A}

Similarly, for any $\bm{x}$ there holds
$$ f(\bm{x}) \geq f(\bm{x}_k) + \bm{g}(\bm{x}_k)^\top (\bm{x} - \bm{x}_k) +
\frac{a}{2} | \bm{x} - \bm{x}_k |^2. $$
:::

:::

::: {.column width="50%"}


::: {.callout-warning appearance="minimal"}
Again, minimize both sides: the left-hand side is minimized at $f^\ast$ and the
right-hand side is minimized at $\bar{\bm{x}} = \bm{x}_k -
\frac{\bm{g}(\bm{x}_k)}{a}$. Subsituting this $\bar{\bm{x}}$ in the right-hand
side gives
$$ f^\ast \geq f(\bm{x}_k) - \frac{1}{2a}|\bm{g}(\bm{x}_k)|^2. $$ {#eq-a}

From @eq-a, we have 
$$ - |\bm{g}(\bm{x}_k)|^2 \leq 2a [f^\ast - f(\bm{x}_k)]. $$

Substituting this in @eq-A gives 
$$ f(\bm{x}_{k+1}) - f^\ast \leq (1-\frac{a}{A})[f(\bm{x}_k - f^\ast)]. $$

This shows that SD makes progress even when it is not close to the solution.
:::

:::

::::


::: {.callout-tip appearance="minimal"}
__Theorem__. Suppose that the Hessian matrix $\bm{F}(\bm{x}^\ast)$ of $f$ at a
relative minimum $\bm{x}^\ast$ has the smallest and largest eigenvalues $a, A >
0$, respectively. If $\{\bm{x}_k\}$ is a sequence generated by the method of
steepest descent that converges to $\bm{x}^\ast$, then the sequence of objective
values $\{f(\bm{x}_k)\}$ converges to $f(\bm{x}^\ast)$ linearly with a
convergence ratio no greater than $(\frac{A-a}{A+a})^2$.
:::



## Accelerated Steepest Descent {.smaller}

<br><br>

::: {.callout-important appearance="minimal"}
There is an _accelerated_ steepest descent method that works as follows:

$$
\begin{align}
&\lambda^0 = 0, & \lambda_{k+1} = \frac{1}{2}(1 + \sqrt{1+4\lambda_k^2}), &
\alpha_k = \frac{1 - \lambda_k}{\lambda_{k+1}}, & \tilde{\bm{x}}_{k+1} =
\bm{x}_k - \frac{1}{\beta}\nabla f(\bm{x}_k)^\top, & \bm{x}_{k+1} = (1 -
\alpha_k) \tilde{\bm{x}}_{k+1} + \alpha_k \tilde{\bm{x}}_k.
\end{align}
$$

Note that $\lambda_k^2 = \lambda_{k+1}(\lambda_{k+1} -1)$, $\lambda_k >
\frac{k}{2}$, and $\alpha_k \leq 0$.
:::

::: {.callout-tip icon=false}
## Theorem (Accelerated Steepest Descent)
Let $f(\bm{x})$ be convex and differentiable everywhere, satisfies the
(first-order) $\beta$-Lipschitz condition, and admits a minimizer $\bm{x}^\ast$.
Then, the method of accelerated steepest descent generates a sequence of
solutions such that

$$
f(\tilde{\bm{x}}_{k+1}) - f(\bm{x}^\ast) \leq \frac{2\beta}{k^2}|\bm{x}_0 -
\bm{x}^\ast|^2, \quad \forall k \geq 1. 
$$
:::
