# Line Search Algorithms

$$
\begin{align}
\operatorname{minimize}_{x \in [a_0, b_0]} & f(x) \in \mathbb{R}
\end{align}
$$

## $0$th-Order Method: Golden Search {.smaller}

:::: {.columns}

::: {.column width="50%"}

::: {.callout-warning appearance="minimal"}
If we evaluate $f$ at only one intermediate point of the interval, we cannot
narrow the range within which we know the minimizer is located.
:::

<center>
<img src="./contents/assets/twointermediate.png" width="75%" /img>
</center>

::: {.callout-warning appearance="minimal"}
We have to evaluate $f$ at two intermediate points.

* We choose the intermediate points in such a way that the reduction in the
range is symmetric, i.e., 
$$ a_1 - a_0 = b_0 - b_1 = \rho(b_0 - a_0), \qquad \rho < \frac{1}{2}. $$

* We then evaluate $f$ at the intermediate points.
  - If $f(a_1) < f(b_1)$ then the minimizer must lie in the range $[a_0, b_1]$.
  - If, on the other hand $f(a_1) \geq f(b_1)$, then the minimizer is located at
    $[a_1, b_0]$.
:::

:::

::: {.column width="50%"}
<center>
<img src="./contents/assets/minloc.png" width="75%" /img>
</center>

::: {.callout-warning appearance="minimal"}
* Starting with the reduced range of uncertainty, we can repeat the process to
find two new points: $a_2$ and $b_2$.
* But this is unnecessary: we know $x^\ast \in [a_0, b_1]$
  - Since $a_1 \in [a_0, b_1]$, we can set $a_1 = b_2$.
  - Only one function evaluation of $f$ at $a_2$ is necessary.
* How do we find the value of $\rho$ that results only in one new evaluation of
$f$?
:::

:::

::::


## $0$th-Order Method: Golden Search {.smaller}

:::: {.columns}

::: {.column width="50%"}
<center>
<img src="./contents/assets/twointermediate.png" width="75%" /img>
</center>

::: {.callout-warning icon=false}
## Finding $\rho$
We choose $\rho$ so that 

$$ \rho (b_1 - a_0) = b_1 - b_2 = 1-2\rho. $$

This has the two solutions: $\phantom{123} \rho_{1,2} = \frac{1}{2}(3 \pm \sqrt{5})$.

* Since we require that $\rho < \frac{1}{2}$, we must take $$\rho =
\frac{3-\sqrt{5}}{2} \approx 0.382. $$

* Observe that $1-\rho = \frac{\sqrt{5}-1}{2} \approx 0.61803$ and $\frac{\rho}{1-\rho} =
\frac{1-\rho}{1}$.
  - Dividing a range in the ratio of $\rho$ to $1-\rho$ has the effect that
    ratio of the shorter segment to the longer equals the ratio of the longer to
    the sum of the two.
  - This rule is referred to as the _golden section_.
* The uncertainty range reduction is $1-\rho$ at each stage.
  - $N$ steps of reduction using the golden section method reduces the range by
    the factor $(1-\rho)^N$.
:::

:::


::: {.column width="50%"}
:::
::::



## $1$st-Order Method: Bisection Method {.smaller}

::: {.callout-important appearance="minimal"}
__Assumptions__: $f$ is unimodal and continuously differentiable. 
:::

:::: {.columns}

::: {.column width="55%"}

::: {.callout-tip icon=false}
## Algorithm
* Let $x_0 = \frac{1}{2}(a_0 + b_0)$, the midpoint of the initial uncertainty
interval.
* Evaluate $f'(x_0)$. 
  - If $f'(x_0) > 0$, deduce that the minimizer lies to the _left_ of $x_0$.
    - Reduce the uncertainty interval to $[a_0, x_0]$.
  - If $f'(x_0) < 0$, deduce that the minimizer lies to the _right_ of $x_0$.
    - Reduce the uncertainty interval to $[x_0, b_0]$.
  - If $f'(x_0) = 0$, declare $x_0$ the minimizer and terminate the search.
* With the new uncertainty interval computed, repeat the process iteratively.
  - Compute the midpoint $x_k$ and check the sign of $f'(x_k)$ and reduce the
    uncertainty to the left or right of $x_k$.
  - Declare $x_k$ the minimizer if $f'(x_k) = 0$.
:::

:::

::: {.column width="45%"}

::: {.callout-warning icon=false}
## Salient features

* Instead of using the values of $f$, the bisection method uses the values of
$f'$.

* At each iteration, the length of the uncertaintly interval is reduced by a
factor of $\frac{1}{2}$.
  - After $N$ steps, the range is reduced by a factor of $(\frac{1}{2})^N$.
  - This factor is smaller than in the golden search or Fibonacci methods.
:::
::: {.callout-note icon=false}
## Example
$$ f(x) = x^4 - 14x^3 + 60x^2 - 70x, \quad x \in [0, 2]. $$

* If we want a precision of $0.3$, then we need $N >
1 - \operatorname{log}_2(\frac{3}{10})$ iterations, i.e., $N = 3$.
:::

:::

::::


## $2$nd-Order Method: Newton's Method {.smaller}
