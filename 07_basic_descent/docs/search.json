[
  {
    "objectID": "07_basic_descent.html#optimization-theory-and-practice",
    "href": "07_basic_descent.html#optimization-theory-and-practice",
    "title": "07_basic_descent",
    "section": "Optimization Theory and Practice",
    "text": "Optimization Theory and Practice\n\n\nBasic Descent Methods\n\n\n\n\nInstructor: Aykut Satici, Ph.D.    Mechanical and Biomedical Engineering  Electrical and Computer Engineering  Boise State University, Boise, ID, USA\n\n\nTopics:  Line Search Algorithms  The Method of Steepest Descent  Accelerated Steepest Descent  Multiplicative Steepest Descent  Newton’s Method: Second-Order  Sequential Quadratic Optimization Methods  Coordinate and Stochastic Gradient Descent"
  },
  {
    "objectID": "07_basic_descent.html#th-order-method-golden-search",
    "href": "07_basic_descent.html#th-order-method-golden-search",
    "title": "07_basic_descent",
    "section": "00th-Order Method: Golden Search",
    "text": "00th-Order Method: Golden Search\n\n\n\n\n\nIf we evaluate ff at only one intermediate point of the interval, we cannot narrow the range within which we know the minimizer is located.\n\n\n\n\n\n\n\n\n\nWe have to evaluate ff at two intermediate points.\n\nWe choose the intermediate points in such a way that the reduction in the range is symmetric, i.e., a1−a0=b0−b1=ρ(b0−a0),ρ<12. a_1 - a_0 = b_0 - b_1 = \\rho(b_0 - a_0), \\qquad \\rho < \\frac{1}{2}. \nWe then evaluate ff at the intermediate points.\n\nIf f(a1)<f(b1)f(a_1) < f(b_1) then the minimizer must lie in the range [a0,b1][a_0, b_1].\nIf, on the other hand f(a1)≥f(b1)f(a_1) \\geq f(b_1), then the minimizer is located at [a1,b0][a_1, b_0].\n\n\n\n\n\n\n\n\n\n\n\n\n\nStarting with the reduced range of uncertainty, we can repeat the process to find two new points: a2a_2 and b2b_2.\nBut this is unnecessary: we know x*∈[a0,b1]x^\\ast \\in [a_0, b_1]\n\nSince a1∈[a0,b1]a_1 \\in [a_0, b_1], we can set a1=b2a_1 = b_2.\nOnly one function evaluation of ff at a2a_2 is necessary.\n\nHow do we find the value of ρ\\rho that results only in one new evaluation of ff?"
  },
  {
    "objectID": "07_basic_descent.html#th-order-method-golden-search-1",
    "href": "07_basic_descent.html#th-order-method-golden-search-1",
    "title": "07_basic_descent",
    "section": "00th-Order Method: Golden Search",
    "text": "00th-Order Method: Golden Search\n\n\n\n\n\n\n\n\nFinding ρ\\rho\n\n\nWe choose ρ\\rho so that\nρ(b1−a0)=b1−b2=1−2ρ. \\rho (b_1 - a_0) = b_1 - b_2 = 1-2\\rho. \nThis has the two solutions: 123ρ1,2=12(3±5)\\phantom{123} \\rho_{1,2} = \\frac{1}{2}(3 \\pm \\sqrt{5}).\n\nSince we require that ρ<12\\rho < \\frac{1}{2}, we must take ρ=3−52≈0.382.\\rho =\n\\frac{3-\\sqrt{5}}{2} \\approx 0.382. \nObserve that 1−ρ=5−12≈0.618031-\\rho = \\frac{\\sqrt{5}-1}{2} \\approx 0.61803 and ρ1−ρ=1−ρ1\\frac{\\rho}{1-\\rho} = \\frac{1-\\rho}{1}.\n\nDividing a range in the ratio of ρ\\rho to 1−ρ1-\\rho has the effect that ratio of the shorter segment to the longer equals the ratio of the longer to the sum of the two.\nThis rule is referred to as the golden section.\n\nThe uncertainty range reduction is 1−ρ1-\\rho at each stage.\n\nNN steps of reduction using the golden section method reduces the range by the factor (1−ρ)N(1-\\rho)^N."
  },
  {
    "objectID": "07_basic_descent.html#st-order-method-bisection-method",
    "href": "07_basic_descent.html#st-order-method-bisection-method",
    "title": "07_basic_descent",
    "section": "11st-Order Method: Bisection Method",
    "text": "11st-Order Method: Bisection Method\n\n\n\nAssumptions: ff is unimodal and continuously differentiable.\n\n\n\n\n\n\n\n\nAlgorithm\n\n\n\nLet x0=12(a0+b0)x_0 = \\frac{1}{2}(a_0 + b_0), the midpoint of the initial uncertainty interval.\nEvaluate f′(x0)f'(x_0).\n\nIf f′(x0)>0f'(x_0) > 0, deduce that the minimizer lies to the left of x0x_0.\n\nReduce the uncertainty interval to [a0,x0][a_0, x_0].\n\nIf f′(x0)<0f'(x_0) < 0, deduce that the minimizer lies to the right of x0x_0.\n\nReduce the uncertainty interval to [x0,b0][x_0, b_0].\n\nIf f′(x0)=0f'(x_0) = 0, declare x0x_0 the minimizer and terminate the search.\n\nWith the new uncertainty interval computed, repeat the process iteratively.\n\nCompute the midpoint xkx_k and check the sign of f′(xk)f'(x_k) and reduce the uncertainty to the left or right of xkx_k.\nDeclare xkx_k the minimizer if f′(xk)=0f'(x_k) = 0.\n\n\n\n\n\n\n\n\n\nSalient features\n\n\n\nInstead of using the values of ff, the bisection method uses the values of f′f'.\nAt each iteration, the length of the uncertaintly interval is reduced by a factor of 12\\frac{1}{2}.\n\nAfter NN steps, the range is reduced by a factor of (12)N(\\frac{1}{2})^N.\nThis factor is smaller than in the golden search or Fibonacci methods.\n\n\n\n\n\n\n\n\nExample\n\n\nf(x)=x4−14x3+60x2−70x,x∈[0,2]. f(x) = x^4 - 14x^3 + 60x^2 - 70x, \\quad x \\in [0, 2]. \n\nIf we want a precision of 0.30.3, then we need N>1−log2(310)N > 1 - \\operatorname{log}_2(\\frac{3}{10}) iterations, i.e., N=3N = 3."
  },
  {
    "objectID": "07_basic_descent.html#nd-order-method-newtons-method",
    "href": "07_basic_descent.html#nd-order-method-newtons-method",
    "title": "07_basic_descent",
    "section": "22nd-Order Method: Newton’s Method",
    "text": "22nd-Order Method: Newton’s Method\n\n\nOptimization Theory and Practice • Aykut C. Satici"
  }
]