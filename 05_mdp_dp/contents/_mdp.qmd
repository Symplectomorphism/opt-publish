# Markov Decision Processes (MDP)

## Introduction

* MDPs are a classical formalization of sequential decision making.
  - Actions influence not just immediate rewards, but also subsequent states,
    and through those, future rewards.
  - They are meant to be a straightforward framing of the problem of learning
    from interaction to achieve a goal.
* MDPs involve delayed reward and the need to trade off immediate and delayed 
reward.
* We will estimate the value $q_\ast(s, a)$ of each action $a$ in each state
$s$,
  - or we estimate the value $v_\ast(s)$ of each state given optimal action
    selections.
* These state-dependent quantities are essential to accurately assigning credit
for long-term consequences to individual action selections.

## The Agent &ndash; Environment Interface {.smaller}

:::: {.columns}

::: {.column width="50%"}

::: {.callout-warning appearance="minimal"}
* The decision maker is called the _agent_ or _controller_.
* The thing it interacts with, everything outside the agent, is called the
_environment_ or _plant_.
* These interact continually, the agent selecting _actions_ (or _control
 signal_) and the environment responding to these control signals and presenting
 new situations to the agent.
* The environment also gives rise to _rewards_, special numerical values that the
agent seeks to maximize over time through its choice of actions.
:::

::: {.callout-warning appearance="minimal"}
* The agent and environment interact at each of a sequence of discrete time
steps, $t = 0, 1, 2, \ldots$. 
  - At each time step $t$, the agent receives some respresentation of the
    environment's _state_$, $S_t \in \mathcal{S}$, and on that basis selects an
    action $A_t \in \mathcal{A}(s)$.
  - One time step later, in part as a consequence of its actions, the agent
    receives a numerical reward, $R_{t+1} \in \mathcal{R} \in \mathbb{R}$ and
    finds itself in a new state, $S_{t+1}$.
* The MDP and agent together give rise to a trajectory that begins like this:
$$ S_0, A_0, S_1, R_1, A_2, R_2, S_2, A_2, R_3, \ldots $$ {#eq-traj}
:::

:::

::: {.column width="50%"}
<center>
<img src="contents/assets/agent_environment.png" width="95%" /img>
</center>

::: {.callout-important appearance="minimal"}
* In a _finite_ MDP, the sets of states, actions, and rewards 
($\mathcal{S}$, $\mathcal{A}$, and $\mathcal{R}$) all have a finite number of elements.
  - In this case, the random variables $R_t$ and $S_t$ have
    well-defined discrete probability distributions dependent on the preceding
    state and action.

$$ p(s', r \mid s, a) \triangleq \mathbb{P}\{S_t = s', R_t = r \mid S_{t-1} = s,
A_{t-1} = a\} $$ {#eq-dyn}

* This function $p$ defines the _dynamics_ of the MDP.
  - It specifies a probability distribution for each choice of $s$ and $a$,
    i.e.,

$$ \sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s', r \mid s, a) = 1,
\quad \forall s \in \mathcal{S}, \; a \in \mathcal{A}(s). $$ {#eq-pprob}

* This is called the _Markov property_.
:::

:::


::::


## The Agent &ndash; Environment Interface {.smaller}

* From the four-argument dynamics function, $p$, one can compute anything else
one might want to know about the environment
  - the _state-transition probabilities_
$$ p(s' \mid s, a) \triangleq \mathbb{P}\{S_t = s' \mid S_{t-1} = a, A_{t-1} =
a\} = \sum_{r \in \mathcal{R}} p(s', r \mid s, a). $$ {#eq-st-trans}
  - the expected rewards for state-action pairs as a two-argument function 
  $$ r(s, a) \triangleq \mathbb{E}[R_t \mid S_{t-1} = s, A_{t-1} = a] = \sum_{r
  \in \mathcal{R}} r \sum_{s' \in \mathcal{S}} p(s', r \mid s, a), $$ {#eq-exp-rew-sa}
  - the expected rewards for state-action-next-state triples as a three argument
    function 
  $$ r(s, a, s') \triangleq \mathbb{E}[R_t \mid S_{t-1} = s, A_{t-1} = a, S_t =
  s'] = \sum_{r \in \mathcal{R}} r \; \frac{p(s', r \mid s, a)}{p(s' \mid s, a)}.
  $$ {#eq-exp-rew-sas}

::: {.column-tip appearance="minimal"}
* The time steps can refer to arbitrary successive stages of decision making.
* The actions can be low-level controls (e.g. voltages), or high-level decisions
(e.g. have lunch, go to grad school).
* They can be determined by low-level sensing (e.g. sensor readings) or they can
be more high-level and abstract (e.g. symbolic descriptions of objects in a
room).
* States can be anything we can know that might be useful in making the
decisions.
:::


## The Agent &ndash; Environment Interface {.smaller}

::: {.callout-tip icon=false}
## Rule of thumb
* Anything that cannot be changed arbitrarily by the agent is considered to be
outside of it and thus part of its environment.
:::

* Not everything in the environtment is unknown to the agent.
  - For example, the agent often knows how its rewards are computed as a
    function of its actions and the states.
  - Reward computation is external to the agent because it defines the task
    facing the agent and thus is beyond its ability to change arbitrarily.
* In fact, some agents know _everything_ about how its environment.

::: {.callout-tip icon=false}
## MDP framework
Whatever the details of the sensory, memory, and control apparatus, and whatever
objective one is trying to achieve, any problem of learning goal-directed
behavior can be reduced to three signals passing back and forth between an agent
and its environment:

1. one signal to represent the choices made by the agent (the actions),
2. one signal to represent the basis on which the choices are made (the states),
3. one signal to define the agent's goal (the rewards).
:::


## Goals and Rewards {.smaller}

* The agent's goal is to _maximize_ the total amount of reward it receives;
_not_ the immediate reward, but the cumulative reward in the long run.

::: {.callout-important icon=false}
## Reward Hypothesis
All of what we mean by goal and purposes can be well thought as the maximization
of the expected value of the cumulative sum of a received scalar signal (called
reward).
:::

* Formulating goals in terms of reward signals has proved to be flexible and
widely applicable.

::: {.callout-note icon=false}
## Examples
| Task | Reward |
|------|--------|
|Robot walking|$+$ve reward on each time step proportional to robot's forward motion|
|Escape from a maze| $-1$ reward for every time step tha passes prior to escape|
|Playing chess|$+1$ for winning, $-1$ for losing, $0$ for drawing|
:::

* The reward signal is no the place to impart to the agent prior knowledge
about _how_ to achieve what we want it to do (better places: initial policy,
iniial value function).
  - Otherwise the agent might find a way to achieve _subgoals_ without achieving
    the real goal!
