[
  {
    "objectID": "05_mdp_dp.html#optimization-theory-and-practice",
    "href": "05_mdp_dp.html#optimization-theory-and-practice",
    "title": "05_mdp_dp",
    "section": "Optimization Theory and Practice",
    "text": "Optimization Theory and Practice\n\n\nMarkov Decision Processes and Dynamic Programming\n\n\n\n\nInstructor: Aykut Satici, Ph.D.Â   Mechanical and Biomedical Engineering  Electrical and Computer Engineering  Boise State University, Boise, ID, USA\n\n\nTopics:  Markov Decision Processes  Policies and Value Functions  Policy Evaluation  Policy Improvement  Policy Iteration  Value Iteration"
  },
  {
    "objectID": "05_mdp_dp.html#introduction",
    "href": "05_mdp_dp.html#introduction",
    "title": "05_mdp_dp",
    "section": "Introduction",
    "text": "Introduction\n\nMDPs are a classical formalization of sequential decision making.\n\nActions influence not just immediate rewards, but also subsequent states, and through those, future rewards.\nThey are meant to be a straightforward framing of the problem of learning from interaction to achieve a goal.\n\nMDPs involve delayed reward and the need to trade off immediate and delayed reward.\nWe will estimate the value q*(s,a)q_\\ast(s, a) of each action aa in each state ss,\n\nor we estimate the value v*(s)v_\\ast(s) of each state given optimal action selections.\n\nThese state-dependent quantities are essential to accurately assigning credit for long-term consequences to individual action selections."
  },
  {
    "objectID": "05_mdp_dp.html#the-agent-environment-interface",
    "href": "05_mdp_dp.html#the-agent-environment-interface",
    "title": "05_mdp_dp",
    "section": "The Agent â€“ Environment Interface",
    "text": "The Agent â€“ Environment Interface\n\n\n\n\n\n\nThe decision maker is called the agent or controller.\nThe thing it interacts with, everything outside the agent, is called the environment or plant.\nThese interact continually, the agent selecting actions (or control signal) and the environment responding to these control signals and presenting new situations to the agent.\nThe environment also gives rise to rewards, special numerical values that the agent seeks to maximize over time through its choice of actions.\n\n\n\n\n\n\n\n\nThe agent and environment interact at each of a sequence of discrete time steps, t=0,1,2,â€¦t = 0, 1, 2, \\ldots.\n\nAt each time step tt, the agent receives some respresentation of the environmentâ€™s state$, Stâˆˆğ’®S_t \\in \\mathcal{S}, and on that basis selects an action Atâˆˆğ’œ(s)A_t \\in \\mathcal{A}(s).\nOne time step later, in part as a consequence of its actions, the agent receives a numerical reward, Rt+1âˆˆâ„›âˆˆâ„R_{t+1} \\in \\mathcal{R} \\in \\mathbb{R} and finds itself in a new state, St+1S_{t+1}.\n\nThe MDP and agent together give rise to a trajectory that begins like this: S0,A0,S1,R1,A2,R2,S2,A2,R3,â€¦(1) S_0, A_0, S_1, R_1, A_2, R_2, S_2, A_2, R_3, \\ldots  \\qquad(1)\n\n\n\n\n\n\n\n\n\n\n\n\nIn a finite MDP, the sets of states, actions, and rewards (ğ’®\\mathcal{S}, ğ’œ\\mathcal{A}, and â„›\\mathcal{R}) all have a finite number of elements.\n\nIn this case, the random variables RtR_t and StS_t have well-defined discrete probability distributions dependent on the preceding state and action.\n\n\np(sâ€²,râˆ£s,a)â‰œâ„™{St=sâ€²,Rt=râˆ£Stâˆ’1=s,Atâˆ’1=a}(2) p(s', r \\mid s, a) \\triangleq \\mathbb{P}\\{S_t = s', R_t = r \\mid S_{t-1} = s,\nA_{t-1} = a\\}  \\qquad(2)\n\nThis function pp defines the dynamics of the MDP.\n\nIt specifies a probability distribution for each choice of ss and aa, i.e.,\n\n\nâˆ‘sâ€²âˆˆğ’®âˆ‘râˆˆâ„›p(sâ€²,râˆ£s,a)=1,âˆ€sâˆˆğ’®,aâˆˆğ’œ(s).(3) \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r \\mid s, a) = 1,\n\\quad \\forall s \\in \\mathcal{S}, \\; a \\in \\mathcal{A}(s).  \\qquad(3)\n\nThis is called the Markov property."
  },
  {
    "objectID": "05_mdp_dp.html#the-agent-environment-interface-1",
    "href": "05_mdp_dp.html#the-agent-environment-interface-1",
    "title": "05_mdp_dp",
    "section": "The Agent â€“ Environment Interface",
    "text": "The Agent â€“ Environment Interface\n\nFrom the four-argument dynamics function, pp, one can compute anything else one might want to know about the environment\n\nthe state-transition probabilities p(sâ€²âˆ£s,a)â‰œâ„™{St=sâ€²âˆ£Stâˆ’1=a,Atâˆ’1=a}=âˆ‘râˆˆâ„›p(sâ€²,râˆ£s,a).(4) p(s' \\mid s, a) \\triangleq \\mathbb{P}\\{S_t = s' \\mid S_{t-1} = a, A_{t-1} =\na\\} = \\sum_{r \\in \\mathcal{R}} p(s', r \\mid s, a).  \\qquad(4)\nthe expected rewards for state-action pairs as a two-argument function r(s,a)â‰œğ”¼[Rtâˆ£Stâˆ’1=s,Atâˆ’1=a]=âˆ‘râˆˆâ„›râˆ‘sâ€²âˆˆğ’®p(sâ€²,râˆ£s,a),(5) r(s, a) \\triangleq \\mathbb{E}[R_t \\mid S_{t-1} = s, A_{t-1} = a] = \\sum_{r\n\\in \\mathcal{R}} r \\sum_{s' \\in \\mathcal{S}} p(s', r \\mid s, a),  \\qquad(5)\nthe expected rewards for state-action-next-state triples as a three argument function r(s,a,sâ€²)â‰œğ”¼[Rtâˆ£Stâˆ’1=s,Atâˆ’1=a,St=sâ€²]=âˆ‘râˆˆâ„›rp(sâ€²,râˆ£s,a)p(sâ€²âˆ£s,a).(6) r(s, a, s') \\triangleq \\mathbb{E}[R_t \\mid S_{t-1} = s, A_{t-1} = a, S_t =\ns'] = \\sum_{r \\in \\mathcal{R}} r \\; \\frac{p(s', r \\mid s, a)}{p(s' \\mid s, a)}.\n \\qquad(6)\n\n\n\n\nThe time steps can refer to arbitrary successive stages of decision making.\nThe actions can be low-level controls (e.g.Â voltages), or high-level decisions (e.g.Â have lunch, go to grad school).\nThey can be determined by low-level sensing (e.g.Â sensor readings) or they can be more high-level and abstract (e.g.Â symbolic descriptions of objects in a room).\nStates can be anything we can know that might be useful in making the decisions."
  },
  {
    "objectID": "05_mdp_dp.html#the-agent-environment-interface-2",
    "href": "05_mdp_dp.html#the-agent-environment-interface-2",
    "title": "05_mdp_dp",
    "section": "The Agent â€“ Environment Interface",
    "text": "The Agent â€“ Environment Interface\n\n\n\nRule of thumb\n\n\n\nAnything that cannot be changed arbitrarily by the agent is considered to be outside of it and thus part of its environment.\n\n\n\n\n\nNot everything in the environtment is unknown to the agent.\n\nFor example, the agent often knows how its rewards are computed as a function of its actions and the states.\nReward computation is external to the agent because it defines the task facing the agent and thus is beyond its ability to change arbitrarily.\n\nIn fact, some agents know everything about how its environment.\n\n\n\n\nMDP framework\n\n\nWhatever the details of the sensory, memory, and control apparatus, and whatever objective one is trying to achieve, any problem of learning goal-directed behavior can be reduced to three signals passing back and forth between an agent and its environment:\n\none signal to represent the choices made by the agent (the actions),\none signal to represent the basis on which the choices are made (the states),\none signal to define the agentâ€™s goal (the rewards)."
  },
  {
    "objectID": "05_mdp_dp.html#goals-and-rewards",
    "href": "05_mdp_dp.html#goals-and-rewards",
    "title": "05_mdp_dp",
    "section": "Goals and Rewards",
    "text": "Goals and Rewards\n\nThe agentâ€™s goal is to maximize the total amount of reward it receives; not the immediate reward, but the cumulative reward in the long run.\n\n\n\n\nReward Hypothesis\n\n\nAll of what we mean by goal and purposes can be well thought as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).\n\n\n\n\nFormulating goals in terms of reward signals has proved to be flexible and widely applicable.\n\n\n\n\nExamples\n\n\n\n\n\n\n\n\n\nTask\nReward\n\n\n\n\nRobot walking\n++ve reward on each time step proportional to robotâ€™s forward motion\n\n\nEscape from a maze\nâˆ’1-1 reward for every time step that passes prior to escape\n\n\nPlaying chess\n+1+1 for winning, âˆ’1-1 for losing, 00 for drawing\n\n\n\n\n\n\n\nThe reward signal is no the place to impart to the agent prior knowledge about how to achieve what we want it to do (better places: initial policy, iniial value function).\n\nOtherwise the agent might find a way to achieve subgoals without achieving the real goal!"
  },
  {
    "objectID": "05_mdp_dp.html#returns-and-episodes",
    "href": "05_mdp_dp.html#returns-and-episodes",
    "title": "05_mdp_dp",
    "section": "Returns and Episodes",
    "text": "Returns and Episodes\n\nIn general, we seek to maximize the expected return, where the return, denoted GtG_t, is defined as some specific function of the reward sequence.\n\nSimplest (bit naÃ¯ve) case: Gtâ‰œRt+1+Rt+2+â‹¯+RT=âˆ‘k=t+1TRk,(7) G_t \\triangleq R_{t+1} + R_{t+2} + \\cdots + R_T = \\sum_{k=t+1}^T R_k,   \\qquad(7)\nDiscounted return: Gtâ‰œâˆ‘k=0âˆÎ³kRt+k+1=âˆ‘k=t+1TÎ³kâˆ’tâˆ’1Rk,0â‰¤Î³â‰¤1.(8) G_t \\triangleq \\sum_{k=0}^\\infty \\gamma^kR_{t+k+1} =\n\\sum_{k=t+1}^T \\gamma^{k-t-1}R_k, \\quad 0 \\leq \\gamma \\leq 1.  \\qquad(8)\n\nThe naÃ¯ve approach makes sense in applications in which there is a natural notion of a final time step (episodes).\n\nEach episodes ends in a state called the terminal state.\nIn episodic tasks we sometimes need to distinguish the set of all nonterminal states, ğ’®\\mathcal{S} from the set of all states plus the terminal state, ğ’®+\\mathcal{S}^+.\n\nIf the task has no final time, or terminal state, then T=âˆT=\\infty.\n\nThe naÃ¯ve return may easily become infinite. Hence we use the discounted return. Gt=Rt+1+Î³(Rt+2+Î³Rt+3+Î³2Rt+4+â‹¯)=Rt+1+Î³Gt+1.(9) G_t = R_{t+1} + \\gamma(R_{t+2} + \\gamma R_{t+3} + \\gamma^2 R_{t+4} + \\cdots)\n= R_{t+1} + \\gamma G_{t+1}.  \\qquad(9)"
  },
  {
    "objectID": "05_mdp_dp.html#how-good-are-a-state-and-an-action",
    "href": "05_mdp_dp.html#how-good-are-a-state-and-an-action",
    "title": "05_mdp_dp",
    "section": "How good are a state and an action?",
    "text": "How good are a state and an action?\n\n\n\nNote\n\n\n\n\n\n\n\n\nOptimization Theory and Practice â€¢ Aykut C. Satici"
  }
]