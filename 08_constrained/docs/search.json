[
  {
    "objectID": "08_constrained.html#optimization-theory-and-practice",
    "href": "08_constrained.html#optimization-theory-and-practice",
    "title": "08_constrained",
    "section": "Optimization Theory and Practice",
    "text": "Optimization Theory and Practice\n\n\nConstrained Optimization Conditions\n\n\n\n\nInstructor: Aykut Satici, Ph.D.Â   Mechanical and Biomedical Engineering  Electrical and Computer Engineering  Boise State University, Boise, ID, USA\n\n\nTopics:  Constraints and Tangent Plane  First-Order Necessary Conditions (Equality)  Equality Constrained Examples  Second Order Conditions (Equality)  Inequality Constraints"
  },
  {
    "objectID": "08_constrained.html#constraints",
    "href": "08_constrained.html#constraints",
    "title": "08_constrained",
    "section": "Constraints",
    "text": "Constraints\nGeneral nonlinear programming problems are of the form\n\n\nminimizef(ğ±)subject toğ¡(ğ±)=ğŸ,ğ (ğ±)â‰¥ğŸ,ğ±âˆˆÎ©.\n\\begin{align}\n\\operatorname{minimize} & f(\\bm{x}) \\\\\n\\text{subject to} & \\bm{h}(\\bm{x}) = \\bm{0}, \\;\\; \\bm{g}(\\bm{x}) \\geq \\bm{0}, \\\\ \n& \\bm{x} \\in \\Omega.\n\\end{align}\n\n\nAn inequality constraint is said to be active at ğ±\\bm{x} if gi(ğ±)=0g_i(\\bm{x}) = 0.\nIt is said to be inactive if gi(ğ±)&gt;0g_i(\\bm{x}) &gt; 0.\nAny equality constraint hi(ğ±)=0h_i(\\bm{x}) = 0 is active.\nIn the figure, g1g_1 is active, g2g_2 and g3g_3 are not.\nIf it were known a priori which constraints were active at an optimal solution then it would be a local minimum point of the problem defined by ignoring the inactive constraints.\n\n\n\nğ¡=(h1,h2,â€¦,hm)\\bm{h} = (h_1, h_2, \\ldots, h_m), ğ =(g1,g2,â€¦,gp)\\;\\;\\bm{g} = (g_1, g_2, \\ldots, g_p) are functional constraints.\nğ±âˆˆÎ©\\bm{x} \\in \\Omega: set constraint.\n\n\n\n\n\n\n\nWe will therefore start by ignoring the inequality constraints and come back to them later."
  },
  {
    "objectID": "08_constrained.html#tangent-plane",
    "href": "08_constrained.html#tangent-plane",
    "title": "08_constrained",
    "section": "Tangent Plane",
    "text": "Tangent Plane\n\nThe equality constraints define a (hyper)surface S={ğ±:h1(ğ±)=h2(ğ±)=â‹¯=hm(ğ±)=0}S = \\{\\bm{x}: h_1(\\bm{x}) = h_2(\\bm{x}) = \\cdots = h_m(\\bm{x}) = 0\\} of â„n\\mathbb{R}^n.\n\nThis hypersurface is of dimension nâˆ’mn-m (subject to a regularity assumption).\nIf the functions hih_i are continuously differentiable, the surface is said to be smooth.\n\nAssociated with a point on a smooth surface is the tangent plane at that point.\n\nA curve on a surface SS is a familty of points ğ±(t)âˆˆS\\bm{x}(t) \\in S, aâ‰¤tâ‰¤ba \\leq t \\leq b.\nThe curve is differentiable if ğ±Ì‡(t)=ddtğ±(t)\\dot{\\bm{x}}(t) = \\frac{d}{d t}\\bm{x}(t) exists, and is twice differentiable if ğ±Ìˆ(t)\\ddot{\\bm{x}}(t) exists.\nA curve ğ±(t)\\bm{x}(t) is said to pass through the point ğ±*\\bm{x}^\\ast if ğ±*=ğ±(t*)\\bm{x}^\\ast = \\bm{x}(t^\\ast) for some aâ‰¤t*â‰¤ba \\leq t^\\ast \\leq b.\n\n\n\n\n\nTangent Plane\n\n\nConsider all differentiable curves on SS passing through a point ğ±*\\bm{x}^\\ast. The tangent plane Tx*ST_{x^\\ast}S at ğ±*\\bm{x}^\\ast of SS is defined as the collection of hte derivatives at ğ±*\\bm{x}^\\ast of all these differentiable curves.\nIf ğ±*\\bm{x}^\\ast is a regular point (to be defined) then we can make the following identification:\nTğ±*S=Mâ‰œ{ğ:âˆ‡ğ¡(ğ±*)ğ=ğŸ}. T_{\\bm{x}^\\ast}S = M \\triangleq \\{\\bm{d}: \\nabla \\bm{h}(\\bm{x}^\\ast)\\bm{d} =\n\\bm{0} \\}."
  },
  {
    "objectID": "08_constrained.html#tangent-plane-2",
    "href": "08_constrained.html#tangent-plane-2",
    "title": "08_constrained",
    "section": "Tangent Plane",
    "text": "Tangent Plane\n\n\n\n\n\nDefinition (Regular Point)\n\n\nA point ğ±*\\bm{x}^\\ast satisfying the constraint ğ¡(ğ±*)=ğŸ\\bm{h}(\\bm{x}^\\ast) = \\bm{0} is said to be a regular point of the constraint if the gradient vectors âˆ‡h1(ğ±*),âˆ‡h2(ğ±*),â€¦,âˆ‡hm(ğ±*)\\nabla h_1(\\bm{x}^\\ast), \\nabla h_2(\\bm{x}^\\ast), \\ldots, \\nabla h_m(\\bm{x}^\\ast) are linearly independent."
  },
  {
    "objectID": "08_constrained.html#first-order-necessary-conditions-1",
    "href": "08_constrained.html#first-order-necessary-conditions-1",
    "title": "08_constrained",
    "section": "First-Order Necessary Conditions",
    "text": "First-Order Necessary Conditions\n\n\n\n\n\nLemma\n\n\nLet ğ±*\\bm{x}^\\ast be a regular point of the constraints ğ¡(ğ±)=ğŸ\\bm{h}(\\bm{x}) = \\bm{0} and a local extremum point of ff subject to these constraints. Then for all ğâˆˆâ„n\\bm{d} \\in \\mathbb{R}^n, we have âˆ‡ğ¡(ğ±*)ğ=ğŸâ‡’âˆ‡f(ğ±*)ğ=0. \\nabla \\bm{h}(\\bm{x}^\\ast) \\bm{d} = \\bm{0} \\;\\; \\Rightarrow \\;\\; \\nabla\nf(\\bm{x}^\\ast)\\bm{d} = 0. \n\n\n\n\n\n\nProof\n\n\nLet ğâˆˆTğ±*S\\bm{d} \\in T_{\\bm{x}^\\ast}S and let ğ±(t)âˆˆS\\bm{x}(t) \\in S such that ğ±(0)=ğ±*\\bm{x}(0) = \\bm{x}^\\ast and ğ±Ì‡(0)=ğ\\dot{\\bm{x}}(0) = \\bm{d} for âˆ’aâ‰¤tâ‰¤a-a \\leq t \\leq a for some a&gt;0a &gt; 0.\nSince ğ±*\\bm{x}^\\ast is a constrained local minimum point of ff, we have\nddtf(ğ±(t))|t=0=âˆ‡f(ğ±*)ğ=0. \\frac{d}{dt}f(\\bm{x}(t))\\Bigg\\rvert_{t=0} = \\nabla f(\\bm{x}^\\ast) \\bm{d} = 0.\n\n\n\n\n\n\n\nThis lemma says that âˆ‡f(ğ±*)âŠ¥Tğ±*S\\quad \\nabla f(\\bm{x}^\\ast) \\perp T_{\\bm{x}^\\ast}S.\n\n\n\n\n\n\n\nTheorem (FONC)\n\n\nLet ğ±*\\bm{x}^\\ast be a regular local minimum point of ff subject to the constraint ğ¡(ğ±)=ğŸ\\bm{h}(\\bm{x}) = \\bm{0}. Then there is a ğ›Œâˆˆâ„m\\bm{\\lambda} \\in \\mathbb{R}^m such that\nâˆ‡f(ğ±*)âˆ’ğ›ŒâŠ¤âˆ‡ğ¡(ğ±*)=ğŸ.(1) \\nabla f(\\bm{x}^\\ast) - \\bm{\\lambda}^\\top \\nabla \\bm{h}(\\bm{x}^\\ast) =\n\\bm{0}.  \\qquad(1)\n\n\n\n\n\n\nProof\n\n\nFrom the lemma, we may conclude that the linear system\nâˆ‡f(ğ±*)ğâ‰ 0,andâˆ‡ğ¡(ğ±*)ğ=ğŸ \\nabla f(\\bm{x}^\\ast) \\bm{d} \\neq 0, \\;\\; \\text{and} \\;\\; \\nabla\n\\bm{h}(\\bm{x}^\\ast)\\bm{d} = \\bm{0} \nHas no feasible solution ğ\\bm{d}. Then, by Farkasâ€™s lemma, its alternative system must have a solution. Specifically, there is a ğ›Œâˆˆâ„m\\bm{\\lambda} \\in \\mathbb{R}^m such that âˆ‡f(ğ±*)âˆ’ğ›ŒâŠ¤âˆ‡ğ¡(ğ±*)=ğŸ\\nabla f(\\bm{x}^\\ast) - \\bm{\\lambda}^\\top \\nabla \\bm{h}(\\bm{x}^\\ast) = \\bm{0}.\n\n\n\n\n\n\nThe FONC EquationÂ 1 together with the constraints ğ¡(ğ±*)=ğŸ\\bm{h}(\\bm{x}^\\ast) = \\bm{0} give a total of n+mn+m equations in the n+mn+m variables comprising ğ±*,ğ›Œ\\bm{x}^\\ast, \\bm{\\lambda}."
  },
  {
    "objectID": "08_constrained.html#lagrangian",
    "href": "08_constrained.html#lagrangian",
    "title": "08_constrained",
    "section": "Lagrangian",
    "text": "Lagrangian\n\nIntroduce the Lagrangian associated with the constrained problem, defined as\n\nâ„“(ğ±,ğ›Œ)=f(ğ±)âˆ’ğ›ŒâŠ¤ğ¡(ğ±).(2) \\ell(\\bm{x}, \\bm{\\lambda}) = f(\\bm{x}) - \\bm{\\lambda}^\\top \\bm{h}(\\bm{x}).  \\qquad(2)\n\nThe FONC can then be expressed as the Lagrangian derivatives\n\nâˆ‡ğ±â„“(ğ±,ğ›Œ)=ğŸ,âˆ‡ğ›Œâ„“(ğ±,ğ›Œ)=ğŸ.(3)\n\\nabla_{\\bm{x}} \\ell(\\bm{x}, \\bm{\\lambda}) = \\bm{0}, \\qquad\n\\nabla_{\\bm{\\lambda}} \\ell(\\bm{x}, \\bm{\\lambda}) = \\bm{0}.\n \\qquad(3)\n\nThe Lagrangian can be viewed as a combined objective function with a penalized term on the cosntraint violations.\n\nEach Î»i\\lambda_i is the penalty weight on equality constraint hi(ğ±)=0h_i(\\bm{x}) = 0.\nWith appropriate Î»i\\lambda_iâ€™s, a constrained problem could then be solved as an unconstrained optimization problem.\nIf ff is convex and ğ¡(ğ±)\\bm{h}(\\bm{x}) is affine ğ€ğ±âˆ’ğ›\\bm{Ax} - \\bm{b}, then â„“(â‹…)\\ell(\\cdot) is convex in ğ±\\bm{x} for every fixed ğ›Œ\\bm{\\lambda}.\n\n\n\n\n\nTheorem\n\n\nThe first-order necessary conditions are sufficient if ff is convex and ğ¡\\bm{h} is affine."
  },
  {
    "objectID": "08_constrained.html#sensitivity",
    "href": "08_constrained.html#sensitivity",
    "title": "08_constrained",
    "section": "Sensitivity",
    "text": "Sensitivity\n\nThe Lagrange multipliers associated with a constrained minimization problem have an interpretation as prices, similar to the prices in LP.\nLet a minimal solution ğ±*\\bm{x}^\\ast be a regular point and ğ›Œ*\\bm{\\lambda}^\\ast be the corresponding Lagrange multiplier vector. Consider the family of problems\n\nz(ğ›)=minimizef(ğ±)1234subject toğ¡(ğ±)=ğ›,ğ›âˆˆâ„m.(4)\n\\begin{align}\nz(\\bm{b}) = &\\operatorname{minimize} &f(\\bm{x}) \\phantom{1234} & \\\\\n& \\text{subject to} & \\bm{h}(\\bm{x}) = \\bm{b}, & \\bm{b} \\in \\mathbb{R}^m.\n\\end{align}\n \\qquad(4)\n\nFor sufficiently small |ğ›||\\bm{b}|, the problem will have a solution point ğ±(ğ›)\\bm{x}(\\bm{b}) near ğ±(ğŸ)=ğ±*\\bm{x}(\\bm{0}) = \\bm{x}^\\ast.\n\nFor each of these solutions, there is a corresponding minimum value z(ğ›)=f(ğ±(ğ›))z(\\bm{b}) = f(\\bm{x}(\\bm{b})).\nThe components of the gradient of this function can be regarded as the incremental rate of change in value per unit change in the constraint requirement.\n\n\n\n\n\nSensitivity Theorem\n\n\nConsider the family of problems EquationÂ 4. Suppose that for every ğ›âˆˆâ„m\\bm{b} \\in \\mathbb{R}^m in a region containing ğŸ\\bm{0}, its minimizer ğ±(ğ›)\\bm{x}(\\bm{b}) is continuously differentiable depending on ğ›\\bm{b}. Let ğ±*=ğ±(ğŸ)\\bm{x}^\\ast = \\bm{x}(\\bm{0}) with the corresponding Lagrange multiplier ğ›Œ*\\bm{\\lambda}^\\ast. Then\nâˆ‡z(ğŸ)=âˆ‡ğ›f(ğ±(ğ›))|ğ›=ğŸ=(ğ›Œ*)âŠ¤. \\nabla z(\\bm{0}) = \\nabla_\\bm{b} f(\\bm{x}(\\bm{b}))\n\\Bigg\\rvert_{\\bm{b}=\\bm{0}} = \\left(\\bm{\\lambda}^\\ast\\right)^\\top."
  },
  {
    "objectID": "08_constrained.html#sensitivity-1",
    "href": "08_constrained.html#sensitivity-1",
    "title": "08_constrained",
    "section": "Sensitivity",
    "text": "Sensitivity\n\n\n\nSensitivity Theorem\n\n\nConsider the family of problems EquationÂ 4. Suppose that for every ğ›âˆˆâ„m\\bm{b} \\in \\mathbb{R}^m in a region containing ğŸ\\bm{0}, its minimizer ğ±(ğ›)\\bm{x}(\\bm{b}) is continuously differentiable depending on ğ›\\bm{b}. Let ğ±*=ğ±(ğŸ)\\bm{x}^\\ast = \\bm{x}(\\bm{0}) with the corresponding Lagrange multiplier ğ›Œ*\\bm{\\lambda}^\\ast. Then\nâˆ‡z(ğŸ)=âˆ‡ğ›f(ğ±(ğ›))|ğ›=ğŸ=(ğ›Œ*)âŠ¤. \\nabla z(\\bm{0}) = \\nabla_\\bm{b} f(\\bm{x}(\\bm{b}))\n\\Bigg\\rvert_{\\bm{b}=\\bm{0}} = \\left(\\bm{\\lambda}^\\ast\\right)^\\top. \n\n\n\n\n\n\nProof\n\n\nUsing the chain rule and taking derivatives with respect to ğ›\\bm{b} on both sides of\nğ›=ğ¡(ğ±(ğ›)) \\bm{b} = \\bm{h}(\\bm{x}(\\bm{b})) \nat ğ›=ğŸ\\bm{b} = \\bm{0}, we have\nğˆ=âˆ‡ğ›ğ¡(ğ±(ğ›))|ğ›=ğŸ=âˆ‡ğ±ğ¡(ğ±(ğŸ))âˆ‡ğ›ğ±(ğŸ)=âˆ‡ğ±ğ¡(ğ±*)âˆ‡ğ›ğ±(ğŸ). \\bm{I} = \\nabla_\\bm{b} \\bm{h}(\\bm{x}(\\bm{b})) \\Bigg\\rvert_{\\bm{b}=\\bm{0}} =\n\\nabla_\\bm{x} \\bm{h}(\\bm{x}(\\bm{0}))\\nabla_\\bm{b}\\bm{x}(\\bm{0}) =\n\\nabla_\\bm{x}\\bm{h}(\\bm{x}^\\ast)\\nabla_\\bm{b}\\bm{x}(\\bm{0}). \nOn the other hand, using the chain rule and the first-order condition for ğ±*\\bm{x}^\\ast and the above matrix equality\nâˆ‡ğ›f(ğ±(ğ›))|ğ›=ğŸ=âˆ‡f(ğ±(ğŸ))âˆ‡ğ›ğ±(ğŸ)=âˆ‡f(ğ±*)âˆ‡ğ›ğ±(ğŸ)=(ğ›Œ*)âŠ¤âˆ‡ğ±ğ¡(ğ±*)âˆ‡ğ›ğ±(ğŸ)=(ğ›Œ*)âŠ¤. \\nabla_\\bm{b} f(\\bm{x}(\\bm{b})) \\Bigg\\rvert_{\\bm{b}=\\bm{0}} = \\nabla\nf(\\bm{x}(\\bm{0})) \\nabla_{\\bm{b}}\\bm{x}(\\bm{0}) = \\nabla f(\\bm{x}^\\ast)\n\\nabla_{\\bm{b}}\\bm{x}(\\bm{0}) = \\left(\\bm{\\lambda}^\\ast\\right)^\\top \\nabla_\\bm{x}\n\\bm{h}(\\bm{x}^\\ast) \\nabla_\\bm{b} \\bm{x}(\\bm{0}) =\n\\left(\\bm{\\lambda}^\\ast\\right)^\\top."
  },
  {
    "objectID": "08_constrained.html#example-1-geometric-prog.-max-volume",
    "href": "08_constrained.html#example-1-geometric-prog.-max-volume",
    "title": "08_constrained",
    "section": "Example 1 â€“ Geometric Prog.: Max Volume",
    "text": "Example 1 â€“ Geometric Prog.: Max Volume"
  },
  {
    "objectID": "08_constrained.html#example-2-hanging-chain",
    "href": "08_constrained.html#example-2-hanging-chain",
    "title": "08_constrained",
    "section": "Example 2 â€“ Hanging Chain",
    "text": "Example 2 â€“ Hanging Chain"
  },
  {
    "objectID": "08_constrained.html#example-3-compressed-sensing",
    "href": "08_constrained.html#example-3-compressed-sensing",
    "title": "08_constrained",
    "section": "Example 3 â€“ Compressed Sensing",
    "text": "Example 3 â€“ Compressed Sensing"
  },
  {
    "objectID": "08_constrained.html#second-order-conditions-1",
    "href": "08_constrained.html#second-order-conditions-1",
    "title": "08_constrained",
    "section": "Second-Order Conditions",
    "text": "Second-Order Conditions\n\n\n\n\n\nTheorem (SONC)\n\n\nSuppose that ğ±*\\bm{x}^\\ast is a regular local minimum of ff subject to ğ¡(ğ±)=ğŸ\\bm{h}(\\bm{x}) = \\bm{0}. Then there is a ğ›Œâˆˆâ„m\\bm{\\lambda} \\in \\mathbb{R}^m such that âˆ‡f(ğ±*)âˆ’ğ›ŒâŠ¤âˆ‡ğ¡(ğ±*)=ğŸ.(5) \\nabla f(\\bm{x}^\\ast) - \\bm{\\lambda}^\\top \\nabla \\bm{h}(\\bm{x}^\\ast) =\n\\bm{0}.  \\qquad(5) If we denote by MM, the tangent plane, then the matrix ğ‹(ğ±*)=ğ…(ğ±*)âˆ’ğ›ŒâŠ¤ğ‡(ğ±*)â‰½ğŸ(6) \\bm{L}(\\bm{x}^\\ast) = \\bm{F}(\\bm{x}^\\ast) - \\bm{\\lambda}^\\top\n\\bm{H}(\\bm{x}^\\ast) \\succeq \\bm{0}  \\qquad(6) on MM, that is, ğâŠ¤ğ‹(ğ±*)ğâ‰¥ğŸ\\bm{d}^\\top \\bm{L}(\\bm{x}^\\ast) \\bm{d} \\geq \\bm{0}, âˆ€ğâˆˆM\\forall \\bm{d} \\in M.\n\n\n\n\n\n\nProof\n\n\nFrom elementary calculus for every twice differentiable curve ğ±(t)âˆˆS\\bm{x}(t) \\in S through ğ±*\\bm{x}^\\ast we have 0â‰¤d2dt2f(ğ±(t))|t=0=ğ±Ì‡(0)âŠ¤ğ…(ğ±*)ğ±Ì‡(0)+âˆ‡f(ğ±*)ğ±Ìˆ(0). 0 \\leq \\frac{d^2}{dt^2}f(\\bm{x}(t))\n\\Bigg\\rvert_{t=0} = \\dot{\\bm{x}}(0)^\\top \\bm{F}(\\bm{x}^\\ast) \\dot{\\bm{x}}(0) +\n\\nabla f(\\bm{x}^\\ast) \\ddot{\\bm{x}}(0).  Furthermore, differentiating the relation ğ›ŒâŠ¤ğ¡(ğ±(t))=0\\bm{\\lambda}^\\top \\bm{h}(\\bm{x}(t)) = 0 twice, we obtain ğ±Ì‡(0)âŠ¤ğ›ŒâŠ¤ğ‡(ğ±*)ğ±Ì‡(0)âˆ’ğ›ŒâŠ¤âˆ‡ğ¡(ğ±*)ğ±Ìˆ(0)=0.\n\\dot{\\bm{x}}(0)^\\top \\bm{\\lambda}^\\top \\bm{H}(\\bm{x}^\\ast)\\dot{\\bm{x}}(0) -\n\\bm{\\lambda}^\\top \\nabla \\bm{h}(\\bm{x}^\\ast) \\ddot{\\bm{x}}(0) = 0.  Additing these two equations yields the result d2dt2f(ğ±(t))|t=0=ğ±Ì‡(0)âŠ¤ğ‹(ğ±*)ğ±Ì‡(0)â‰¥0. \\frac{d^2}{dt^2}f(\\bm{x}(t)) \\Bigg\\rvert_{t=0} = \\dot{\\bm{x}}(0)^\\top\n\\bm{L}(\\bm{x}^\\ast) \\dot{\\bm{x}}(0) \\geq 0.  Since ğ±Ì‡(0)\\dot{\\bm{x}}(0) is arbitrary in MM, we have the stated conclusion.\n\n\n\n\n\n\n\nTheorem (SOSC)\n\n\nSuppose there is a point ğ±*\\bm{x}^\\ast satisfying ğ¡(ğ±*)=ğŸ\\bm{h}(\\bm{x}^\\ast) = \\bm{0}, and a ğ›Œ\\bm{\\lambda} such that EquationÂ 5 holds. Suppose also that the matrix ğ‹(ğ±*)â‰»ğŸ\\bm{L}(\\bm{x}^\\ast) \\succ \\bm{0} on MM. Then ğ±*\\bm{x}^\\ast is a strict local minimum of ff subject to ğ¡(ğ±)=ğŸ\\bm{h}(\\bm{x}) = \\bm{0}.\n\n\n\n\n\n\nProof\n\n\nIf ğ±*\\bm{x}^\\ast is not a strict relative minimum point, âˆƒ\\exists a sequence of feasible points {ğ²k}\\{\\bm{y}_k\\} converging to ğ±*\\bm{x}^\\ast s.t. for each kk, f(ğ²k)â‰¤f(ğ±*)f(\\bm{y}_k) \\leq f(\\bm{x}^\\ast). Write ğ²k=ğ±*+Î´kğ¬k\\bm{y}_k = \\bm{x}^\\ast + \\delta_k \\bm{s}_k, where |ğ¬k|=1|\\bm{s}_k| = 1 and Î´k&gt;0\\delta_k &gt; 0, âˆ€k\\forall k. By Bolzano-Weierstrass some subsequence of {ğ¬k}\\{\\bm{s}_k\\} converges. WLOG assume ğ¬kâ†’ğ¬*\\bm{s}_k \\rightarrow \\bm{s}^\\ast. We also have ğ¡(ğ²k)âˆ’ğ¡(ğ±*)=ğŸ\\bm{h}(\\bm{y}_k) - \\bm{h}(\\bm{x}^\\ast) = \\bm{0} which implies âˆ‡ğ¡(ğ±*)ğ¬*=ğŸ\\nabla \\bm{h}(\\bm{x}^\\ast)\\bm{s}^\\ast = \\bm{0}. We have\n0=hi(ğ²k)=hi(ğ±*)+Î´kâˆ‡hi(ğ±*)ğ¬k+Î´k22ğ¬kâŠ¤âˆ‡2hi(ğ›ˆi)ğ¬k(7) 0 = h_i(\\bm{y}_k) = h_i(\\bm{x}^\\ast) + \\delta_k \\nabla\nh_i(\\bm{x}^\\ast)\\bm{s}_k + \\frac{\\delta_k^2}{2}\\bm{s}_k^\\top \\nabla^2\nh_i(\\bm{\\eta}_i) \\bm{s}_k  \\qquad(7) 0â‰¥f(ğ²k)âˆ’f(ğ±*)=Î´kâˆ‡f(ğ±*)ğ¬k+Î´k22ğ¬kâŠ¤âˆ‡2f(ğ›ˆ0)ğ¬k(8) 0 \\geq f(\\bm{y}_k) - f(\\bm{x}^\\ast) =  \\delta_k \\nabla\nf(\\bm{x}^\\ast)\\bm{s}_k + \\frac{\\delta_k^2}{2}\\bm{s}_k^\\top \\nabla^2\nf(\\bm{\\eta}_0) \\bm{s}_k  \\qquad(8)\nMultiply EquationÂ 7 by âˆ’Î»i-\\lambda_i and add to EquationÂ 8 to obtain\n0â‰¥Î´k22ğ¬kâŠ¤{âˆ‡2f(ğ›ˆ0)âˆ’âˆ‘i=1mÎ»iâˆ‡2hi(ğ›ˆi)}ğ¬k,â‡’â‡askâ†’âˆ. 0 \\geq \\frac{\\delta_k^2}{2}\\bm{s}_k^\\top \\left\\{ \\nabla^2 f(\\bm{\\eta}_0) -\n\\sum_{i=1}^m \\lambda_i \\nabla^2 h_i(\\bm{\\eta}_i) \\right\\}\\bm{s}_k, \\quad\n\\Rightarrow\\!\\Leftarrow \\;\\; \\text{as} \\;\\; k \\rightarrow \\infty."
  },
  {
    "objectID": "08_constrained.html#example",
    "href": "08_constrained.html#example",
    "title": "08_constrained",
    "section": "Example",
    "text": "Example\n\n\nConsider the problem\nmaximize(x1âˆ’1)2+(x2âˆ’1)2subject tox12+x22âˆ’1=0.\n\\begin{align}\n\\operatorname{maximize} & (x_1 - 1)^2 + (x_2 - 1)^2 \\\\\n\\text{subject to} & x_1^2 + x_2^2 - 1 = 0.\n\\end{align}\n\nThe Lagrangian and subsection FONC would be\nâ„“(x1,x2,Î»)=(x1âˆ’1)2+(x2âˆ’1)2âˆ’Î»(x12+x22âˆ’1),âˆ‡ğ±â„“(x1,x2,Î»)=(2x1(1âˆ’Î»)âˆ’22x2(1âˆ’Î»)âˆ’2)=ğŸ.\n\\begin{align}\n\\ell(x_1, x_2, \\lambda) &= (x_1 - 1)^2 + (x_2 - 1)^2 - \\lambda(x_1^2 + x_2^2 -\n1), \\\\\n\\nabla_{\\bm{x}}\\ell(x_1, x_2, \\lambda) &= \\begin{pmatrix} 2x_1(1-\\lambda) - 2 \\\\\n2x_2(1-\\lambda) - 2 \\end{pmatrix} = \\bm{0}.\n\\end{align}\n\nFrom the two equations we conclude x1=x2x_1 = x_2, together with x12+x22âˆ’1=0x_1^2 + x_2^2 - 1= 0.\nWe have the two first-order stationary solutions x1=x2=12,Î»=1âˆ’2x1=x2=âˆ’12,Î»=1+2. \n\\begin{align}\nx_1 &= x_2 = \\frac{1}{\\sqrt{2}}, \\quad \\lambda = 1-\\sqrt{2} \\\\\nx_1 &= x_2 = -\\frac{1}{\\sqrt{2}}, \\quad \\lambda = 1+\\sqrt{2}. \n\\end{align}\n\n\n\n\n\nThe Lagrangian Hessian matrix ğ…âˆ’ğ›ŒâŠ¤ğ‡\\bm{F}-\\bm{\\lambda}^\\top \\bm{H} at these Î»\\lambdas becomes\n[2(1âˆ’Î»)002(1âˆ’Î»)]|Î»=1âˆ’2=[220022][2(1âˆ’Î»)002(1âˆ’Î»)]|Î»=1+2=[âˆ’2200âˆ’22]\n\\begin{align}\n\\left. \\begin{bmatrix}\n2(1-\\lambda) & 0 \\\\ 0 & 2(1-\\lambda)\n\\end{bmatrix}\\right\\rvert_{\\lambda = 1-\\sqrt{2}} &= \n\\begin{bmatrix}\n2\\sqrt{2} & 0 \\\\ 0 & 2\\sqrt{2}\n\\end{bmatrix} \\\\\n\\left. \\begin{bmatrix}\n2(1-\\lambda) & 0 \\\\ 0 & 2(1-\\lambda)\n\\end{bmatrix}\\right\\rvert_{\\lambda = 1+\\sqrt{2}} &= \n\\begin{bmatrix}\n-2\\sqrt{2} & 0 \\\\ 0 & -2\\sqrt{2}\n\\end{bmatrix}\n\\end{align}\n\n\n\n\nSo which is minimum, which is maximum?"
  },
  {
    "objectID": "08_constrained.html#eigenvalues-in-the-tangent-subspace",
    "href": "08_constrained.html#eigenvalues-in-the-tangent-subspace",
    "title": "08_constrained",
    "section": "Eigenvalues in the Tangent Subspace",
    "text": "Eigenvalues in the Tangent Subspace\n\n\n\nGiven any vector ğâˆˆM\\bm{d} \\in M, the vector ğ‹ğâˆˆâ„n\\bm{Ld} \\in \\mathbb{R}^n, but not necessarily in MM.\nWe project ğ‹ğ\\bm{Ld} orthogonally back onto MM as in figure.\n\nThis is the restriction of ğ‹\\bm{L} to MM operating on ğ\\bm{d}.\nIn this way, we obtain a linear transformation ğ‹M:Mâ†’M\\bm{L}_M: M \\rightarrow M.\n\nA vector ğ²âˆˆM\\bm{y} \\in M is an eigenvector of ğ‹M\\bm{L}_M if âˆƒÎ»\\exists \\lambda s.t. ğ‹Mğ²=Î»ğ²\\bm{L}_M\\bm{y} = \\lambda \\bm{y} (Î»\\lambda: eigenvalue of ğ‹M\\bm{L}_M).\n\nIn terms of ğ‹\\bm{L}, we see that ğ²\\bm{y} is an eigenvector of ğ‹M\\bm{L}_M if ğ‹ğ²\\bm{Ly} can be written as a sum of Î»ğ²\\lambda \\bm{y} and a vector orthogonal to MM.\n\nIntroduce an orthonormal basis {ğ1,â€¦,ğnâˆ’m}\\{\\bm{e}_1, \\ldots, \\bm{e}_{n-m}\\} of MM.\n\nDefine ğ„â‰œ[ğ1ğ2â‹¯ğnâˆ’m]\\bm{E} \\triangleq \\begin{bmatrix} \\bm{e}_1 & \\bm{e}_2 & \\cdots \\bm{e}_{n-m} \\end{bmatrix}.\nAny vector ğ²âˆˆM\\bm{y} \\in M can be written as ğ²=ğ„ğ³\\bm{y} = \\bm{Ez} for some ğ³âˆˆâ„nâˆ’m\\bm{z} \\in \\mathbb{R}^{n-m}.\nğ‹ğ„ğ³\\bm{LEz} represents the action of LL on such a vector.\n\n\n\n\n\n\n\n\n\n\n\nTo project the result back to MM and express the result back in terms of the basis {ğ1,ğ2,â€¦,ğnâˆ’m}\\{\\bm{e}_1, \\bm{e}_2, \\ldots, \\bm{e}_{n-m}\\}, we multiply by ğ„âŠ¤\\bm{E}^\\top: ğ„âŠ¤ğ‹ğ„\\bm{E}^\\top \\bm{LE} is the matrix representation of ğ‹\\bm{L} restricted to MM."
  },
  {
    "objectID": "08_constrained.html#example-1",
    "href": "08_constrained.html#example-1",
    "title": "08_constrained",
    "section": "Example",
    "text": "Example\n\n\n\n\n\nProblem\n\n\nminimizex1+x22+x2x3+2x32subject to12(x12+x22+x32)=1.\n\\begin{align}\n\\operatorname{minimize} & x_1 + x_2^2 + x_2x_3 + 2x_3^2 \\\\\n\\text{subject to} & \\frac{1}{2}\\left(x_1^2 + x_2^2 + x_3^2 \\right) = 1.\n\\end{align}\n\n\n\n\n\n\n\nFONC\n\n\n1âˆ’Î»x1=0,2x2+x3âˆ’Î»x2=0,x2+4x3âˆ’Î»x3=0.\n\\begin{align}\n1 - \\lambda x_1 &= 0, \\\\\n2x_2 + x_3 - \\lambda x_2 &= 0, \\\\\nx_2 + 4x_3 - \\lambda x_3 &= 0.\n\\end{align}\n\nwith one solution x1=1x_1 = 1, x2=0x_2 = 0, x3=0x_3 = 0, Î»=1\\lambda = 1.\n\n\n\n\n\n\nSOC\n\n\nğ‹=[âˆ’100011013]\n\\bm{L} = \\begin{bmatrix}\n-1 & 0 & 0 \\\\ 0 & 1 & 1 \\\\ 0 & 1 & 3\n\\end{bmatrix}\n\nand the corresponding subspace MM is\nM={ğ²:y1=0}. M = \\{ \\bm{y}: y_1 = 0 \\}. \n\n\n\n\n \n\nIn this case MM is the subspace spanned by the standard bases ğ2\\bm{e}_2 and ğ3\\bm{e}_3 of â„3\\mathbb{R}^3.\nTherefore the restriction of ğ‹\\bm{L} is computed to be\n\nğ‹M=[010001][âˆ’100011013][001001]=[1113].\n\\bm{L}_M = \\begin{bmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix}\n-1 & 0 & 0 \\\\ 0 & 1 & 1 \\\\ 0 & 1 & 3\n\\end{bmatrix}\n\\begin{bmatrix} 0 & 0 \\\\ 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 1 \\\\\n1 & 3 \\end{bmatrix}.\n\n\nğ‹M\\bm{L}_M is seen to be positive definite.\n\nTherefore the point in question is a relative minimum point."
  },
  {
    "objectID": "08_constrained.html#projected-hessians",
    "href": "08_constrained.html#projected-hessians",
    "title": "08_constrained",
    "section": "Projected Hessians",
    "text": "Projected Hessians\n\nAlternatively, we can construct matrices and determinants of order nn rather than nâˆ’mn-m.\nFor simplicity, let ğ€=âˆ‡ğ¡\\bm{A} = \\nabla \\bm{h}, which has full row rank.\nAny ğ±\\bm{x} satisfying ğ€ğ±=ğŸ\\bm{Ax} = \\bm{0} can be expressed as ğ±=(ğˆâˆ’ğ€âŠ¤(ğ€ğ€âŠ¤)âˆ’1ğ€)ğ³â‰œğğ€ğ³,ğ³âˆˆâ„n. \\bm{x} = (\\bm{I} - \\bm{A}^\\top(\\bm{AA}^\\top)^{-1}\\bm{A})\\bm{z} \\triangleq\n\\bm{P}_{\\bm{A}}\\bm{z}, \\qquad \\bm{z} \\in \\mathbb{R}^n. \nğğ€\\bm{P}_\\bm{A} is the so-called projection matrix onto the nullspace of ğ€\\bm{A} (i.e.Â onto MM)\n\nIf ğ±âŠ¤ğ‹ğ±â‰¥0,âˆ€ğ±âˆˆM\\bm{x}^\\top \\bm{L}\\bm{x} \\geq 0, \\;\\; \\forall \\bm{x} \\in M, then ğ³âŠ¤ğğ€ğ‹ğğ€ğ³â‰¥0,âˆ€ğ³âˆˆâ„n\\bm{z}^\\top \\bm{P}_\\bm{A}\\bm{LP}_\\bm{A}\\bm{z} \\geq 0, \\;\\; \\forall \\bm{z} \\in \\mathbb{R}^n or the matrix ğğ€ğ‹ğğ€â‰½ğŸ\\bm{P}_\\bm{A}\\bm{L}\\bm{P}_\\bm{A} \\succeq \\bm{0}.\nFurthermore, if ğğ€ğ‹ğğ€\\bm{P}_\\bm{A}\\bm{LP}_\\bm{A} has rank nâˆ’mn-m, then ğ‹M\\bm{L}_M is positive definite.\n\n\n\n\n\nProjected Hessian Test\n\n\nThe matrix ğ‹\\bm{L} is positive definite on MM iff the projected Hessian matrix to MM is positive semidefinite with rank nâˆ’mn-m.\n\n\n\nIn the previous example we had ğ€=âˆ‡ğ¡=[100]\\bm{A} = \\nabla \\bm{h} = \\begin{bmatrix} 1 & 0 & 0 \\end{bmatrix}. Hence\n ğğ€=ğˆâˆ’[100][100]âŠ¤=[000010001]â‡’ğğ€ğ‹ğğ€=[000011013].\n\\bm{P}_\\bm{A} = \\bm{I} - \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\\n\\end{bmatrix}\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ \\end{bmatrix}^\\top = \\begin{bmatrix}\n0 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\quad \\Rightarrow \\quad\n\\bm{P}_\\bm{A}\\bm{LP}_\\bm{A} = \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 1 & 1 \\\\ 0 & 1 &\n3 \\end{bmatrix}."
  },
  {
    "objectID": "08_constrained.html#first-order-necessary-conditions-2",
    "href": "08_constrained.html#first-order-necessary-conditions-2",
    "title": "08_constrained",
    "section": "First-Order Necessary Conditions",
    "text": "First-Order Necessary Conditions\n\n\n\nDefinition\n\n\nLet ğ±*\\bm{x}^\\ast be a point satisfying the constraints\nğ¡(ğ±*)=ğŸ,ğ (ğ±*)â‰¥ğŸ,(9) \\bm{h}(\\bm{x}^\\ast) = \\bm{0}, \\;\\; \\bm{g}(\\bm{x}^\\ast) \\geq \\bm{0},  \\qquad(9)\nand let JJ be the set of indices jj for which gj(ğ±*)=0g_j(\\bm{x}^\\ast) = 0. Then ğ±*\\bm{x}^\\ast is said to be a regular point of the constraints EquationÂ 9 if the gradient vectors âˆ‡hi(ğ±*)\\nabla h_i(\\bm{x}^\\ast), âˆ‡gj(ğ±*)\\nabla g_j(\\bm{x}^\\ast), 1â‰¤iâ‰¤m1 \\leq i \\leq m, jâˆˆJj \\in J are linearly independent.\n\n\n\n\n\n\nKarush-Kuhn-Tucker (KKT) Conditions\n\n\nLet ğ±*\\bm{x}^\\ast be a relative minimum point for the problem\nminimizef(ğ±)subject toğ¡(ğ±)=ğŸ,ğ (ğ±)â‰¥ğŸ,\n\\begin{align}\n\\operatorname{minimize} & f(\\bm{x}) \\\\\n\\text{subject to} & \\bm{h}(\\bm{x}) = \\bm{0}, \\quad \\bm{g}(\\bm{x}) \\geq \\bm{0},\n\\end{align}\n\nand suppose ğ±*\\bm{x}^\\ast is a regular point for the constraints. Then there is a vector ğ›Œâˆˆâ„m\\bm{\\lambda} \\in \\mathbb{R}^m and a vector ğ›âˆˆâ„p\\bm{\\mu} \\in \\mathbb{R}^p with ğ›â‰¥ğŸ\\bm{\\mu} \\geq \\bm{0} such that\nâˆ‡f(ğ±*)âˆ’ğ›ŒâŠ¤âˆ‡ğ¡(ğ±*)âˆ’ğ›âŠ¤âˆ‡ğ (ğ±*)=ğŸ,ğ›âŠ¤ğ (ğ±*)=0.(10)\n\\begin{align}\n\\nabla f(\\bm{x}^\\ast) - \\bm{\\lambda}^\\top \\nabla \\bm{h}(\\bm{x}^\\ast) -\n\\bm{\\mu}^\\top \\nabla \\bm{g}(\\bm{x}^\\ast) &= \\bm{0}, \\\\\n\\bm{\\mu}^\\top \\bm{g}(\\bm{x}^\\ast) = 0.\n\\end{align}\n \\qquad(10)"
  },
  {
    "objectID": "08_constrained.html#karush-kuhn-tucker-kkt-proof",
    "href": "08_constrained.html#karush-kuhn-tucker-kkt-proof",
    "title": "08_constrained",
    "section": "Karush-Kuhn-Tucker (KKT) Proof",
    "text": "Karush-Kuhn-Tucker (KKT) Proof"
  },
  {
    "objectID": "08_constrained.html#example-soft-margin-minimization-in-svm",
    "href": "08_constrained.html#example-soft-margin-minimization-in-svm",
    "title": "08_constrained",
    "section": "Example â€“ Soft-Margin Minimization in SVM",
    "text": "Example â€“ Soft-Margin Minimization in SVM\n\n\n\nOptimization Theory and Practice â€¢ Aykut C. Satici"
  }
]