# Inequality Constraints

## First-Order Necessary Conditions {.smaller}

::: {.callout-tip icon=false}
## Definition
Let $\bm{x}^\ast$ be a point satisfying the constraints 

$$ \bm{h}(\bm{x}^\ast) = \bm{0}, \;\; \bm{g}(\bm{x}^\ast) \geq \bm{0}, $$ {#eq-mix-cons} 

and let $J$ be
the set of indices $j$ for which $g_j(\bm{x}^\ast) = 0$. Then $\bm{x}^\ast$ is
said to be a _regular point_ of the constraints @eq-mix-cons if the gradient
vectors $\nabla h_i(\bm{x}^\ast)$, $\nabla g_j(\bm{x}^\ast)$, $1 \leq i \leq m$,
$j \in J$ are linearly independent.
:::


::: {.callout-tip icon=false}
## Karush-Kuhn-Tucker (KKT) Conditions
Let $\bm{x}^\ast$ be a relative minimum point for the problem 

$$
\begin{align}
\operatorname{minimize} & f(\bm{x}) \\
\text{subject to} & \bm{h}(\bm{x}) = \bm{0}, \quad \bm{g}(\bm{x}) \geq \bm{0},
\end{align}
$$ {#eq-nlp}

and suppose $\bm{x}^\ast$ is a regular point for the constraints. Then there is
a vector $\bm{\lambda} \in \mathbb{R}^m$ and a vector $\bm{\mu} \in
\mathbb{R}^p$ with $\bm{\mu} \geq \bm{0}$ such that 

$$
\begin{align}
\nabla f(\bm{x}^\ast) - \bm{\lambda}^\top \nabla \bm{h}(\bm{x}^\ast) -
\bm{\mu}^\top \nabla \bm{g}(\bm{x}^\ast) &= \bm{0}, \\
\bm{\mu}^\top \bm{g}(\bm{x}^\ast) = 0.
\end{align}
$$ {#eq-kkt}
:::


## Karush-Kuhn-Tucker (KKT) Conditions {.smaller}

::: {.callout-note icon=false}
## Proof
Since $\bm{\mu} \geq \bm{0}$ and $\bm{g}(\bm{x}^\ast) \geq \bm{0}$, the second
of @eq-kkt is equivalent to the statement that a component of $\bm{\mu}$ may be
nonzero only if the corresponding constraint is active. This is a _complementary
slackness_ condition studied in LP, which states that $\bm{g}(\bm{x}^\ast)_j >
0$ implies $\mu_j = 0$ and $\mu_j = 0$ implies $\bm{g}(\bm{x}^\ast)_j = 0$.

Since $\bm{x}^\ast$ is a relative minimum point over the constraint set, it is
also a relative minimum over the subset of that set defined by setting the
active constraints to zero. Thus, for the resulting equality constrained
problem, defined in a nbhd. of $\bm{x}^\ast$, there are Lagrange multipliers.
Therefore, we conclude that first of @eq-kkt holds with $\mu_j = 0$ if
$g_j(\bm{x}^\ast) \neq 0$.

It remains to be shown that $\bm{\mu} \geq \bm{0}$. Suppose $\mu_k < 0$ for some
$k \in J$. Let $S'$ and $M'$ be the surface and the tangent plane, resp.,
defined by all _other_ active constraints at $\bm{x}^\ast$. By the regularity
assumption, there is a $\bm{d}$ such that $\bm{d} \in M'$, that is, $\nabla
\bm{h}(\bm{x}^\ast)\bm{d} = \bm{0}$ and $\nabla g_j(\bm{x}^\ast) \bm{d} = 0$ for
all $j \in J$ but $j \neq k$, and $\nabla g_k(\bm{x}^\ast) \bm{d} > 0$.
Multiplying this $\bm{d}$ from the right to the first of @eq-kkt, we have

$$ \nabla f(\bm{x}^\ast) \bm{d} - \mu_k \nabla g_k(\bm{x}^\ast) \bm{d} = 0 \quad
\text{or} \quad \nabla f(\bm{x}^\ast) \bm{d} = \mu_k \nabla g_k(\bm{x}^\ast)
\bm{d} < 0, $$

which implies that $\bm{d}$ is a descent direction for the objective function.

Let $\bm{x}(t) \in S'$ with $\bm{x}(0) = \bm{x}^\ast$ and $\dot{\bm{x}}(0) =
\bm{d}$. Then for small $t \geq 0$, $\bm{x}(t)$ is feasible &ndash; it remains
on the surface of $S'$ and $g_k(\bm{x}(t)) > 0$ because $\nabla
g_k(\bm{x}^\ast)\bm{d} > 0$ (that is, constrant $g_k$ becomes inactive). But

$$
\frac{df}{dt}(\bm{x}(t))\Bigg\rvert_{t=0} = \nabla f(\bm{x}^\ast)\bm{d} < \bm{0}
$$

which contradicts the minimality of $\bm{x}(0) = \bm{x}^\ast$.
:::

## The Lagrangian and First-Order Conditions {.smaller}

Introduce the _Lagrangian_ associated with the problem, defined as 

$$ \ell(\bm{x}, \bm{\lambda}, \bm{\mu}) = f(\bm{x}) - \bm{\lambda}^\top
\bm{h}(\bm{x}) - \bm{\mu}^\top \bm{g}(\bm{x}). $$ {#eq-lagrangian-inq}

* The Lagrangian can again be viewed as an unconstrained objective function
combined with the original objective with two penalized terms on constraint
violations.
  - $\lambda_i$ is the penalty weight on the equality $h_i(\bm{x}) = 0$ 
  - $\mu_j$ is the penalty weight on the inequality $g_j(\bm{x})$. 
    - There should be no penalty if $g_j(\bm{x}) > 0$, so that $\mu_j = 0$, 
    - Otherwise, $\mu_j$ needs to be increased to a positive value in the
      Lagrangian to pump up the value of $g_j(\bm{x})$ when the Lagrangian is
      minimized.

::: {.callout-important icon=false}
## First-Order Necessary Conditions
* (OVC) The original variable constraints of the problem @eq-mix-cons.
* (MSC) The multiplier sign constraints: $\bm{\lambda}$ "free" and $\bm{\mu} \geq \bm{0}$. In general, the sign
of the multiplier is determined by the sense of the original constraint: (i) if 
it is $=$ then the sign is "free", (ii) if it is $\leq$ or $\geq$ then the sign 
is $\leq$ or $\geq$, resp.
* (LDC) The Lagrangian derivative condition: the first of @eq-kkt
* (CSC) The complementarity slackness condition: the second of @eq-kkt
:::


## Example {.smaller}

:::: {.columns}

::: {.column width="50%"}

<br>
<br>

::: {.callout-important appearance="minimal"}
$$
\begin{align}
\operatorname{maximize} & (x_1 - 1)^2 + (x_2 - 1)^2 \\
\text{subject to} & 1 - x_1^2 - x_2^2 \geq 0.
\end{align}
$$

The Lagrangian and the (LDC) conditions are
$$
\begin{align}
\ell(x_1, x_2, \mu(\geq 0)) &= (x_1 - 1)^2 + (x_2 - 1)^2 \\ 
& \phantom{123} - \mu(1 - x_1^2 - x_2^2), \\
(\text{LDC}) \;\; \nabla_\bm{x}\ell(x_1, x_2, \mu) &= \begin{pmatrix}
2x_1 (1+\mu) - 2 \\ 2x_2(1+\mu) - 2
\end{pmatrix} = \bm{0},
\end{align}
$$

and the (CSC) condition is $\mu(1 - x_1^2 - x_2^2) = 0$.

:::
:::

::: {.column width="50%"}
* From the two equations of (LDC) and $\mu \geq 0$, we conclude $x_1 = x_2$.
* We first try $\mu = 0$, which, from the two eqns. of (LDC) leads to $x_1 = x_2
= 1$ and violates the inequality constraint.
* Thus the constraint must be active, which gives rise to two possible
    solutions
$$ ( x_1 = x_2 = \frac{1}{\sqrt{2}} ) \;\; \text{and} \;\; (x_1 = x_2 = \frac{-1}{\sqrt{2}}). $$
* The former, again from (LDC), makes $\mu = \sqrt{2} - 1$; while the latter
    makes $\mu = -\sqrt{2} - 1$, which violates (MSC).
* Thus, the only qualified first-order solution is $x_1 = x_2 =
\frac{1}{\sqrt{2}}$, with the corresponding $\mu = \sqrt{2} - 1$.
:::

::::


## Convex Problems {.smaller}

If $f$ is convex and $\bm{h}(\bm{x})$ is affine $\bm{Ax} - \bm{b}$, and
$\bm{g}(\bm{x})$ are concave functions, then $\ell(\cdot)$ is convex in $\bm{x}$
for every fixed $\bm{\lambda}$ and $\bm{\mu} (\geq \bm{0})$.

Therefore if $\bm{x}^\ast$ meets the first of @eq-kkt, then $\bm{x}^\ast$ is the
global minimizer of the unconstrained $\ell(\bm{x}, \bm{\lambda}, \bm{\mu})$
with the same $\bm{\lambda}$ and $\bm{\mu}$.

::: {.callout-tip icon=false}
## Theorem
The FONC are sufficient if $f$ is convex, $\bm{h}$ is affine, and $g_j(\bm{x})$
is concave for all $j$.
:::

::: {.callout-note icon=false}
## Proof
Let $\bm{x}$ be any feasible solution and $\bm{x}^\ast$, together with
$\bm{\lambda}^\ast$ and $\bm{\mu}^\ast$ satisfy the FONC. Then we have

$$
\begin{align}
0 &\leq \ell(\bm{x}, \bm{\lambda}^\ast, \bm{\mu}^\ast) - \ell(\bm{x}^\ast,
\bm{\lambda}^\ast, \bm{\mu}^\ast) = f(\bm{x}) - f(\bm{x}^\ast) -
(\bm{\lambda}^\ast)^\top (\bm{h}(\bm{x}) - \bm{h}(\bm{x}^\ast)) -
(\bm{\mu}^\ast)^\top (\bm{g}(\bm{x}) - \bm{g}(\bm{x}^\ast)) \\
&= f(\bm{x}) - f(\bm{x}^\ast) - (\bm{\mu}^\ast)^\top (\bm{g}(\bm{x}) -
\bm{g}(\bm{x}^\ast)) = f(\bm{x}) - f(\bm{x}^\ast) - \sum_{j \in J} \mu_j
(g_j(\bm{x}) - g_j(\bm{x}^\ast)) \\
&= f(\bm{x}) - f(\bm{x}^\ast) - \sum_{j \in J} \mu_j g_j(\bm{x}) \leq f(\bm{x})
- f(\bm{x}^\ast).
\end{align}
$$

which completes the proof.
:::


## Second-Order Conditions {.smaller}

:::: {.columns}

::: {.column width="50%"}

<br> <br>

::: {.callout-tip icon=false}
## SONC
Suppose $\bm{x}^\ast$ is a regular point of the constraints. If $\bm{x}^\ast$ is
a relative minimum point for the problem @eq-nlp, then there is a $\bm{\lambda}
\in \mathbb{R}^m$, $\bm{\mu} \in \mathbb{R}^p$, $\bm{\mu} \geq 0$ such that
@eq-kkt hold and such that $$ \bm{L}(\bm{x}^\ast) = \bm{F}(\bm{x}^\ast) -
\bm{\lambda}^\top \bm{H}(\bm{x}^\ast) - \bm{\mu}^\top \bm{G}(\bm{x}^\ast) $$ {#eq-sonc-nlp} 
is positive semidefinite on the tangent subspace of the active constraints in
$\bm{x}^\ast$.
:::

:::

::: {.column width="50%"}
::: {.callout-tip icon=false}
## SOSC
Sufficient conditions that a point satisfying @eq-mix-cons be a strict relative
point of the problem @eq-nlp is that there exist $\bm{\lambda} \in
\mathbb{R}^m$, $\bm{\mu} \in \mathbb{R}^p$ such that 

$$
\begin{align}
\bm{\mu} &\geq 0 \\
\bm{\mu}^\top \bm{g}(\bm{x}^\ast) &= 0 \\
\nabla f(\bm{x}^\ast) - \bm{\lambda}^\top \nabla \bm{h}(\bm{x}^\ast) -
\bm{\mu}^\top \nabla \bm{g}(\bm{x}^\ast) &= 0,
\end{align}
$$

and the Hessian matrix 

$$ \bm{L}(\bm{x}^\ast) = \bm{F}(\bm{x}^\ast) -
\bm{\lambda}^\top \bm{H}(\bm{x}^\ast) - \bm{\mu}^\top \bm{G}(\bm{x}^\ast) $$

is positive definite on the subspace


$$
M' = \left\{ \bm{d}: \nabla \bm{h}(\bm{x}^\ast)\bm{d} = 0, \nabla
g_j(\bm{x}^\ast) \bm{d} = 0 \;\; \forall j \in J \right\}, 
$$

where $J = \{j; g_j(\bm{x}^\ast) = 0, \; \mu_j > 0\}$.

:::
:::

::::


## Sensitivity {.smaller}

::: {.callout-tip icon=false}
## Sensitivity Theorem
Consider the family of problems

$$
\begin{align}
\operatorname{minimize} & f(\bm{x}) \\
\text{subject to} & \bm{h}(\bm{x}) = \bm{b}, \quad \bm{g}(\bm{x}) \geq \bm{c}.
\end{align}
$$ {#eq-sensitivity-nlp}

Suppose that for $\bm{b} = \bm{0}$, $\bm{c} = \bm{0}$, there is a local solution
$\bm{x}^\ast$ that is a regular point and that together with the associated
Lagrange multipliers, $\bm{\lambda}, \bm{\mu} \geq \bm{0}$, satisfies the SOSC
for a strict local minimum. Assume further that no active inequality constraints
is degenerate.

Then for every $(\bm{b}, \bm{c}) \in \mathbb{R}^{m+p}$ in a region containing
$(\bm{0}, \bm{0})$, there is a solution $\bm{x}(\bm{b}, \bm{c})$, depending
continuously on $(\bm{b}, \bm{c})$, such that $\bm{x}(\bm{0}, \bm{0}) =
\bm{x}^\ast$ and $\bm{x}(\bm{b}, \bm{c})$ is a relative minimum of
@eq-sensitivity-nlp. Furthermore

$$
\begin{align}
\nabla_\bm{b} f(\bm{x}(\bm{b}, \bm{c}))\Bigg\rvert_{(\bm{0}, \bm{0})} &=
\bm{\lambda}^\top, \\
\nabla_\bm{c} f(\bm{x}(\bm{b}, \bm{c}))\Bigg\rvert_{(\bm{0}, \bm{0})} &=
\bm{\mu}^\top. \\
\end{align} 
$$ {#eq-sensitivity-grad-nlp}

:::



## Example &ndash; Soft-Margin Minimization in SVM {.smaller}

::: {.callout-important appearance="minimal"}
* In the original SVM example, the two data sets were separable, but in reality
data may come in as inseparable.
* An objective function would need to be added to the model.
* Let $\bm{A}$ represent the data matrix for $\bm{a}_i$ and $\bm{B}$ represent
the data for $\bm{b}_j$. Also let $\bm{1}$ denote the vector of all ones.
:::


:::: {.columns}

::: {.column width="50%"}

::: {.callout-warning icon=false}
## Optimization problem
$$
\begin{align}
\operatorname{minimize} & \frac{1}{2}|\bm{x}|^2 + \beta(\bm{1}^\top\bm{u} +
\bm{1}^\top \bm{v}) \\
\text{subject to} & \bm{A}^\top \bm{x} + \bm{1}x_0 + \bm{u} \geq 1 \\
& -\bm{B}^\top \bm{x} - \bm{1}x_0 + \bm{v} \geq \bm{1} \\
& \bm{u} \geq \bm{0}, \; \bm{v} \geq \bm{0}.
\end{align}
$$

* $\{\bm{y}: \bm{x}^\top \bm{y} + x_0 = 0\}$ is the desired hyperplane.
* $u_i$ and $v_j$ represent possible error margins of $\bm{a}_i$ and $\bm{b}_j$.
:::
::: {.callout-warning icon=false}
## Lagrangian
$$
\begin{align}
&\ell(\bm{x},x_0, \bm{u}, \bm{v}, \bm{\xi}_A, \bm{\xi}_B) =
\frac{1}{2}|\bm{x}|^2 + \beta(\bm{1}^\top \bm{u} + \bm{1}^\top \bm{v}) \\
&\phantom{234} - \bm{\xi}_A^\top (\bm{A}^\top \bm{x} + \bm{1}x_0 + \bm{u} -
\bm{1}) - \bm{\xi}_B^\top (-\bm{B}^\top \bm{x} - \bm{1}x_0 + \bm{v} -
\bm{1}).
\end{align}
$$
:::
:::


::: {.column width="50%"}

::: {.callout-important icon=false}
## First-Order Conditions
* (MSC): $\phantom{231445}\bm{\xi}_A \geq \bm{0} \;\; \bm{\xi}_B \geq \bm{0}$.
* (LDC): 
$$
\begin{align}
\nabla_\bm{x}\ell(\cdot) &= \bm{x} - \bm{A\xi}_A + \bm{B\xi}_B = \bm{0}, \\
\nabla_{x_0}\ell(\cdot) &= -\bm{1}^\top \bm{\xi}_A + \bm{1}^\top \bm{x}_B = 0,
\\
\nabla_\bm{u} \ell(\cdot) &= \beta \bm{1} - \bm{\xi}_A \geq \bm{0}, \\
\nabla_\bm{v} \ell(\cdot) &= \beta \bm{1} - \bm{\xi}_B \geq \bm{0}.
\end{align}
$$

* (CSC): 
$$
\begin{align}
\bm{\xi}_A^\top (\bm{A}^\top \bm{x} + \bm{1}x_0 + \bm{u} - \bm{1}) &= 0, \\ 
\bm{\xi}_B^\top (-\bm{B}^\top \bm{x} - \bm{1}x_0 + \bm{v} - \bm{1}) &= 0, \\ 
\bm{u}^\top(\beta \bm{1} - \bm{\xi}_A) &= 0, \\
\bm{v}^\top(\beta \bm{1} - \bm{\xi}_B) &= 0.
\end{align}
$$
:::

:::

::::
