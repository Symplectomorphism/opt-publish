# Equality Constrained Optimization Examples

## Example 1 &ndash; Geometric Prog.: Max Volume {.smaller}

::: {.callout-important appearance="minimal"}
We seek to construct a cardboard box of maximum volume, given a fixed area of
the cardboard.
:::

:::: {.columns}

::: {.column width="50%"}

<br>

::: {.callout-warning appearance="minimal"}
$$
\begin{align}
\operatorname{maximize} & xyz \\
\text{subject to} & (xy + yz + xz) = \frac{c}{2}, \quad c > 0 \,(\text{area}).
\end{align}
$$
:::


::: {.callout-important icon=false}
## First-Order Necessary Conditions
$$
\begin{align}
yz - \lambda (y+z) &= 0, \\
xz - \lambda (x+z) &= 0, \\
xy - \lambda (x+y) &= 0.
\end{align}
$$
:::

::: {.callout-important appearance="minimal"}
Since no variables can be zero, we have 
$$x = y = z = \sqrt{\frac{c}{6}} \quad \text{and} \quad \lambda =
\frac{\sqrt{6c}}{12}. $$
:::

:::

::: {.column width="50%"}
* Summing the FONC gives $(xy + yz + xz) - 2\lambda(x+y+z) = 0$.
* Using the constraint with this implies
$$ \frac{c}{2} - 2\lambda(x+y+z) = 0. $$
  - From this it is clear that $\lambda \neq 0$.
* Next, we see that none of $x$, $y$, and $z$ are zero.
  - This is because if, say, $x=0$, then $z$ becomes zero (second eq.), which
    implies $y=0$ from the first equation.
* Multiply the first by $x$, second by $y$ and subtract to obtain 
$$ \lambda(x-y)z = 0. $$
* Similarly operate on second and third to obtain $\lambda (y-z)x = 0$.
:::

::::


## Example 2 &ndash; Hanging Chain {.smaller}

::: {.callout-important appearance="minimal"}
A chain is suspended from two thin hooks that are $16$ ft. apart on a horizontal
line. Each link is one foot in length (measured inside). We wish to formulate
the problem to determine the equilibrium shape of the chain.

<center>
<img src="./contents/assets/hangingchain.png" width="70%" /img>
</center>

The solution can be found by minimizing the potential energy of the chain.

:::

:::: {.columns}

::: {.column width="50%"}

::: {.callout-warning appearance="minimal"}
* Let link $i$ span an $x$ distance of $x_i$ and a $y$ distance of $y_i$.
* Then $x_i^2 + y_i^2 = 1$.
* The potential energy of a link is its weight times its height.
* The total potential energy of the chain is the sum of those of each link.
:::

:::

::: {.column width="50%"}
::: {.callout-warning appearance="minimal"}
* With the top of the chain as reference and assuming the mass of each link is
concentrated at its center

$$
\begin{align}
P &= \frac{y_1}{2} + (y_1 + \frac{y_2}{2}) + (y_1 + y_2 + \frac{y_3}{2}) +
\cdots \\
&+ \, (y_1 + y_2 + \cdots + y_{n-1} + \frac{y_n}{2}) = \sum_{i=1}^n
(n-i+\frac{1}{2})y_i.
\end{align}
$$

where $n = 20$ in our example.
:::
:::

::::


## Example 2 &ndash; Hanging Chain {.smaller}

::: {.callout-important appearance="minimal"}
__Constraints__: The total $y$ displacement is zero and the total $x$
displacement is $16$.
:::

:::: {.columns}

::: {.column width="50%"}

::: {.callout-warning appearance="minimal"}
## Formulation
$$
\begin{align}
\operatorname{minimize} & \sum_{i=1}^n (n-i + \frac{1}{2})y_i \\
\text{subject to} & \sum_{i=1}^n y_i = 0, \quad \sum_{i=1}^n \sqrt{1 - y_i^2} = 16.
\end{align}
$$
:::

:::

::: {.column width="50%"}

::: {.callout-warning appearance="minimal"}
## First-Order Necessary Conditions
$$
\begin{align}
&(n-i + \frac{1}{2}) - \lambda + \frac{\mu y_i}{\sqrt{1-y_i^2}} = 0, \;\; i = 1,
2, \ldots, n. \\
\end{align}
$$
:::

:::

::::

* FONC directly leads to 

$$
y_i = -\frac{n - i + \frac{1}{2} - \lambda}{\sqrt{\mu^2 + (n - i + \frac{1}{2} -
\lambda)^2}}.
$$

* The solution is determined once the Lagrange multiplier are known.
  - They must be selected so that the solution satisfies the two constraints.

## Example 3 &ndash; Compressed Sensing {.smaller}

::: {.callout-important appearance="minimal"}
* We often want to find the sparsest solution to fit exact data measurements in
regression.
* That is, we want to minimize the number of nonzero entries in $\bm{x}$ that
satisfies a system of linear equations $\bm{Ax} = \bm{b}$.
  - This discrete cardinality function is not continuous so we approximate it by
    a continuous and mostly differentiable pseudo-norm function.
$$ \left(|\bm{x}|_p \right)^p = \sum_{j=1}^n |x_j|^p, \quad 0 < p \leq 1. $$
  - This becomes the L$_1$ norm function when $p = 1$.
:::


:::: {.columns}

::: {.column width="50%"}

::: {.callout-warning appearance="minimal"}
We want to solve the lienar equality constrained minimization problem.

$$
\begin{align}
\operatorname{minimize} & \sum_{j=1}^n |x_j|^p \\
\text{subject to} & \bm{Ax} - \bm{b} = \bm{0}.
\end{align}
$$

* The derivative of $|x_j|^p$, when $x_j \neq 0$ is $p(|x_j|^{p-1}
\operatorname{sign}(x_j))$.

* Let us remove those zero entries in $\bm{x}$, then the remaining nonzero
variables must still meet the FONC: for the $j^{\text{th}}$ column $\bm{a}_j$ of
$\bm{A}$ and some $\bm{\lambda}$

$$ p ( |x_j|^{p-1} \operatorname{sign}(x_j) ) - \bm{\lambda}^\top\bm{a}_j = 0,
\;\; \forall x_j \neq 0. $$
:::

:::

::: {.column width="50%"}

::: {.callout-warning appearance="minimal"}

* Multiplying each equation by $x_j$ from the right and summing them up, we have

$$ p \sum_{j: x_j \neq 0} |x_j|^p = \bm{\lambda}^\top \left(\sum_{j: x_j \neq 0}
\bm{a}_j x_j \right) = \bm{\lambda}^\top\bm{b} \leq |\bm{\lambda}| |\bm{b}|. $$

This means that the sum of the $p^{\text{th}}$ power of absolute values of the
nonzero entries is bounded above. For $p = \frac{1}{2}$, we have $\sum
\sqrt{|x_j|} \leq 2 |\bm{\lambda}| |\bm{b}|$. Moreover,

$$
|x_j|^{-\frac{1}{2}} \operatorname{sign}(x_j) = 2\bm{\lambda}^\top \bm{a}_j,
\;\; \Longrightarrow \;\; \frac{1}{\sqrt{|x_j|}} \leq 2 |\bm{\lambda}|
|\bm{a}_j|.
$$

* This establishes a lower bound on the absolute values of each nonzero entry of
any possible local minimizer of the problem.

:::

:::

::::
