[
  {
    "objectID": "02_basic_lp.html#optimization-theory-and-practice",
    "href": "02_basic_lp.html#optimization-theory-and-practice",
    "title": "02_basic_lp",
    "section": "Optimization Theory and Practice",
    "text": "Optimization Theory and Practice\n\n\nBasic Properties of Linear Programs\n\n\n\n\nInstructor: Aykut Satici, Ph.D.Â   Mechanical and Biomedical Engineering  Electrical and Computer Engineering  Boise State University, Boise, ID, USA\n\n\nTopics:  Introduction  Examples  Basic Feasible Solutions  Fundamental Theorem  Convex Geometry  Farkasâ€™s Lemma"
  },
  {
    "objectID": "02_basic_lp.html#standard-form",
    "href": "02_basic_lp.html#standard-form",
    "title": "02_basic_lp",
    "section": "Standard Form",
    "text": "Standard Form\n\n\n\n\n\n\nLinear Program (LP)\n\n\nAn LP is an optimization problem in which the objective function is linear in the unknowns and the constraints consist of linear (in)equalities.\n\n\n\n\n\n\n\n\n\nStandard form\n\n\nminimizeğœâŠ¤ğ±=c1x1+c2x2+â‹¯+cnxnsubject toğ€ğ±=ğ›andğ±â‰¥ğŸ.\n\\begin{align}\n\\operatorname{minimize} & \\bm{c}^\\top \\bm{x} = c_1x_1 + c_2x_2 + \\cdots + c_nx_n\n\\\\\n\\text{subject to} & \\bm{A}\\bm{x} = \\bm{b} \\quad \\text{and} \\quad \\bm{x} \\geq\n\\bm{0}.\n\\end{align}\n\n\nğœ,ğ±âˆˆâ„n\\bm{c}, \\bm{x} \\in \\mathbb{R}^n are column vectors, ğ€âˆˆâ„mÃ—n\\bm{A} \\in \\mathbb{R}^{m \\times n} a fat matrix (m&lt;nm &lt; n), ğ›âˆˆâ„m\\bm{b} \\in \\mathbb{R}^m a column vector.\nbib_iâ€™s, cic_iâ€™s and aija_{ij}â€™s are fixed real constants, and the xix_iâ€™s are real numbers to be determined.\nWe assume that each equation has been multiplied by minus unity, if necessary, so that each biâ‰¥0b_i \\geq 0."
  },
  {
    "objectID": "02_basic_lp.html#slack-variables",
    "href": "02_basic_lp.html#slack-variables",
    "title": "02_basic_lp",
    "section": "Slack Variables",
    "text": "Slack Variables\n\n\n\n\n\nmaximizec1x2+c2x2+â‹¯+cnxnsubject toa11x1+a12x2+â‹¯+a1nxnâ‰¤b1,a21x1+a22x2+â‹¯+a2nxnâ‰¤b2,â‹®â‹®am1x1+am2x2+â‹¯+amnxnâ‰¤bm,x1,x2,â€¦,xnâ‰¥0.\n\\begin{align}\n\\operatorname{maximize} & c_1x_2 + c_2x_2 + \\cdots + c_nx_n \\\\\n\\text{subject to} & a_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n &\\leq b_1, \\\\\n& a_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n &\\leq b_2, \\\\\n& \\vdots & \\vdots \\\\\n& a_{m1}x_1 + a_{m2}x_2 + \\cdots + a_{mn}x_n &\\leq b_m, \\\\\n& x_1, x_2, \\ldots, x_n \\geq 0.\n\\end{align}\n\n\n\n\nThis problem may be alternatively expressed as\n\n\n\nminimizeâˆ’c1x2âˆ’c2x2âˆ’â‹¯âˆ’cnxnsubject toa11x1+a12x2+â‹¯+a1nxn+xn+1=b1,a21x1+a22x2+â‹¯+a2nxn12+xn+2=b2,â‹®â‹®am1x1+am2x2+â‹¯+amnxn12345+xn+m=bm,x1,x2,â€¦,xn+1,â€¦,xn+mâ‰¥0.\n\\begin{align}\n\\operatorname{minimize} & -c_1x_2 - c_2x_2 - \\cdots - c_nx_n \\\\\n\\text{subject to} & a_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n + x_{n+1} &= b_1, \\\\\n& a_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n \\phantom{12} + x_{n+2} &= b_2, \\\\\n& \\vdots & \\vdots \\\\\n& a_{m1}x_1 + a_{m2}x_2 + \\cdots + a_{mn}x_n \\phantom{12345} + x_{n+m} &= b_m, \\\\\n& x_1, x_2, \\ldots, x_{n+1}, \\ldots, x_{n+m} \\geq 0.\n\\end{align}\n\n\n\n\n\n\nThe new nonnegative variables xn+ix_{n+i}, i=1,â€¦,mi=1, \\ldots, m convert inequalities to equalities are called slack variables.\n\nÂ \n\nThe constrant matrix now is transformed to ğ€â†’[ğ€ğˆ]\\bm{A} \\rightarrow \\begin{bmatrix} \\bm{A} & \\bm{I} \\end{bmatrix}."
  },
  {
    "objectID": "02_basic_lp.html#surplus-variables",
    "href": "02_basic_lp.html#surplus-variables",
    "title": "02_basic_lp",
    "section": "Surplus Variables",
    "text": "Surplus Variables\n\nIf the linear equalities are reversed so that a typical inequality is\n\nai1x1+ai2x2+â‹¯+ainxnâ‰¥bi, a_{i1}x_1 + a_{i2}x_2 + \\cdots + a_{in} x_n \\geq b_i, \nit is clear that this is equivalent to\nai1x1+ai2x2+â‹¯+ainxnâˆ’yi=bi, a_{i1}x_1 + a_{i2}x_2 + \\cdots + a_{in} x_n - y_i = b_i, \nwith yiâ‰¥0y_i \\geq 0.\n\nVariables, such as yiy_i, adjoined in this fashion to convert a â€œgreater than or equal toâ€ inequality to equality are called surplus variables.\nBy suitably multiplying by minus unity, and adjoining slack and surplus variables, any set of linear inequalities can be converted to standard form."
  },
  {
    "objectID": "02_basic_lp.html#free-variables-first-method",
    "href": "02_basic_lp.html#free-variables-first-method",
    "title": "02_basic_lp",
    "section": "Free variables â€“ First Method",
    "text": "Free variables â€“ First Method\n\n\n\nSuppose one or more of the unknown variables is not required to be nonnegative, say, x1â‰¥0x_1 \\geq 0 is not present so that x1x_1 is free to take on either positive or negative values.\n\n\n\n\nWe then write x1=u1âˆ’v1x_1 = u_1 - v_1, and require that u1,v1â‰¥0u_1, v_1 \\geq 0.\nWe substitute u1âˆ’v1u_1 - v_1 for x1x_1 everywhere.\nThe problem is then expressed in terms of the n+1n+1 variables u1,v1,x2,x3,â€¦,xnu_1, v_1, x_2, x_3, \\ldots, x_n.\nThere is a certain degree of redundancy introduced in this technique since a constant added to u1u_1 and v1v_1 does not change x1x_1.\nNevertheless, the simplex method can still be used to find the solution."
  },
  {
    "objectID": "02_basic_lp.html#free-variables-second-method",
    "href": "02_basic_lp.html#free-variables-second-method",
    "title": "02_basic_lp",
    "section": "Free variables â€“ Second Method",
    "text": "Free variables â€“ Second Method\n\nTake the same free variable situation, i.e, x1x_1 is free to take on positive or negative values.\nOne can take any one of the mm equality constraints which has a nonzero coefficient for x1x_1, say, for example,\n\nai1x1+ai2x2+â‹¯+ainxn=bi, a_{i1}x_1 + a_{i2}x_2 + \\cdots + a_{in}x_n = b_i, \nwhere ai1â‰ 0a_{i1} \\neq 0.\n\nThen x1x_1 can be expressed as a linear combination of the other variables, plus a constant.\nThis expression can be substituted everywhere for x1x_1 and we are led to a new problem of exactly the same form but expressed in terms of the variables x2,x3,â€¦,xnx_2, x_3, \\ldots, x_n only.\nFurthermore, the ithi^{\\text{th}} equation, used to determine x1x_1 is now identically zero and it too can be eliminated.\nWe obtain a linear program having nâˆ’1n-1 variables and mâˆ’1m-1 constraint equations."
  },
  {
    "objectID": "02_basic_lp.html#example-specific-case",
    "href": "02_basic_lp.html#example-specific-case",
    "title": "02_basic_lp",
    "section": "Example â€“ Specific Case",
    "text": "Example â€“ Specific Case\n\n\n\n\n\nminimizex1+3x2+4x3subject tox1+2x2+3x3=5,2x1+3x2+x3=6,x2,x3â‰¥0.\n\\begin{align}\n\\operatorname{minimize} & x_1 + 3x_2 + 4x_3 \\\\\n\\text{subject to} & x_1 + 2x_2 + 3x_3 = 5, \\\\\n& 2x_1 + 3x_2 + x_3 = 6, \\\\\n& x_2, x_3 \\geq 0.\n\\end{align}\n\n\n\n\n\nx1x_1 is free, so we can solve for it from the first constraint, obtaining\nx1=5âˆ’2x2âˆ’x3.(1) x_1 = 5 - 2x_2 - x_3. \\qquad(1)\n\n\nSubstituting this into the objective and the second constraint, we obtain the equivalent problem\nminimizex2+3x3subject tox2+x3=4,x2,x3â‰¥0.\n\\begin{align}\n\\operatorname{minimize} & x_2 + 3x_3 \\\\\n\\text{subject to}& x_2 + x_3 = 4, \\\\\n& x_2, x_3 \\geq 0.\n\\end{align}\n\nwhich is a problem in standard form.\nAfter the smaller problem is solved (what is the answer?) the value for x1=âˆ’3x_1 = -3 can be found from EquationÂ 1."
  },
  {
    "objectID": "02_basic_lp.html#example-1-the-diet-problem",
    "href": "02_basic_lp.html#example-1-the-diet-problem",
    "title": "02_basic_lp",
    "section": "Example 1 â€“ The Diet Problem",
    "text": "Example 1 â€“ The Diet Problem\n\n\n\nDetermine the most economical diet that satisfies the basic minimum nutritional requirements for good health\n\n\n\n\n\n\n\n\n\nThere are available nn different foods.\nThere are mm basic nutritional ingredients,\nEach unit of food jj contains aija_{ij} units of the ithi^{\\text{th}} nutrient.\n\n\n\njthj^{\\text{th}} food sells at a price cjc_j per unit.\nEach individual must receive at least bib_i units of the ithi^{\\text{th}} nutrient per day.\n\n\n\n\nIf we denote by xjx_j the number of units of food jj in the diet, the problem is to select xjx_jâ€™s to minimize the total cost c1x1+c2x2+â‹¯+cnxn c_1x_1 + c_2x_2 + \\cdots + c_nx_n \n\n\nsubject to the nutritional constraints ai1x1+ai2x2+â‹¯+ainxnâ‰¥bi,i=1,â€¦,m, a_{i1}x_1 + a_{i2}x_2 + \\cdots + a_{in}x_n \\geq b_i, \\; i=1, \\ldots, m, \n\n\nand the nonnegative constraints x1â‰¥0,x2â‰¥0,â€¦,xnâ‰¥0, x_1 \\geq 0, x_2 \\geq 0, \\ldots, x_n \\geq 0,  on the food quantities.\n\n\n\n\n\nThis problem can be converted to standard form by subtracting a nonnegative surplus variable from the left side of each of the mm linear inequalities."
  },
  {
    "objectID": "02_basic_lp.html#example-2-the-resource-allocation-problem",
    "href": "02_basic_lp.html#example-2-the-resource-allocation-problem",
    "title": "02_basic_lp",
    "section": "Example 2â€“ The Resource-Allocation Problem",
    "text": "Example 2â€“ The Resource-Allocation Problem\n\n\n\nA facility is capable of manufacturing nn different products.\nEach product can be produced at any level xjâ‰¥0x_j \\geq 0, j=1,2,â€¦,nj=1, 2, \\ldots, n.\nEach unit of the jthj^{\\text{th}} product needs aija_{ij} units of the ithi^{\\text{th}} resource, i=1,2,â€¦,mi = 1, 2, \\ldots, m.\n\n\n\nEach product may require various amounts of mm different resources.\nEach unit of the jthj^{\\text{th}} product can sell for Ï€j\\pi_j dollars.\nEach bib_i, i=1,2,â€¦,mi = 1, 2, \\ldots, m describe the available quantities of the mm resources.\n\n\n\n\nWe wish to manufacture products at maximum revenue\nmaximizeÏ€1x1+Ï€2x2+â‹¯+Ï€nxn\n\\begin{align}\n\\operatorname{maximize} & \\pi_1x_1 + \\pi_2x_2 + \\cdots + \\pi_nx_n\n\\end{align}\n\n\n\nsubject to the resource constraints\nsubject toai1x1+ai2x2+â‹¯+ainxnâ‰¤bi,i=1,â€¦,m\n\\begin{align}\n\\text{subject to} & a_{i1}x_1 + a_{i2}x_2 + \\cdots + a_{in}x_n \\leq b_i, \\; i=1,\n\\ldots, m\n\\end{align}\n\nand the nonnegativity consraints on all production variables.\n\n\n\nThe problem can also be interpreted as\n\nfund nn different activities, where\nÏ€j\\pi_j is the full reward from the jthj^{\\text{th}} activity,\nxjx_j is restricted to 0â‰¤xjâ‰¤10 \\leq x_j \\leq 1, representing the funding level from 0%0\\% to 100%100\\%."
  },
  {
    "objectID": "02_basic_lp.html#example-3-the-transportation-problem",
    "href": "02_basic_lp.html#example-3-the-transportation-problem",
    "title": "02_basic_lp",
    "section": "Example 3 â€“ The Transportation Problem",
    "text": "Example 3 â€“ The Transportation Problem\n\n\n\nQuantities a1,a2,â€¦,ama_1, a_2, \\ldots, a_m of a certain product are to be shipped from mm locations.\nShipping a unit of product from origin ii to destination jj costs cijc_{ij}.\n\n\n\nThese products will be received in amounts of b1,b2,â€¦,bnb_1, b_2, \\ldots, b_n at each of nn destinations.\nWe want to determine the amounts xijx_{ij} to be shipped between each origin-destination pair i=1,2,â€¦,mi = 1, 2, \\ldots, m; j=1,2,â€¦,nj=1, 2, \\ldots, n.\n\n\n\n\n\n\n\nx11x_{11}\nx12x_{12}\nâ‹¯\\cdots\nx1nx_{1n}\n|\na1a_1\n\n\nx21x_{21}\nx22x_{22}\nâ‹¯\\cdots\nx2nx_{2n}\n|\na2a_2\n\n\nâ‹®\\vdots\nâ‹®\\vdots\nâ‹®\\vdots\nâ‹®\\vdots\n|\nâ‹®\\vdots\n\n\nxm1x_{m1}\nxm2x_{m2}\nâ‹¯\\cdots\nxmnx_{mn}\n|\nama_m\n\n\nâ€”â€”\nâ€”â€”\nâ€”â€”\nâ€”â€”\n\n\n\n\nb1b_{1}\nb2b_{2}\nâ‹¯\\cdots\nbnb_{n}\n\n\n\n\n\n\n\nThe ithi^{\\text{th}} row in this array defines the variables associated with the ithi^{\\text{th}} origin.\nThe jthj^{\\text{th}} column defines the variables associated with the jthj^{\\text{th}} destination.\nProblem: find the nonnegative variables xijx_{ij} so that the sum across the ithi^{\\text{th}} row is aja_j, the sum down the jthj^{\\text{th}} column is bjb_j, and the weighted sum âˆ‘j=1nâˆ‘i=1mcijxij\\sum_{j=1}^n\\sum_{i=1}^m c_{ij}x_{ij} is minimized."
  },
  {
    "objectID": "02_basic_lp.html#example-4-the-maximal-flow-problem",
    "href": "02_basic_lp.html#example-4-the-maximal-flow-problem",
    "title": "02_basic_lp",
    "section": "Example 4 â€“ The Maximal Flow Problem",
    "text": "Example 4 â€“ The Maximal Flow Problem\n\n\n\n\n\nMaximal flow problem\n\n\nDetermine the maximal flow that can be established in such a network.\n\n\n\nmaximizefsubject toâˆ‘j=1nx1jâˆ’âˆ‘j=1nxj1âˆ’f=0,âˆ‘j=1nxijâˆ’âˆ‘j=1nxji=0,iâ‰ 1,m,âˆ‘j=1nxmjâˆ’âˆ‘j=1nxjm+f=0,0â‰¤xijâ‰¤kij,âˆ€i,j,\n\\begin{align}\n\\operatorname{maximize} & f \\\\\n\\text{subject to} & \\sum_{j=1}^n x_{1j} - \\sum_{j=1}^n x_{j1} - f = 0, \\\\\n& \\sum_{j=1}^n x_{ij} - \\sum_{j=1}^n x_{ji}  = 0, \\quad i \\neq 1, m, \\\\\n& \\sum_{j=1}^n x_{mj} - \\sum_{j=1}^n x_{jm} + f = 0, \\\\\n& 0 \\leq x_{ij} \\leq k_{ij}, \\quad \\forall i, j,\n\\end{align}\n\nwhere kij=0k_{ij} = 0 for those no-arc pairs (i,j)(i,j).\n\n\n\n\n\n\n\n\nCapacitated network in which two special nodes, called the source (node 1); and the sink (node mm) are distinguished.\nAll other nodes must satisfy the conservation requirement: net flow into these nodes must be zero.\n\nthe source may have a net outflow,\nthe sink may have a net inflow.\n\nThe outlow ff of the source will equal the inflow of the sink."
  },
  {
    "objectID": "02_basic_lp.html#example-5-a-supply-chain-problem",
    "href": "02_basic_lp.html#example-5-a-supply-chain-problem",
    "title": "02_basic_lp",
    "section": "Example 5 â€“ A Supply-Chain Problem",
    "text": "Example 5 â€“ A Supply-Chain Problem\n\n\n\nA warehouse is buying and selling stock of a certain commodity in order to maximize profit over a certain length of time.\n\n\n\n\n\n\nWarehouse has a fixed capacity CC.\nThe price, pip_i, of the commodity is known to fluctuate over a number of time periods, say months, indexed by ii.\nThe warehouse is originally empty and is required to be empty at the end of the last period.\n\n\n\nThere is a cost rr per unit of holding stock for one period.\nIn any period the same price holds for both purchase and sale.\nxix_i: level of stock in the warehouse at the beginning of period ii, uiu_i: amount bought during this period, sis_i: amount sold during this period.\n\n\n\nmaximizeâˆ‘i=1n(pi(siâˆ’ui)âˆ’rxi)subject toxi+1=xi+uiâˆ’si,i=1,2,â€¦,nâˆ’1,0=xn+unâˆ’sn,xi+zi=C,i=2,â€¦,n,x1=0,xiâ‰¥0,uiâ‰¥0,siâ‰¥0,ziâ‰¥0,\n\\begin{align}\n\\operatorname{maximize} & \\sum_{i=1}^n \\left(p_i(s_i - u_i) - rx_i \\right) \\\\\n\\text{subject to} & x_{i+1} = x_i + u_i - s_i, \\quad i = 1, 2, \\ldots, n-1, \\\\\n& 0 = x_n + u_n - s_n, \\\\\n& x_i + z_i = C, \\quad i = 2, \\ldots, n, \\\\\n& x_1 = 0, x_i \\geq 0, u_i \\geq 0, s_i \\geq 0, z_i \\geq 0,\n\\end{align}\n\nwhere ziz_i is a slack variable."
  },
  {
    "objectID": "02_basic_lp.html#example-6-linear-classifier-and-support-vector-machine",
    "href": "02_basic_lp.html#example-6-linear-classifier-and-support-vector-machine",
    "title": "02_basic_lp",
    "section": "Example 6 â€“ Linear Classifier and Support Vector Machine",
    "text": "Example 6 â€“ Linear Classifier and Support Vector Machine\n\n\n\n\n\ndd-dimensional data points are to be classified into two distinct classes.\n\n\n\n\nIn general, we have vectors ğšiâˆˆâ„d\\bm{a}_i \\in \\mathbb{R}^d for i=1,2,â€¦,n1i=1, 2, \\ldots, n_1 and vector ğ›jâˆˆâ„d\\bm{b}_j \\in \\mathbb{R}^d for j=1,2,â€¦,n2j = 1, 2, \\ldots, n_2.\nWe wish to find a hyperplane that separates ğši\\bm{a}_iâ€™s from the ğ›j\\bm{b}_jâ€™s, i.e., find a slope-vector yâˆˆâ„dy \\in \\mathbb{R}^d and an intercept Î²\\beta such that\n\nğšiâŠ¤ğ²+Î²â‰¥1,âˆ€i,ğ›jâŠ¤ğ²+Î²â‰¤1,âˆ€j,\n\\begin{align}\n  \\bm{a}_i^\\top \\bm{y} + \\beta &\\geq 1, \\quad \\forall i, \\\\\n  \\bm{b}_j^\\top \\bm{y} + \\beta &\\leq 1, \\quad \\forall j, \\\\\n\\end{align}\n\nwhere {ğ±:ğ±ğ²+Î²=0}\\{\\bm{x}: \\bm{x}^\\bm{y} + \\beta = 0\\} is the desired hyperplane.\n\nThe separation is defined by the fixed margins +1+1 and âˆ’1-1, which could be made soft or variable later.\n\n\n\n\n\n\n\n\nExample\n\n\n\nTwo-dimensional data points may be grade averages in science and humanities for different students.\nWe also know the academic major of each student, as being science or humanities, which serves as the classification."
  },
  {
    "objectID": "02_basic_lp.html#example-7-markov-decision-process-mdp",
    "href": "02_basic_lp.html#example-7-markov-decision-process-mdp",
    "title": "02_basic_lp",
    "section": "Example 7 â€“ Markov Decision Process (MDP)",
    "text": "Example 7 â€“ Markov Decision Process (MDP)\n\n\n\n\n\nMarkov Decision Process\n\n\nAn MDP problem is defined by a finite number of states, indexed by i=1,â€¦,mi = 1, \\ldots, m, where each state has a set of a finite number of actions, ğ’œi\\mathcal{A}_i, to take.\n\n\n\n\nEach action, say jâˆˆğ’œij \\in \\mathcal{A}_i, is associated with\n\nan immediate cost cjc_j of taking,\na probability distribution ğ©jâˆˆâ„m\\bm{p}_j \\in \\mathbb{R}^m to transfer to all possible states at the next time period.\n\nA stationary policy for the decision maker is a function Ï€={Ï€1,Ï€2,â‹¯,Ï€m}\\pi = \\{\\pi_1, \\pi_2, \\cdots, \\pi_m\\} that specifies a single action in every state, Ï€iâˆˆğ’œi\\pi_i \\in \\mathcal{A}_i.\n\n\n\n\n\n\n\n\nMDP Problem\n\n\nFind a stationary policy to minimize or maximize the discounted sum of expected costs or rewards over the infinite time horizon with a discount factor 0â‰¤Î³&lt;10 \\leq \\gamma &lt; 1, when the process starts from state i0i^0:\nâˆ‘t=0âˆÎ³tğ”¼[cÏ€it]\n\\sum_{t=0}^\\infty \\gamma^t \\mathbb{E}[c_{\\pi_{i^t}}]"
  },
  {
    "objectID": "02_basic_lp.html#example-7-markov-decision-process-mdp-1",
    "href": "02_basic_lp.html#example-7-markov-decision-process-mdp-1",
    "title": "02_basic_lp",
    "section": "Example 7 â€“ Markov Decision Process (MDP)",
    "text": "Example 7 â€“ Markov Decision Process (MDP)\n\n\n\n\n\nMaze Runner Game\n\n\nEach square represents a state, where each of states {1,2,3,4}\\{1, 2, 3, 4\\} has two possible actions to take:\n\nred action: moves to the next state at the next time period,\nblue action: shortcut move, with a probability distribution, to a state at the next time period.\n\nEach state of {5,6}\\{5, 6\\} has only one action\n\nmoving to state 66 (Exit state),\nmoving to state 11 (Start state).\n\nAll actions have zero cost, except state 55â€™s (Trap state) action, which has a 11-unit cost to get out.\nSuppose that the game is played infinitely, what is the optimal policy? That is, which action is best to take for every state at any time, to minimize the present-expected total cost?\n\n\n\n\n\n\n\n\n\n\nMDP Problem\n\n\nTwo constraints for the two actions of State 11\ny1âˆ’Î³y2â‰¤0,y1âˆ’Î³(14y3+14y5+14y6)â‰¤0.\ny_1 - \\gamma y_2 \\leq 0, \\;\\; y_1 - \\gamma(\\frac{1}{4}y_3 +\n\\frac{1}{4}y_5 + \\frac{1}{4}y_6) \\leq 0.\n\nOnly constraint for the single action of State 55\ny5âˆ’Î³y6â‰¤1.\ny_5 - \\gamma y_6 \\leq 1."
  },
  {
    "objectID": "02_basic_lp.html#example-7-markov-decision-process-mdp-2",
    "href": "02_basic_lp.html#example-7-markov-decision-process-mdp-2",
    "title": "02_basic_lp",
    "section": "Example 7 â€“ Markov Decision Process (MDP)",
    "text": "Example 7 â€“ Markov Decision Process (MDP)\n\nLet yi*y_i^\\ast, i=1,â€¦,mi = 1, \\ldots, m represent the optimal present-expected cost when the process starts at state ii and time 00\n\nalso called the cost-to-go value of state ii.\nThe yi*y_i^\\astâ€™s follow Bellmanâ€™s principle of optimality:\n\n\nyi*=minjâˆˆğ’œi(cj+Î³ğ©jâŠ¤ğ²*),\ny_i^\\ast = \\operatorname{min}_{j \\in \\mathcal{A}_i} (c_j + \\gamma \\bm{p}_j^\\top\n\\bm{y}^\\ast),\n\nwhere cjc_j is the immediate cost of taking action jâˆˆğ’œij \\in \\mathcal{A}_i at the current time period, and ğ©jâŠ¤ğ²*\\bm{p}_j^\\top \\bm{y}^\\ast is the optimal expected cost from the next time period, and then on.\n\nWhen yi*y_i^\\ast is known for every state, the optimal action in each state would be\n\nÏ€i*=argminjâˆˆğ’œi(cj+Î³ğ©jâŠ¤ğ²*),âˆ€i.\n\\pi_i^\\ast = \\operatorname{arg min}_{j \\in \\mathcal{A}_i} (c_j + \\gamma\n\\bm{p}_j^\\top \\bm{y}^\\ast), \\quad \\forall i."
  },
  {
    "objectID": "02_basic_lp.html#example-7-markov-decision-process-mdp-3",
    "href": "02_basic_lp.html#example-7-markov-decision-process-mdp-3",
    "title": "02_basic_lp",
    "section": "Example 7 â€“ Markov Decision Process (MDP)",
    "text": "Example 7 â€“ Markov Decision Process (MDP)\n\nOne observes that ğ²*âˆˆâ„m\\bm{y}^\\ast \\in \\mathbb{R}^m is a fixed-point of the Bellman operator, and it can be computed by the following linear program.\n\nmaximizeâˆ‘i=1myisubject toy1âˆ’Î³ğ©jâŠ¤ğ²â‰¤cjâˆ€jâˆˆğ’œ1,â‹®yiâˆ’Î³ğ©jâŠ¤ğ²â‰¤cjâˆ€jâˆˆğ’œi,â‹®ymâˆ’Î³ğ©jâŠ¤ğ²â‰¤cjâˆ€jâˆˆğ’œm.\n\\begin{align}\n\\operatorname{maximize} & \\sum_{i=1}^m y_i \\\\\n\\text{subject to} & y_1 - \\gamma \\bm{p}_j^\\top \\bm{y} \\leq c_j \\quad \\forall j \\in \\mathcal{A}_1, \\\\\n& \\vdots \\\\\n& y_i - \\gamma \\bm{p}_j^\\top \\bm{y} \\leq c_j \\quad \\forall j \\in \\mathcal{A}_i, \\\\\n& \\vdots \\\\\n& y_m - \\gamma \\bm{p}_j^\\top \\bm{y} \\leq c_j \\quad \\forall j \\in \\mathcal{A}_m. \\\\\n\\end{align}\n\n\nBasically, we relax the â€œmin\\operatorname{min}â€ operator to â€œâ‰¤\\leqâ€ from Bellmanâ€™s principle and make them into the constraints and then maximize the sum of yiy_iâ€™s as the objective.\nWhen the objective is maximized, at least one ineqality constraint in ğ’œi\\mathcal{A}_i must become equal for every state ii so that ğ²\\bm{y} is a fixed point solution of the Bellman operator."
  },
  {
    "objectID": "02_basic_lp.html#basic-solutions",
    "href": "02_basic_lp.html#basic-solutions",
    "title": "02_basic_lp",
    "section": "Basic Solutions",
    "text": "Basic Solutions\n\n\n\n\n\nSystem of equalities\n\n\nğ€ğ±=ğ›(2) \\bm{Ax} = \\bm{b}  \\qquad(2)\n\nFrom the nn columns of ğ€\\bm{A}, we select a set of mm linearly independent columns.\nWLOG, assume that the first mm columns of ğ€\\bm{A} are selected: denote the mÃ—mm \\times m matrix determined by these columns by ğ\\bm{B}.\nThe matrix ğ\\bm{B} is nonsingular and we may uniquely solve the equation ğğ±ğ=ğ›orğ±ğ=ğâˆ’1ğ› \\bm{Bx_B} = \\bm{b} \\quad \\text{or} \\quad \\bm{x_B} = \\bm{B}^{-1}\\bm{b} \n\n\n\n\n\nWe refer to ğ\\bm{B} as a basis, since ğ\\bm{B} consists of mm linearly independent columns that can be regarded as a basis for â„m\\mathbb{R}^m.\n\n\n\n\n\nDefinition\n\n\nGiven the set of mm simultaneous linear equations nn unknowns EquationÂ 2, let ğ\\bm{B} be any nonsingular mÃ—mm \\times m submatrix made up of columns of ğ€\\bm{A}. Then, if all nâˆ’mn-m components of ğ±\\bm{x} not associated with columns of ğ\\bm{B} are set equal to zero, the solution to the resulting set of equations is said to be a basic solution to EquationÂ 2 with respect to basis ğ\\bm{B}.\n\n\n\n\n\n\nDefinition\n\n\nThe components of ğ±\\bm{x} associated with the columns of ğ\\bm{B}, denoted by the subvector ğ±B\\bm{x}_B according to the same column index order in ğ\\bm{B}, are called basic variables.\n\n\n\n\n\n\n\n\nFull-Rank Assumption\n\n\nThe mÃ—nm \\times n matrix ğ€\\bm{A} has m&lt;nm &lt; n, and the mm rows of ğ€\\bm{A} are linearly independent."
  },
  {
    "objectID": "02_basic_lp.html#basic-feasible-solutions-1",
    "href": "02_basic_lp.html#basic-feasible-solutions-1",
    "title": "02_basic_lp",
    "section": "Basic Feasible Solutions",
    "text": "Basic Feasible Solutions\n\n\n\nDefinition\n\n\nIf one or more of the basic variables in a basic solution has value zero, that solution is said to be a degenerate basic solution.\n\n\n\n\nIn a nondegenerate basic solution, the basic variables, and hence the basis ğ\\bm{B}, can be immediately identified from the positive components of the solution.\n\nThere is ambiguity associated with a degenerate basic solution â€“ some of the nonbasic variables can be interchanged!\n\n\n\n\n\nDefinition\n\n\nA vector ğ±\\bm{x} satisfying\nğ€ğ±=ğ›,ğ±â‰¥ğŸ,(3)\n\\bm{Ax} = \\bm{b}, \\quad \\bm{x} \\geq \\bm{0},\n \\qquad(3)\nis said to be feasible for these constraints. A feasible solution to the constraints EquationÂ 3 that is also basic is said to be a basic feasible solution; if this solution is also a degenerate basic solution, it is called a degenerate basic feasible solution."
  },
  {
    "objectID": "02_basic_lp.html#the-fundamental-theorem-of-lp",
    "href": "02_basic_lp.html#the-fundamental-theorem-of-lp",
    "title": "02_basic_lp",
    "section": "The Fundamental Theorem of LP",
    "text": "The Fundamental Theorem of LP\n\nCorresponding to a linear program in standard form\nminimizeğœâŠ¤ğ±subject toğ€ğ±=ğ›,ğ±â‰¥0(4)\n\\begin{align}\n\\operatorname{minimize} & \\bm{c}^\\top \\bm{x}  \\\\\n\\text{subject to} & \\bm{Ax} = \\bm{b}, \\;\\; \\bm{x} \\geq 0\n\\end{align} \n \\qquad(4)\na feasible solution to the constraints that achieves the minimum value of the objective function subject to those constraints is said to be an optimal feasible solution. If this solution is basic, it is an optimal basic feasible solution.\n\n\n\n\n\nFundamental Theorem\n\n\nGiven a linear program in standard form EquationÂ 4 where ğ€\\bm{A} is an mÃ—nm \\times n matrix of rank mm,\n\nif there is a feasible solution, there is a basic feasible solution;\nif there is an optimal feasible solution, there is an optimal basic feasible solution.\n\n\n\n\n\n\n\n\n\nProof (1)\n\n\nDenote the columns of ğ€\\bm{A} by ğš1,ğš2,â€¦,ğšn\\bm{a}_1, \\bm{a}_2, \\ldots, \\bm{a}_n. Suppose ğ±=(x1,x2,â€¦,xn)\\bm{x} = (x_1, x_2, \\ldots, x_n) is a feasible solution. Then, in terms of the columns of ğ€\\bm{A}, this solution satisfies\nx1ğš1+x2ğš2+â‹¯+xnğšn=ğ›. x_1\\bm{a}_1 + x_2\\bm{a}_2 + \\cdots + x_n\\bm{a}_n = \\bm{b}."
  },
  {
    "objectID": "02_basic_lp.html#the-fundamental-theorem-of-lp-1",
    "href": "02_basic_lp.html#the-fundamental-theorem-of-lp-1",
    "title": "02_basic_lp",
    "section": "The Fundamental Theorem of LP",
    "text": "The Fundamental Theorem of LP\n\n\n\nProof (1) - Continued -\n\n\nAssume that exactly pp of the variables xix_i are greater than zero, and wlog, that they are the first pp variables. Thus\nx1ğš1+x2ğš2+â‹¯+xpğšp=ğ›.(5) x_1 \\bm{a}_1 + x_2 \\bm{a}_2 + \\cdots + x_p \\bm{a}_p = \\bm{b}.  \\qquad(5)\nThere are now two cases, corresponding as to whether the set ğš1,ğš2,â€¦,ğšp\\bm{a}_1, \\bm{a}_2, \\ldots, \\bm{a}_p is linearly independent or linearly dependent.\nCase 1: Assume ğš1,ğš2,â€¦,ğšp\\bm{a}_1, \\bm{a}_2, \\ldots, \\bm{a}_p are linearly independent. Then clearly, pâ‰¤mp \\leq m. If p=mp = m, the solution is basic and the proof is complete. If p&lt;mp &lt; m, then, since ğ€\\bm{A} has rank mm, mâˆ’pm-p vectors can be found from the remaining nâˆ’pn-p vectors so that the resulting set of mm vectors is linearly independent. Assigning the value zero to the corresponding mâˆ’pm-p variables yields a (degenerate) basic feasible solution.\nCase 2: Assume ğš1,ğš2,â€¦,ğšp\\bm{a}_1, \\bm{a}_2, \\ldots, \\bm{a}_p are linearly dependent. Then there is a nontrivial linear combination of these vectors that is zero. Thus there are constants y1,y2,â€¦,ypy_1, y_2, \\ldots, y_p, at least one of which can be assumed to be positive, such that\ny1ğš1+y2ğš2+â‹¯+ypğšp=ğŸ.(6) y_1\\bm{a}_1 + y_2\\bm{a}_2 + \\cdots + y_p\\bm{a}_p = \\bm{0}.  \\qquad(6)\nMultiplying this equation by a scalar Îµ\\varepsilon and subtracting it from EquationÂ 5, we obtain\n(x1âˆ’Îµy1)ğš1+(x2âˆ’Îµy2)ğš2+â‹¯+(xpâˆ’Îµyp)ğšp=ğ›.(7) (x_1 - \\varepsilon y_1)\\bm{a}_1 + (x_2 - \\varepsilon y_2)\\bm{a}_2 + \\cdots + \n   (x_p - \\varepsilon y_p)\\bm{a}_p = \\bm{b}.  \\qquad(7)\nThis equation hods for every Îµ\\varepsilon, and for each Îµ\\varepsilon the components xjâˆ’Îµyjx_j - \\varepsilon y_j correspond to a solution of the linear equalities â€” although they may violate xiâˆ’Îµyiâ‰¥0x_i - \\varepsilon y_i \\geq 0. Denoting ğ²=(y1,y2,â€¦,yp,0,0,â€¦,0)\\bm{y} = (y_1, y_2, \\ldots, y_p, 0, 0, \\ldots, 0), we see that for any Îµ\\varepsilon\nğ±âˆ’Îµğ²(8) \\bm{x} - \\varepsilon \\bm{y}  \\qquad(8)\nis a solution to the equalities. For Îµ=0\\varepsilon = 0, this reduces to the original feasible solution."
  },
  {
    "objectID": "02_basic_lp.html#the-fundamental-theorem-of-lp-2",
    "href": "02_basic_lp.html#the-fundamental-theorem-of-lp-2",
    "title": "02_basic_lp",
    "section": "The Fundamental Theorem of LP",
    "text": "The Fundamental Theorem of LP\n\n\n\nProof (1) - Continued -\n\n\nAs Îµ\\varepsilon is increased from zero, the various components increase, decrease, or remain constant, depending upon whether the correspoding yiy_i is negative, positive, or zero. Since we assume at least one yiy_i is positive, at least one component will decrease as Îµ\\varepsilon is increased. we increase Îµ\\varepsilon to the first point where one or more components become zero. Specifically, we set\nÎµ=min{xiyi:yi&gt;0}. \\varepsilon = \\operatorname{min}\\left\\{\\frac{x_i}{y_i}: y_i &gt; 0 \\right\\}. \nFor this value of Îµ\\varepsilon, the solution given by EquationÂ 8 is feasible and has at most pâˆ’1p-1 positive variables. Repeating this process as necessary, we can eliminate positive variables until we have a feasible solution with corresponding columns that are linearly independent. At that point Case 1 applies.\n\n\n\n\n\n\n\n\nProof (2)\n\n\nLet ğ±=(x1,x2,â€¦,xn)\\bm{x} = (x_1, x_2, \\ldots, x_n) be an optimal feasible solution and, as in the proof of (1) above, suppose there are exactly pp positive variables x1,x2,â€¦,xpx_1, x_2, \\ldots, x_p. Again, there are two cases; and Case 1, corresponding to the linear independence is exactly the same as before.\nCase 2 also goes exactly the same as before, but it must be shown that for any Îµ\\varepsilon the solution EquationÂ 8 is optimal. To show this, note that the value of the solution ğ±âˆ’Îµğ²\\bm{x} - \\varepsilon\\bm{y} is\nğœâŠ¤ğ±âˆ’ÎµğœâŠ¤ğ².(9) \\bm{c}^\\top \\bm{x} - \\varepsilon \\bm{c}^\\top \\bm{y}.  \\qquad(9)\nFor Îµ\\varepsilon sufficiently small in magnitude, ğ±âˆ’Îµğ²\\bm{x} - \\varepsilon \\bm{y} is a feasible solution for positive or negative values of Îµ\\varepsilon. Thus, we conclude that ğœâŠ¤ğ²=ğŸ\\bm{c}^\\top \\bm{y} = \\bm{0} (why?).\n\n\n\n\n\n\n\n\nProof (2)\n\n\nLet ğ±=(x1,x2,â€¦,xn)\\bm{x} = (x_1, x_2, \\ldots, x_n) be an optimal feasible solution and, as in the proof of (1) above, suppose there are exactly pp positive variables x1,x2,â€¦,xpx_1, x_2, \\ldots, x_p. Again, there are two cases; and Case 1, corresponding to the linear independence is exactly the same as before.\nCase 2 also goes exactly the same as before, but it must be shown that for any Îµ\\varepsilon the solution EquationÂ 8 is optimal. To show this, note that the value of the solution ğ±âˆ’Îµğ²\\bm{x} - \\varepsilon\\bm{y} is\nğœâŠ¤ğ±âˆ’ÎµğœâŠ¤ğ².(10) \\bm{c}^\\top \\bm{x} - \\varepsilon \\bm{c}^\\top \\bm{y}.  \\qquad(10)\nFor Îµ\\varepsilon sufficiently small in magnitude, ğ±âˆ’Îµğ²\\bm{x} - \\varepsilon \\bm{y} is a feasible solution for positive or negative values of Îµ\\varepsilon. Thus, we conclude that ğœâŠ¤ğ²=ğŸ\\bm{c}^\\top \\bm{y} = \\bm{0}. For, if ğœâŠ¤ğ²â‰ ğŸ\\bm{c}^\\top \\bm{y} \\neq \\bm{0}, an Îµ\\varepsilon of small magnitude and proper sign could be determined so as to render EquationÂ 10 smaller than ğœâŠ¤ğ²\\bm{c}^\\top \\bm{y} while maintaining feasibility."
  },
  {
    "objectID": "02_basic_lp.html#the-fundamental-theorem-of-lp-3",
    "href": "02_basic_lp.html#the-fundamental-theorem-of-lp-3",
    "title": "02_basic_lp",
    "section": "The Fundamental Theorem of LP",
    "text": "The Fundamental Theorem of LP\n\nPart (1) of the theorem is commonly referred to as CarathÃ©odoryâ€™s theorem.\nPart (2) of the theorem reduces the task of solving a linear program to that of searching over basic feasible solutions.\nSince for a problem having nn variables and mm constraints there are at most\n\n(nm)=n!m!(nâˆ’m)!\n\\binom{n}{m} = \\frac{n!}{m!(n-m)!}\n\nbasic solutions (corresponding to the number of ways of selecting mm of nn columns), there are only a finite number of possibilities.\n\nThus the fundamental theorem yields an obvious, but terribly inefficient, finite search technique.\nBy expanding upon the technique of proof as well as the statement of the fundamental theorem, the efficient simplex procedure is derived."
  },
  {
    "objectID": "02_basic_lp.html#extreme-points",
    "href": "02_basic_lp.html#extreme-points",
    "title": "02_basic_lp",
    "section": "Extreme Points",
    "text": "Extreme Points\n\n\n\nDefinition\n\n\nA point ğ±\\bm{x} in a convex set CC is said to be an extreme point of CC if there are no two distinct points ğ±1\\bm{x}_1 and ğ±2\\bm{x}_2 in CC such that ğ±=Î±ğ±1+(1âˆ’Î±)ğ±2\\bm{x} = \\alpha \\bm{x}_1 + (1-\\alpha)\\bm{x}_2 for some Î±\\alpha, 0&lt;Î±&lt;10 &lt; \\alpha &lt; 1.\n\n\n\n\nAn extreme point is thus a point that does not lie strictly within a line segment connecting two other points of the set.\n\n\n\n\nTheorem (Equivalence of Extreme Points and Basic Solutions).\n\n\nLet ğ€\\bm{A} be an mÃ—nm \\times n matrix of rank mm and ğ›\\bm{b} an mm-vector. Let KK be the convex polytope consisting of all nn-vectors ğ±\\bm{x} satisfying\nğ€ğ±=ğ›,ğ±â‰¥ğŸ.(11) \\bm{Ax} = \\bm{b}, \\;\\; \\bm{x} \\geq \\bm{0}.  \\qquad(11)\nA vector ğ±\\bm{x} is an extreme point of KK if and only if ğ±\\bm{x} is a basic feasible solution to EquationÂ 11.\n\n\n\n\n\n\nProof\n\n\nSuppose first that ğ±=(x1,x2,â€¦,xm,0,0,â€¦,0)\\bm{x} = (x_1, x_2, \\ldots, x_m, 0, 0, \\ldots, 0) is a basic feasible solution to EquationÂ 11. Then\nx1ğš1+x2ğš2+â‹¯+xmğšm=ğ›, x_1 \\bm{a}_1 + x_2\\bm{a}_2 + \\cdots + x_m\\bm{a}_m = \\bm{b}, \nwhere ğš1,ğš2,â€¦,ğšm\\bm{a}_1, \\bm{a}_2, \\ldots, \\bm{a}_m, the first mm columns of ğ€\\bm{A} are linearly independent. Suppose that ğ±\\bm{x} could be expressed as a convex combination of two other points in K; say, ğ±=Î±ğ²+(1âˆ’Î±)ğ³\\bm{x} = \\alpha \\bm{y} + (1-\\alpha)\\bm{z}, 0&lt;Î±&lt;10 &lt; \\alpha &lt; 1, ğ²â‰ ğ³\\bm{y} \\neq \\bm{z}."
  },
  {
    "objectID": "02_basic_lp.html#extreme-points-1",
    "href": "02_basic_lp.html#extreme-points-1",
    "title": "02_basic_lp",
    "section": "Extreme Points",
    "text": "Extreme Points\n\n\n\nProof\n\n\nSince all components of ğ±\\bm{x}, ğ²\\bm{y}, ğ³\\bm{z} are nonnegative and since 0&lt;Î±&lt;10 &lt; \\alpha &lt; 1, it follows immediately that the last nâˆ’mn-m components of ğ²\\bm{y} and ğ³\\bm{z} are zero. Thus, in particular, we have\ny1ğš1+y2ğš2+â‹¯+ymğšm=ğ› y_1\\bm{a}_1 + y_2\\bm{a}_2 + \\cdots + y_m\\bm{a}_m = \\bm{b} \nand\nz1ğš1+z2ğš2+â‹¯+zmğšm=ğ›. z_1\\bm{a}_1 + z_2\\bm{a}_2 + \\cdots + z_m\\bm{a}_m = \\bm{b}. \nSince the vectors ğš1,ğš2,â€¦,ğšm\\bm{a}_1, \\bm{a}_2, \\ldots, \\bm{a}_m are linearly independent, however, it follows that ğ±=ğ²=ğ³\\bm{x} = \\bm{y} = \\bm{z} and hence xx is an extreme point of KK.\nConversely, assume that ğ±\\bm{x} is an extreme point of KK. Let us assume that the nonzero components of ğ±\\bm{x} are the first kk components. Then\nx1ğš1+x2ğš2+â‹¯+xkğšk=ğ›,xi&gt;0,i=1,2,â€¦,k. x_1\\bm{a}_1 + x_2\\bm{a}_2 + \\cdots + x_k\\bm{a}_k = \\bm{b}, \\quad x_i &gt; 0,\n\\quad i = 1, 2, \\ldots, k. \nTo show that ğ±\\bm{x} is a bsic feasible solution (BFS) it must be shown that the vectors ğš1,ğš2,â€¦,ğšk\\bm{a}_1, \\bm{a}_2, \\ldots, \\bm{a}_k are linearly independent. We do this by contradiction. Suppose they are linearly dependent. Then there is a nontrivial linear combination that is zero:\ny1ğš1+y2ğš2+â‹¯ykğšk=ğŸ. y_1\\bm{a}_1 + y_2\\bm{a}_2 + \\cdots y_k\\bm{a}_k = \\bm{0}. \nDefine the nn-vector ğ²=(y1,y2,â€¦,yk,0,0,â€¦,0)\\bm{y} = (y_1, y_2, \\ldots, y_k, 0, 0, \\ldots, 0). Since xi&gt;0x_i &gt; 0, 1â‰¤iâ‰¤k1 \\leq i \\leq k, it is possible to select Îµ\\varepsilon such that\nğ±+Îµğ²â‰¥ğŸ,ğ±âˆ’Îµğ²â‰¥ğŸ. \\bm{x} + \\varepsilon \\bm{y} \\geq \\bm{0}, \\quad \\bm{x} - \\varepsilon \\bm{y}\n\\geq \\bm{0}. \nWe then have ğ±=12(ğ±+Îµğ²)+12(ğ±âˆ’Îµğ²)\\bm{x} = \\frac{1}{2}(\\bm{x}+\\varepsilon\\bm{y}) + \\frac{1}{2}(\\bm{x}-\\varepsilon\\bm{y}) which expresses ğ±\\bm{x} as a convex combination of two distinct vectors in KK."
  },
  {
    "objectID": "02_basic_lp.html#extreme-points-2",
    "href": "02_basic_lp.html#extreme-points-2",
    "title": "02_basic_lp",
    "section": "Extreme Points",
    "text": "Extreme Points\n\n\n\nCorollary\n\n\nIf the convex set KK corresponding to EquationÂ 11 is nonempty, it has at least one extreme point.\n\n\n\n\n\n\nCorollary\n\n\nIf there is a finite optimal solution to a linear programming problem, there is a finite optimal solution which is an extreme point of the constraint set.\n\n\n\n\n\n\nCorollary\n\n\nThe constraint set KK corresponding to EquationÂ 11 possesses at most a finite number of extreme points and each of them is finite.\n\n\n\n\n\n\nCorollary\n\n\nIf the convex polytope KK corresponding to EquationÂ 11 is bounded, then KK is a convex polyhedron, that is, KK consists of points that are convex combinations of a finite number of points."
  },
  {
    "objectID": "02_basic_lp.html#convex-geometry-examples",
    "href": "02_basic_lp.html#convex-geometry-examples",
    "title": "02_basic_lp",
    "section": "Convex Geometry â€“ Examples",
    "text": "Convex Geometry â€“ Examples\n\n\n\n\n\n\nConsider the constraint set in â„3\\mathbb{R}^3 defined by\nx1+x2+x3=1,x1,x2,x3â‰¥0.\n\\begin{align}\nx_1 + x_2 + x_3 &= 1, \\\\\nx_1, x_2, x_3 &\\geq 0.\n\\end{align}\n\n\nThis set is illustrated in the figure.\nIt has three extreme points, corresponding to the three basic solutions to x1+x2+x3=1x_1 + x_2 + x_3 = 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConsider the constraint set in â„3\\mathbb{R}^3 defined by\nx1+x2+x3=1,2x1+3x2=1,x1,x2,x3â‰¥0.\n\\begin{align}\nx_1 + x_2 + x_3 &= 1, \\\\\n2x_1 + 3x_2 &= 1, \\\\\nx_1, x_2, x_3 &\\geq 0.\n\\end{align}\n\n\nThis set is illustrated in the figure.\nIt has two extreme points, corresponding to the two basic feasible solutions.\nNote that the system of equations itself has three basic solutions: (2,âˆ’1,0),(12,0,12),(0,13,23), (2, -1, 0), (\\frac{1}{2}, 0, \\frac{1}{2}), (0, \\frac{1}{3}, \\frac{2}{3}),  the first of which is not feasible."
  },
  {
    "objectID": "02_basic_lp.html#convex-geometry-examples-1",
    "href": "02_basic_lp.html#convex-geometry-examples-1",
    "title": "02_basic_lp",
    "section": "Convex Geometry â€“ Examples",
    "text": "Convex Geometry â€“ Examples\n\n\n\n\n\n\nConsider the constraint set in â„2\\mathbb{R}^2 defined by\nx1+83x2â‰¤4,x1+x2â‰¤2,2x1â‰¤3,x1,x2â‰¥0.\n\\begin{align}\nx_1 + \\frac{8}{3}x_2 &\\leq 4, \\\\\nx_1 + x_2 &\\leq 2, \\\\\n2x_1 &\\leq 3, \\\\\nx_1, x_2 &\\geq 0.\n\\end{align}\n\n\nThe set has five extreme points.\nIn order to compare this example, we must introduce slack variables to yield the equivalent set in â„5\\mathbb{R}^5.\n\nx1+83x2+x3=4,x1+x2+x4=2,2x1+x5=3x1,x2,x3,x4,x5â‰¥0.\n\\begin{align}\nx_1 + \\frac{8}{3}x_2 + x_3 &= 4, \\\\\nx_1 + x_2 + x_4 &= 2, \\\\\n2_x1 + x_5 &= 3 \\\\\nx_1, x_2, x_3, x_4, x_5 \\geq 0.\n\\end{align}\n\n\n\n\n\n\n\n\nA basic solution for this system is obtained by setting any two variables to zero and solving for the remaining three.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA basic solution for this system is obtained by setting any two variables to zero and solving for the remaining three.\nAs indicated in the figure, each edge of the figure corresponds to one variable being zero, and the extreme points are the points where two variables are zero.\n\n\n\n\n\n\n\n\nEven when not expressed in the standard form, the extreme points of the set defined by the constraints of a lienar program correspond to the possible solution points.\nThe level sets of an objective function âˆ’2x1âˆ’x2-2x_1 - x_2 is included in the bottom figure.\n\nAs the level varies, different parallel lines are obtained.\nThe optimal value of the linear program is the smallest value of this level for which the corresponding line has a point in common with the feasible set.\nIn the figure, this occurs at the point (32,12)(\\frac{3}{2}, \\frac{1}{2}) with the level z=âˆ’72z = -\\frac{7}{2}."
  },
  {
    "objectID": "02_basic_lp.html#infeasibility-certificates",
    "href": "02_basic_lp.html#infeasibility-certificates",
    "title": "02_basic_lp",
    "section": "(In)feasibility Certificates",
    "text": "(In)feasibility Certificates\n\n\n\nTheorem (Farkasâ€™s Lemma).\n\n\nLet ğ€\\bm{A} be an mÃ—nm \\times n matrix and ğ›\\bm{b} be an mm-vector. The system of constraints ğ€ğ±=ğ›,ğ±â‰¥ğŸ(12) \\bm{Ax} = \\bm{b}, \\quad \\bm{x} \\geq \\bm{0}  \\qquad(12) has a feasible solution ğ±\\bm{x} if and only if the system of constraints âˆ’ğ²âŠ¤ğ€â‰¥ğŸ,ğ²âŠ¤ğ›=1(or&gt;0)(13) -\\bm{y}^\\top \\bm{A} \\geq \\bm{0}, \\quad \\bm{y}^\\top \\bm{b} = 1 (\\text{or} &gt; 0)\n \\qquad(13) has no feasible solution ğ²\\bm{y}. Therefore a single feasible solution ğ²\\bm{y} for system EquationÂ 13 establishes an infeasibility certificate for the system EquationÂ 12.\n\n\n\n\nThe two systems, EquationÂ 12 and EquationÂ 13, are called alternative systems: one of them is feasible and the other is infeasible.\n\n\n\n\nExample 1\n\n\nSuppose ğ€=[11],ğ›=âˆ’1\\bm{A} = \\begin{bmatrix} 1 & 1 \\end{bmatrix}, \\quad \\bm{b} = -1. Then, y=âˆ’1y = -1 is feasible for system EquationÂ 13, which proves that the system EquationÂ 12 is infeasible."
  },
  {
    "objectID": "02_basic_lp.html#infeasibility-certificates-1",
    "href": "02_basic_lp.html#infeasibility-certificates-1",
    "title": "02_basic_lp",
    "section": "(In)feasibility Certificates",
    "text": "(In)feasibility Certificates\n\n\n\nLemma\n\n\nLet CC be the cone generated by the columns of matrix ğ€\\bm{A}, that is C={ğ€ğ±âˆˆâ„m:ğ±â‰¥ğŸ}. C = \\{\\bm{Ax} \\in \\mathbb{R}^m: \\bm{x} \\geq \\bm{0}\\}.  Then C is a closed and convex set.\n\n\n\n\n\n\nProof (of Farkasâ€™s Lemma).\n\n\nLet the system EquationÂ 12 have a feasible solution, say ğ±â€¾\\bar{\\bm{x}}. Then, the system EquationÂ 13 must be infeasible, since, otherwise, we have a contradiction\n0&lt;ğ²âŠ¤ğ›=ğ²âŠ¤(ğ€ğ±â€¾)=(ğ²âŠ¤ğ€)ğ±â€¾â‰¤0, 0 &lt; \\bm{y}^\\top \\bm{b} = \\bm{y}^\\top(\\bm{A}\\bar{\\bm{x}}) = (\\bm{y}^\\top\n\\bm{A})\\bar{\\bm{x}} \\leq 0, \nfrom ğ±â€¾â‰¥ğŸ\\bar{\\bm{x}} \\geq \\bm{0} and ğ²âŠ¤ğ€â‰¤ğŸ\\bm{y}^\\top \\bm{A} \\leq \\bm{0}.\nNow, let the system EquationÂ 12 have no feasible solution, that is, ğ›âˆ‰C:={ğ€ğ±:ğ±â‰¥0}\\bm{b} \\notin C := \\{\\bm{Ax}: \\bm{x} \\geq 0\\}. We now prove that its alternative system EquationÂ 13 must have a feasible solution.\nSince points ğ›\\bm{b} is not in CC and CC is a closed convex set, by the separating hyperplane theorem, there is a ğ²\\bm{y} such that ğ²âŠ¤ğ›&gt;supğœâˆˆCğ²âŠ¤ğœ. \\bm{y}^\\top\n\\bm{b} &gt; \\operatorname{sup}_{\\bm{c} \\in C} \\bm{y}^\\top \\bm{c}.  But we know that ğœ=ğ€ğ±\\bm{c} = \\bm{Ax} for some ğ±â‰¥ğŸ\\bm{x} \\geq \\bm{0}, so we have ğ²âŠ¤ğ›&gt;supğ±â‰¥ğŸğ²âŠ¤ğ€ğ±=supğ±â‰¥ğŸ(ğ²âŠ¤ğ€)ğ±.(14) \\bm{y}^\\top \\bm{b} &gt; \\operatorname{sup}_{\\bm{x}\\geq\\bm{0}} \\bm{y}^\\top\n\\bm{Ax} = \\operatorname{sup}_{\\bm{x} \\geq \\bm{0}} (\\bm{y}^\\top \\bm{A})\\bm{x}.  \\qquad(14) Setting ğ±=ğŸ\\bm{x} = \\bm{0}, we have ğ²âŠ¤ğ›&gt;0\\bm{y}^\\top \\bm{b} &gt; 0 from inequality EquationÂ 14."
  },
  {
    "objectID": "02_basic_lp.html#infeasibility-certificates-2",
    "href": "02_basic_lp.html#infeasibility-certificates-2",
    "title": "02_basic_lp",
    "section": "(In)feasibility Certificates",
    "text": "(In)feasibility Certificates\n\n\n\nProof (of Farkasâ€™s Lemma) - Continued -\n\n\nFurthermore, inequality EquationÂ 14 also implies ğ²âŠ¤ğ€â‰¤ğŸ\\bm{y}^\\top \\bm{A} \\leq \\bm{0}. Since otherwise, say the first entry of ğ²âŠ¤ğ€\\bm{y}^\\top \\bm{A}, (ğ²âŠ¤ğ€)1(\\bm{y}^\\top \\bm{A})_1, is positive. We can then choose a vector ğ±â€¾â‰¥ğŸ\\bar{\\bm{x}} \\geq \\bm{0} such that\nxâ€¾1=Î±&gt;0,xâ€¾2=â‹¯=xâ€¾n=0. \\bar{x}_1 = \\alpha &gt; 0, \\bar{x}_2 = \\cdots = \\bar{x}_n = 0. \nThen, from this choice, we have\nsupğ±â‰¥ğŸ(ğ²âŠ¤ğ€)ğ±â‰¥(ğ²âŠ¤ğ€)ğ±â€¾=Î±(ğ²âŠ¤ğ€)1. \\operatorname{sup}_{\\bm{x} \\geq \\bm{0}} (\\bm{y}^\\top\\bm{A})\\bm{x} \\geq\n(\\bm{y}^\\top \\bm{A})\\bar{\\bm{x}} = \\alpha(\\bm{y}^\\top \\bm{A})_1. \nThis tends to âˆ\\infty as Î±â†’âˆ\\alpha \\rightarrow \\infty. This is a contradiction because (ğ²âŠ¤ğ€)ğ±â€¾(\\bm{y}^\\top\\bm{A})\\bar{\\bm{x}} should be bounded from above by inequality EquationÂ 14. Therefore, ğ²\\bm{y} identified in the separating hyperplane theorem is a feasible solution to system EquationÂ 13. Finally, we can always scale ğ²\\bm{y} such that ğ²âŠ¤ğ›=1\\bm{y}^\\top \\bm{b} = 1.\n\n\n\n\n\n\nGeometric Interpretation\n\n\nIf ğ›\\bm{b} is not in the closed and convex cone generated by the columns of the matrix ğ€\\bm{A}, then there must be a hyperplane separating ğ›\\bm{b} and the cone, and the feasible solution ğ²\\bm{y} to the alternative system is the slope-vector of the hyperplane."
  },
  {
    "objectID": "02_basic_lp.html#variant-of-farkass-lemma",
    "href": "02_basic_lp.html#variant-of-farkass-lemma",
    "title": "02_basic_lp",
    "section": "Variant of Farkasâ€™s Lemma",
    "text": "Variant of Farkasâ€™s Lemma\n\n\n\nCorollary\n\n\nLet ğ€\\bm{A} be an mÃ—nm \\times n matrix and ğœ\\bm{c} an nn-vector. The system of constraints\nğ€âŠ¤ğ²â‰¤c(15) \\bm{A}^\\top \\bm{y} \\leq c  \\qquad(15)\nhas a feasible solution ğ²\\bm{y} if and only if the system of constraints\nğ€ğ±=ğŸ,ğ±â‰¥ğŸ,ğœâŠ¤ğ±=âˆ’1(or&lt;0)(16) \\bm{Ax} = \\bm{0}, \\quad \\bm{x} \\geq \\bm{0}, \\quad \\bm{c}^\\top \\bm{x} = -1 \\;\n(\\text{or} &lt; 0)  \\qquad(16)\nhas no feasible solution ğ±\\bm{x}. Therefore a single feasible solution ğ±\\bm{x} for system EquationÂ 16 establishes an infeasibility certificate for the system EquationÂ 15.\n\n\n\n\n\nOptimization Theory and Practice â€¢ Aykut C. Satici"
  }
]