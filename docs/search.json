[
  {
    "objectID": "05_mdp_lp.html#optimization-theory-and-practice",
    "href": "05_mdp_lp.html#optimization-theory-and-practice",
    "title": "05_mdp_lp",
    "section": "Optimization Theory and Practice",
    "text": "Optimization Theory and Practice\n\n\nMarkov Decision Processes and Linear Programming\n\n\n\n\nInstructor: Aykut Satici, Ph.D.   Mechanical and Biomedical Engineering  Electrical and Computer Engineering  Boise State University, Boise, ID, USA\n\n\nTopics:  Markov Decision Processes  Policies and Value Functions  Bellman Optimality and Linear Programming"
  },
  {
    "objectID": "05_mdp_lp.html#introduction",
    "href": "05_mdp_lp.html#introduction",
    "title": "05_mdp_lp",
    "section": "Introduction",
    "text": "Introduction\n\nMDPs are a classical formalization of sequential decision making.\n\nActions influence not just immediate rewards, but also subsequent states, and through those, future rewards.\nThey are meant to be a straightforward framing of the problem of learning from interaction to achieve a goal.\n\nMDPs involve delayed reward and the need to trade off immediate and delayed reward.\nWe will estimate the value q*(s,a)q_\\ast(s, a) of each action aa in each state ss,\n\nor we estimate the value v*(s)v_\\ast(s) of each state given optimal action selections.\n\nThese state-dependent quantities are essential to accurately assigning credit for long-term consequences to individual action selections."
  },
  {
    "objectID": "05_mdp_lp.html#the-agent-environment-interface",
    "href": "05_mdp_lp.html#the-agent-environment-interface",
    "title": "05_mdp_lp",
    "section": "The Agent – Environment Interface",
    "text": "The Agent – Environment Interface\n\n\n\n\n\n\nThe decision maker is called the agent or controller.\nThe thing it interacts with, everything outside the agent, is called the environment or plant.\nThese interact continually, the agent selecting actions (or control signal) and the environment responding to these control signals and presenting new situations to the agent.\nThe environment also gives rise to rewards, special numerical values that the agent seeks to maximize over time through its choice of actions.\n\n\n\n\n\n\n\n\nThe agent and environment interact at each of a sequence of discrete time steps, t=0,1,2,…t = 0, 1, 2, \\ldots.\n\nAt each time step tt, the agent receives some respresentation of the environment’s state, St∈𝒮S_t \\in \\mathcal{S}, and on that basis selects an action At∈𝒜(s)A_t \\in \\mathcal{A}(s).\nOne time step later, in part as a consequence of its actions, the agent receives a numerical reward, Rt+1∈ℛ∈ℝR_{t+1} \\in \\mathcal{R} \\in \\mathbb{R} and finds itself in a new state, St+1S_{t+1}.\n\nThe MDP and agent together give rise to a trajectory that begins like this: S0,A0,S1,R1,A2,R2,S2,A2,R3,…(1) S_0, A_0, S_1, R_1, A_2, R_2, S_2, A_2, R_3, \\ldots  \\qquad(1)\n\n\n\n\n\n\n\n\n\n\n\n\nIn a finite MDP, the sets of states, actions, and rewards\n(𝒮,𝒜,ℛ)(\\mathcal{S}, \\mathcal{A}, \\mathcal{R}) all have a finite number of elements.\n\nIn this case, the random variables RtR_t and StS_t have well defined discrete probability distributions dependent on the preceding state and action.\n\n\np(s′,r∣s,a)≜ℙ{St=s′,Rt=r∣St−1=s,At−1=a}(2) p(s', r \\mid s, a) \\triangleq \\mathbb{P}\\{S_t = s', R_t = r \\mid S_{t-1} = s,\nA_{t-1} = a\\}  \\qquad(2)\n\nThis function pp defines the dynamics of the MDP.\n\nIt specifies a probability distribution for each choice of ss and aa, i.e.,\n\n\n∑s′∈𝒮∑r∈ℛp(s′,r∣s,a)=1,∀s∈𝒮,a∈𝒜(s).(3) \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r \\mid s, a) = 1,\n\\quad \\forall s \\in \\mathcal{S}, \\; a \\in \\mathcal{A}(s).  \\qquad(3)\n\nThis is called the Markov property."
  },
  {
    "objectID": "05_mdp_lp.html#the-agent-environment-interface-1",
    "href": "05_mdp_lp.html#the-agent-environment-interface-1",
    "title": "05_mdp_lp",
    "section": "The Agent – Environment Interface",
    "text": "The Agent – Environment Interface\n\nFrom the four-argument dynamics function, pp, one can compute anything else one might want to know about the environment\n\nthe state-transition probabilities p(s′∣s,a)≜ℙ{St=s′∣St−1=a,At−1=a}=∑r∈ℛp(s′,r∣s,a).(4) p(s' \\mid s, a) \\triangleq \\mathbb{P}\\{S_t = s' \\mid S_{t-1} = a, A_{t-1} =\na\\} = \\sum_{r \\in \\mathcal{R}} p(s', r \\mid s, a).  \\qquad(4)\nthe expected rewards for state-action pairs as a two-argument function r(s,a)≜𝔼[Rt∣St−1=s,At−1=a]=∑r∈ℛr∑s′∈𝒮p(s′,r∣s,a),(5) r(s, a) \\triangleq \\mathbb{E}[R_t \\mid S_{t-1} = s, A_{t-1} = a] = \\sum_{r\n\\in \\mathcal{R}} r \\sum_{s' \\in \\mathcal{S}} p(s', r \\mid s, a),  \\qquad(5)\nthe expected rewards for state-action-next-state triples as a three argument function r(s,a,s′)≜𝔼[Rt∣St−1=s,At−1=a,St=s′]=∑r∈ℛrp(s′,r∣s,a)p(s′∣s,a).(6) r(s, a, s') \\triangleq \\mathbb{E}[R_t \\mid S_{t-1} = s, A_{t-1} = a, S_t =\ns'] = \\sum_{r \\in \\mathcal{R}} r \\; \\frac{p(s', r \\mid s, a)}{p(s' \\mid s, a)}.\n \\qquad(6)\n\n\n\n\nThe time steps can refer to arbitrary successive stages of decision making.\nThe actions can be low-level controls (e.g. voltages), or high-level decisions (e.g. have lunch, go to grad school).\nThey can be determined by low-level sensing (e.g. sensor readings) or they can be more high-level and abstract (e.g. symbolic descriptions of objects in a room).\nStates can be anything we can know that might be useful in making the decisions."
  },
  {
    "objectID": "05_mdp_lp.html#the-agent-environment-interface-2",
    "href": "05_mdp_lp.html#the-agent-environment-interface-2",
    "title": "05_mdp_lp",
    "section": "The Agent – Environment Interface",
    "text": "The Agent – Environment Interface\n\n\n\nRule of thumb\n\n\n\nAnything that cannot be changed arbitrarily by the agent is considered to be outside of it and thus part of its environment.\n\n\n\n\n\nNot everything in the environment is unknown to the agent.\n\nFor example, the agent often knows how its rewards are computed as a function of its actions and the states.\nReward computation is external to the agent because it defines the task facing the agent and thus is beyond its ability to change arbitrarily.\n\nIn fact, some agents know everything about how its environment.\n\n\n\n\nMDP framework\n\n\nWhatever the details of the sensory, memory, and control apparatus, and whatever objective one is trying to achieve, any problem of learning goal-directed behavior can be reduced to three signals passing back and forth between an agent and its environment:\n\none signal to represent the choices made by the agent (the actions),\none signal to represent the basis on which the choices are made (the states),\none signal to define the agent’s goal (the rewards)."
  },
  {
    "objectID": "05_mdp_lp.html#examples",
    "href": "05_mdp_lp.html#examples",
    "title": "05_mdp_lp",
    "section": "Examples",
    "text": "Examples\n\n\n\nExample 1 – Bioreactor\n\n\nSuppose we want to determine moment-by-moment temperatures and stirring rates for a bioreactor.\n\nThe actions might be target temperatures and target stirring rates that are passed to the lower-level control system,\n\nThe control system, in turn, will activate heating elements and motors to attain the targets.\n\nThe states could be the outputs of a thermocouple and other sensory readings\nThe rewards might be moment-by-moment measures of the rate at which the useful chemical is being produced by the bioreactor.\n\n\n\n\n\n\n\nExample 2 – Pick-and-Place Robot\n\n\nSuppose we want to control the motion of a robot arm in a repetitive pick-and-place task.\n\nThe actions might be the voltages applied to each motor at each joint.\nThe states might be the latest readings of joint angles and velocities.\nThe reward might be +1+1 for each object successfully picked up and placed.\n\nTo encourage smooth movements, at each time step, a small, negative reward could be given as a function of the moment-to-moment jerkiness of the motion.\n\n\n\n\n\n\n\n\nHomework\n\n\nDevise three example tasks of your own that fit into the MDP framework, identifying for each states, actions, and rewards."
  },
  {
    "objectID": "05_mdp_lp.html#example-recycling-robot",
    "href": "05_mdp_lp.html#example-recycling-robot",
    "title": "05_mdp_lp",
    "section": "Example — Recycling Robot",
    "text": "Example — Recycling Robot\n\n\n\n\n\nA mobile robot has the job of collecting empty soda cans in an office environment.\nIt has sensors for detecting cans, and an arm and gripper that can pick them up and place them in an onboard bin; it runs on a rechargable battery.\nThe robot’s control system has components for\n\ninterpreting sensory information,\nfor navigating,\nand for controlling the arm and gripper.\n\nHigh-level decisions about how to search for cans are to be made by the agent on the current charge level of the battery.\n\n𝒮={𝚑𝚒𝚐𝚑,𝚕𝚘𝚠}\\mathcal{S} = \\{\\texttt{high}, \\texttt{low}\\}.\n𝒜(𝚑𝚒𝚐𝚑)={𝚜𝚎𝚊𝚛𝚌𝚑,𝚠𝚊𝚒𝚝}\\mathcal{A}(\\texttt{high}) = \\{\\texttt{search}, \\texttt{wait}\\}, 𝒜(𝚕𝚘𝚠)={𝚜𝚎𝚊𝚛𝚌𝚑,𝚠𝚊𝚒𝚝,𝚛𝚎𝚌𝚑𝚊𝚛𝚐𝚎}\\mathcal{A}(\\texttt{low}) = \\{\\texttt{search}, \\texttt{wait}, \\texttt{recharge}\\}.\n\n\n\n\n\nThe rewards are zero most of the time, but become positive when the robot secures an empty can, or large and negative if the battery runs all the down.\nBest way to find cans is to actively search for them, but this runs down the robot’s battery.\n\nIf the battery is depleted, the robot must be rescued (producing a low reward).\n\nIf the energy is 𝚑𝚒𝚐𝚑\\texttt{high} then a period active search can always be completed w/o risk of depleting the battery.\nA period of searching that begins with a 𝚑𝚒𝚐𝚑\\texttt{high} energy level leaves the energy level 𝚑𝚒𝚐𝚑\\texttt{high} with probability α\\alpha and reduces it to 𝚕𝚘𝚠\\texttt{low} with probability 1−α1-\\alpha.\nA period of searching undertaken when the energy level is 𝚕𝚘𝚠\\texttt{low} leaves it 𝚕𝚘𝚠\\texttt{low} with probability β\\beta and depletes the battery with probability 1−β1-\\beta.\n\nRobot must be rescued and the battery is then recharged back to 𝚑𝚒𝚐𝚑\\texttt{high}.\n\n\n\n\n\nEach can collected by the robot counts as a unit reward, whereas a reward of −3-3 results whenever the robot has to be rescued.\nLet rsearchr_{\\text{search}} and rwaitr_{\\text{wait}} with rsearch>rwaitr_{\\text{search}} > r_{\\text{wait}} denote the expected number of cans the robot will collect.\nFinally, suppose that no cans can be collected during a run home for recharging, and that no cans can be collected on a step in which the battery is depleted.\n\n\n\n\nExercise\n\n\nGive a table analogous to the one on the right but for p(s′,r|s,a)p(s', r | s, a). It should have columns for s,a,s′,rs, a, s', r and p(s′,r|s,a)p(s', r|s,a) and a row every 44-tuple for which p(s′,r|s,a)>0p(s', r| s, a) > 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTransition graph\n\n\nstates: open circle | actions: solid circle"
  },
  {
    "objectID": "05_mdp_lp.html#goals-and-rewards",
    "href": "05_mdp_lp.html#goals-and-rewards",
    "title": "05_mdp_lp",
    "section": "Goals and Rewards",
    "text": "Goals and Rewards\n\nThe agent’s goal is to maximize the total amount of reward it receives; not the immediate reward, but the cumulative reward in the long run.\n\n\n\n\nReward Hypothesis\n\n\nAll of what we mean by goal and purposes can be well thought as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).\n\n\n\n\nFormulating goals in terms of reward signals has proved to be flexible and widely applicable.\n\n\n\n\nExamples\n\n\n\n\n\n\n\n\n\nTask\nReward\n\n\n\n\nRobot walking\n++ve reward on each time step proportional to robot’s forward motion\n\n\nEscape from a maze\n−1-1 reward for every time step that passes prior to escape\n\n\nPlaying chess\n+1+1 for winning, −1-1 for losing, 00 for drawing\n\n\n\n\n\n\n\nThe reward signal is no the place to impart to the agent prior knowledge about how to achieve what we want it to do (better places: initial policy, iniial value function).\n\nOtherwise the agent might find a way to achieve subgoals without achieving the real goal!"
  },
  {
    "objectID": "05_mdp_lp.html#returns-and-episodes",
    "href": "05_mdp_lp.html#returns-and-episodes",
    "title": "05_mdp_lp",
    "section": "Returns and Episodes",
    "text": "Returns and Episodes\n\nIn general, we seek to maximize the expected return, where the return, denoted GtG_t, is defined as some specific function of the reward sequence.\n\nSimplest (bit naïve) case: Gt≜Rt+1+Rt+2+⋯+RT=∑k=t+1TRk,(7) G_t \\triangleq R_{t+1} + R_{t+2} + \\cdots + R_T = \\sum_{k=t+1}^T R_k,   \\qquad(7)\nDiscounted return: Gt≜∑k=0∞γkRt+k+1=∑k=t+1Tγk−t−1Rk,0≤γ≤1.(8) G_t \\triangleq \\sum_{k=0}^\\infty \\gamma^kR_{t+k+1} =\n\\sum_{k=t+1}^T \\gamma^{k-t-1}R_k, \\quad 0 \\leq \\gamma \\leq 1.  \\qquad(8)\n\nThe naïve approach makes sense in applications in which there is a natural notion of a final time step (episodes).\n\nEach episodes ends in a state called the terminal state.\nIn episodic tasks we sometimes need to distinguish the set of all nonterminal states, 𝒮\\mathcal{S} from the set of all states plus the terminal state, 𝒮+\\mathcal{S}^+.\n\nIf the task has no final time, or terminal state, then T=∞T=\\infty.\n\nThe naïve return may easily become infinite. Hence we use the discounted return. Gt=Rt+1+γ(Rt+2+γRt+3+γ2Rt+4+⋯)=Rt+1+γGt+1.(9) G_t = R_{t+1} + \\gamma(R_{t+2} + \\gamma R_{t+3} + \\gamma^2 R_{t+4} + \\cdots)\n= R_{t+1} + \\gamma G_{t+1}.  \\qquad(9)"
  },
  {
    "objectID": "05_mdp_lp.html#example-pole-balancing",
    "href": "05_mdp_lp.html#example-pole-balancing",
    "title": "05_mdp_lp",
    "section": "Example — Pole-Balancing",
    "text": "Example — Pole-Balancing\n\n\n\nA failure is said to occur if the pole falls past a given angle from vertical or if the cart runs off the track.\n\nThe pole is reset to vertical after each failure.\n\nThis task could be treated as episodic, where the natural episodes are the repeated atempts to balance the pole.\n\nThe reward in this case would be +1+1 for every time step on which failure did not occur.\nThe return at each time would be the number of steps until failure.\nSuccessful balancing forever would mean a return of infinity.\n\n\n\n\n\n\n\n\n\nObjective: Apply forces to a cart moving along a track so as to keep a pole hinged to the cart from falling over.\n\n\n\n\n\n\nAlternatively, we could treat pole-balancing as a continuing task, using discounting.\n\nIn his case the reward would be −1-1 on each failur and zero all other times.\nThe return at each time would the be related to −γK−1-\\gamma^{K-1}, where KK is the number of time steps before failure.\n\nIn either case, the return is maximized by keeping the pole balanced for as long as possible."
  },
  {
    "objectID": "05_mdp_lp.html#how-good-are-a-state-and-an-action",
    "href": "05_mdp_lp.html#how-good-are-a-state-and-an-action",
    "title": "05_mdp_lp",
    "section": "How good are a state and an action?",
    "text": "How good are a state and an action?\n\nThe rewards the agent can expect to receive in the future depend on what actions it will take.\nValue functions are defined with respect to paricular ways of acting, called policies.\n\n\n\n\n\n\nDefinition\n\n\nA policy is a mapping from states o probabilities of selecting each possible action.\n\nIf the agent is following policy π\\pi at time tt, then π(a|s)\\pi(a|s) is the probability that At=aA_t = a given that St=sS_t = s.\nπ(a|s)\\pi(a|s) defines a probability distribution over a∈𝒜(s)a \\in \\mathcal{A}(s) for each s∈𝒮s \\in \\mathcal{S}.\n\n\n\n\n\n\n\n\nDefinition (State-Value Function for Policy π\\pi)\n\n\nThe value function of a state ss under a policy π\\pi, denoted vπ(s)v_\\pi(s), is the expected return when starting in ss and following π\\pi thereafter.\nvπ(s)≜𝔼π[Gt|St=s]=𝔼π[∑k=0∞γkRt+k+1|St=s],∀s∈𝒮.(10) \n\\begin{align}\nv_\\pi(s) &\\triangleq \\mathbb{E}_\\pi[G_t | S_t = s] \\\\ \n&= \\mathbb{E}_\\pi\\left[\\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} | S_t = s \\right],\n\\;\\; \\forall s \\in \\mathcal{S}.\n\\end{align}\n \\qquad(10)\n\nThe value of the terminal state, if any, is always zero.\n\n\n\n\n\n\n\n\n\nDefinition (Action-Value Function for Policy π\\pi)\n\n\nWe define the value of taking action aa in state ss under a policy π\\pi, denoted qπ(s,a)q_\\pi(s, a), as the expected return starting from ss, taking the action aa, and thereafter following policy π\\pi:\nqπ(s,a)≜𝔼π[Gt|St=s,At=a]=𝔼π[∑k=0∞γkRt+k+1|St=s,At=a].(11) \n\\begin{align}\nq_\\pi(s, a) &\\triangleq \\mathbb{E}_\\pi[G_t | S_t=s, A_t=a] \\\\\n&= \\mathbb{E}_\\pi\\left[\\sum_{k=0}^\\infty \\left. \\gamma^k R_{t+k+1}\\right| S_t=s, \nA_t=a \\right].\n\\end{align}\n \\qquad(11)"
  },
  {
    "objectID": "05_mdp_lp.html#bellman-equation",
    "href": "05_mdp_lp.html#bellman-equation",
    "title": "05_mdp_lp",
    "section": "Bellman Equation",
    "text": "Bellman Equation\n\n\n\nFundamental property of Value Functions\n\n\nValue functions saisfy recursive relationships similar that which we established for the return Equation 9.\nFor any policy π\\pi and any state ss, the following consistency condition holds between he value of ss and he value of its possible successor states:\nvπ(s)≜=𝔼π[Gt|St=s]=𝔼π[Rt+1+γGt+1|St=s],by (9)=∑a∈𝒜(s)π(a|s)∑s′,rp(s′,r|s,a)[r+γ𝔼π[Gt+1|St+1=s′]]=∑a∈𝒜(s)π(a|s)∑s′,rp(s′,r|s,a)[r+γvπ(s′)],∀s∈𝒮.(12)\n\\begin{align}\nv_\\pi(s) \\triangleq &= \\mathbb{E}_\\pi[G_t|S_t=s] = \\mathbb{E}_\\pi[R_{t+1} +\n\\gamma G_{t+1}|S_t = s], \\quad {\\text{by (9)} }\\\\\n&= \\sum_{a \\in \\mathcal{A}(s)} \\pi(a|s)\\sum_{s',r}p(s',r|s,a)\\left[r + \\gamma\n\\mathbb{E}_\\pi[G_{t+1}|S_{t+1}=s']\\right] \\\\\n&= \\sum_{a \\in \\mathcal{A}(s)} \\pi(a|s) \\sum_{s', r}p(s',r|s,a)\\left[r + \\gamma\nv_\\pi(s')\\right],\n\\quad \\forall s \\in \\mathcal{S}.\n\\end{align}\n \\qquad(12)\n\n\n\n\n\n\nEach open circle represents a state and each solid circle represents a state-action pair.\nStarting from state ss, the root node at the top, the agent could take any of some set of actions — three are shown — based on its policy π\\pi.\n\n\n\n\n\n\n\n\nFrom each of these, the environment could respond with one of several next states, s′s' (two are shown), along with a reward, rr, depending on its dynamics, given by the function pp.\nBellman equation averages over all the possibilites, weighing each by its probability of occuring: the value of the start state must equal the (discounted) value of the expected next state, plus the reward expected along the way."
  },
  {
    "objectID": "05_mdp_lp.html#example-gridworld",
    "href": "05_mdp_lp.html#example-gridworld",
    "title": "05_mdp_lp",
    "section": "Example — Gridworld",
    "text": "Example — Gridworld\n\n\n\n\n\nGridworld is a finite Markov Decision Process.\n\nThe cells of the grid correspond to the states.\nAt each cell, four actions are possible: 𝚗𝚘𝚛𝚝𝚑\\texttt{north}, 𝚜𝚘𝚞𝚝𝚑\\texttt{south}, 𝚎𝚊𝚜𝚝\\texttt{east}, 𝚠𝚎𝚜𝚝\\texttt{west}\nThese actions deterministically cause the agent to move one cell in the respective direction of the grid.\n\nActions that would take the agent off the grid leave its location unchanged, but also incur a reward of −1-1.\nOther actions reward 00, except those that move the agent out of the special states AA and BB.\n\nFrom state AA, all four actions yield a reward of +10+10 and take the agent to A′A'.\nFrom state BB, all actions yield a reward of +5+5 and take the agent to B′B'.\n\n\n\n\n\nExercise\n\n\nThe Bellman Equation 12 must hold for each state for the value function vπv_\\pi. Show numerically that this equation holds for the center state, valued at +0.7+0.7, w.r.t. its four neighboring states, valued at +2.3+2.3, +0.4+0.4, −0.4-0.4, and +0.7+0.7 (accurate to one decimal place).\n\n\n\n\n\n\nThe agent selects all four actions with equal probability in all states.\nFigure at the bottom shows the value function, vπv_\\pi, for this policy, for the discounted reward case with γ=910\\gamma = \\frac{9}{10}.\nNotice the negative values near the lower edge.\n\nthese are the results of the high probability of hitting the edge of the grid under the random policy.\n\nState AA is the best state to be in under this policy.\n\nNote: AA’s expected return is less than its immediate reward of 1010 because from AA the agent is taken to state A′A' from which it is likely to run into the edge of the grid.\n\nState BB, on the other hand, is valued more than its immediate reward of 55, because from BB the agent is taken to B′B' which has a positive value.\n\nFrom B′B' the expected penalty (negative reward) for possibly running into an edge is more than compensated for by the expected gain for possibly stumbling onto AA or BB."
  },
  {
    "objectID": "05_mdp_lp.html#optimal-policies-and-optimal-value-functions",
    "href": "05_mdp_lp.html#optimal-policies-and-optimal-value-functions",
    "title": "05_mdp_lp",
    "section": " Optimal Policies and Optimal Value Functions ",
    "text": "Optimal Policies and Optimal Value Functions \n\nWe want to find a policy that achieves a lot of reward over the long run.\nValue functions define a partial ordering over policies: a policy π\\pi is defined to be better than or equal to a policy π′\\pi' if its expected return is greater than or equal to that of π′\\pi' for all states. π≥π′⇔vπ(s)≥vπ′(s),∀s∈𝒮. \\pi \\geq \\pi' \\Longleftrightarrow v_{\\pi}(s) \\geq v_{\\pi'}(s), \\quad\n\\forall s \\in \\mathcal{S}. \n∃\\exists always at least one policy that is better than or equal to all other policies: an optimal policy.\n\nAlthough there may be more than one, let us denot all the optimal policies by π*\\pi_\\ast.\nThey share the same state-value function, called the optimal state-value function:\nv*(s)≜maxπvπ(s),∀s∈𝒮.(13) v_\\ast(s) \\triangleq \\operatorname{max}_\\pi v_{\\pi}(s), \\quad\n\\forall s \\in \\mathcal{S}.  \\qquad(13)\n\nOptimal policies also share the same optimal action-value function:\nq*(s,a)≜maxπqπ(s,a),∀s∈𝒮,a∈𝒜(s).(14) q_\\ast(s, a) \\triangleq \\operatorname{max}_\\pi q_{\\pi}(s, a), \\quad\n  \\forall s \\in \\mathcal{S}, \\; a \\in \\mathcal{A}(s).  \\qquad(14)\nFor the state-action pair (s,a)(s,a), this function gives the expected return for taking action aa in state ss and thereafter following an optimal policy.\n\nThus, we can write q*q_\\ast in terms of v*v_\\ast as follows: q*(s,a)=𝔼[Rt+1+γv*(St+1)∣St=s,At=a].(15) q_\\ast (s, a) = \\mathbb{E}[R_{t+1} + \\gamma v_\\ast(S_{t+1}) \\mid S_t = s, A_t =\na].  \\qquad(15)"
  },
  {
    "objectID": "05_mdp_lp.html#bellman-optimality-equation",
    "href": "05_mdp_lp.html#bellman-optimality-equation",
    "title": "05_mdp_lp",
    "section": "Bellman Optimality Equation",
    "text": "Bellman Optimality Equation\n\nBecause v*v_\\ast is the value function for a policy, it must satisfy the self-consistency condition given by the Bellman equation for state values Equation 12.\nBut it is also the optimal value function so v*v_\\ast’s consistency condition can be written in a special form w/o reference to any specific policy!\n\n\n\n\nThe Bellman optimality equation expresses the fact that the value of a state under an optimal policy must equal the expected return for the best action from that state.\n\n\n\nv*(s)=maxa∈𝒜(s)qπ*(s,a)=maxa𝔼π*[Gt|St=s,At=a]=maxa𝔼π*[Rt+1+γGt+1|St=s,At=a]=maxa𝔼π*[Rt+1+γv*(St+1)|St=s,At=a]=maxa∑s′,rp(s′,r|s,a)[r+γv*(s′)].(16)\n\\begin{align}\nv_\\ast(s) &= \\operatorname{max}_{a \\in \\mathcal{A}(s)} q_{\\pi_\\ast}(s, a) =\n\\operatorname{max}_{a} \\mathbb{E}_{\\pi_\\ast}[G_t | S_t =s, A_t = a] \\\\\n&= \\operatorname{max}_a \\mathbb{E}_{\\pi_\\ast}[R_{t+1} + \\gamma G_{t+1} | S_t =\ns, A_t = a] \\\\\n&= \\operatorname{max}_a \\mathbb{E}_{\\pi_\\ast}[R_{t+1} + \\gamma v_\\ast(S_{t+1}) | S_t =\ns, A_t = a] \\\\\n&= \\operatorname{max}_a \\sum_{s', r} p(s', r|s, a)[r + \\gamma v_\\ast(s')].\n\\end{align}\n \\qquad(16)\n\nThe Bellman optimality equation for q*q_\\ast is\n\nq*(s,a)=𝔼[Rt+1+γmaxa′q*(St+1,a)∣St=s,At=a]=∑s′,rp(s′,r|s,a)[r+γmaxa′q*(s′,a′)].(17)\nq_\\ast(s, a) = \\mathbb{E}\\left[R_{t+1} + \\gamma \\operatorname{max}_{a'}\nq_\\ast(S_{t+1}, a) \\mid S_t = s, A_t = a\\right] = \\sum_{s', r} p(s', r|s, a) \n\\left[ r+ \\gamma \\operatorname{max}_{a'} q_\\ast (s', a') \\right].\n \\qquad(17)"
  },
  {
    "objectID": "05_mdp_lp.html#linear-programming-methods",
    "href": "05_mdp_lp.html#linear-programming-methods",
    "title": "05_mdp_lp",
    "section": "Linear Programming Methods",
    "text": "Linear Programming Methods\n\n\n\n\n\nShort-hand notation\n\n\nFor any function J:𝒮→ℝJ: \\mathcal{S} \\rightarrow \\mathbb{R}, define the functional TJTJ by\n(TJ)(s)=maxa∈𝒜(s)r(s,a)+𝔼[γJ(St+1)∣St=s,At=a],s∈𝒮. \n\\begin{align}\n(TJ)(s) = \\operatorname{max}_{a \\in \\mathcal{A}(s)} &r(s,a) + \\mathbb{E}\\left[ \n    \\gamma J(S_{t+1}) \\mid \\right. \\\\\n    &\\left. S_t = s, A_t = a\\right], \\quad s \\in \\mathcal{S}.\n\\end{align}\n\n\n\n\n\n\n\n\nMonotonicity Lemma\n\n\nFor any J:𝒮→ℝJ: \\mathcal{S} \\rightarrow \\mathbb{R} and J′:𝒮→ℝJ': \\mathcal{S} \\rightarrow \\mathbb{R}, such that for all s∈𝒮s \\in \\mathcal{S}, J(s)≥J′(s)J(s) \\geq J'(s), and any stationary policy π:𝒮→𝒜\\pi: \\mathcal{S} \\rightarrow \\mathcal{A}, we have (TkJ)(s)≥(TkJ′)(s),s∈𝒮,k=1,2,…. (T^kJ)(s) \\geq (T^kJ')(s), \\quad s \\in \\mathcal{S}, \\;\\; k = 1, 2, \\ldots .  In particular, if J:𝒮→ℛJ: \\mathcal{S} \\rightarrow \\mathcal{R} is such that for all s∈𝒮s \\in \\mathcal{S}, J(s)≥(TJ)(s)J(s) \\geq (TJ)(s), (TkJ)(s)≥(Tk+1J)(s),s∈𝒮,k=1,2,…. (T^kJ)(s) \\geq (T^{k+1}J)(s), \\quad s \\in \\mathcal{S}, \\;\\; k = 1, 2, \\ldots .\n\n\n\n\n\n\n\n\nMain idea\n\n\nSince TT is monotone, if J≥TJJ \\geq TJ for some JJ, we also have J≥TkJJ \\geq T^kJ for all kk and since1 limk→∞TkJ=J*\\lim_{k\\rightarrow \\infty} T^kJ = J^\\ast, it follows that J≥TJ⇒J≥J*=TJ*. J \\geq TJ \\quad \\Rightarrow \\quad J \\geq J^\\ast = TJ^\\ast. \n\nThus, J*J^\\ast is the “smallest” JJ that satisfies the constraint J≥TJJ \\geq TJ.\nThis constraint can be written as a finite system of linear inequalities, yielding an LP to find the optimal value function.\n\n\n\n\nminimize∑sJ(s)subject toJ(s)≥r(s,a)+γ∑s′p(s′|s,a)J(s′),s,s′∈𝒮,a∈𝒜(s).(18)\n\\begin{align}\n\\operatorname{minimize} & \\sum_{s} J(s) \\\\\n\\text{subject to} & J(s) \\geq r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) J(s'), \\quad\ns, s' \\in \\mathcal{S}, \\;\\; a \\in \\mathcal{A}(s).\n\\end{align}\n \\qquad(18)\nD. Bertsekas, Dynamic Programming and Optimal Control Vol. 2, Athena Scientific, 2018"
  },
  {
    "objectID": "05_mdp_lp.html#cost-approximation-based-on-lp",
    "href": "05_mdp_lp.html#cost-approximation-based-on-lp",
    "title": "05_mdp_lp",
    "section": "Cost Approximation-Based on LP",
    "text": "Cost Approximation-Based on LP\n\n\n\nWhen the number of states is very large or infinite, we may consdier finding an approximation to the optimal value fuinction.\n\nThis can be used, in turn, to obtain a (suboptimal) policy by maximization in Bellman’s equation.\n\n\n\n\n\n\nLinear form approximation\n\n\nJ̃(s;θ)=∑k=1sθkϕk(s), \\tilde{J}(s; \\theta) = \\sum_{k=1}^s \\theta_k \\phi_k(s), \n\nθ=(θ1,θ2,…,θs)\\theta = (\\theta_1, \\theta_2, \\ldots, \\theta_s) is a vector of parameters.\nϕk:𝒮→ℝ\\phi_k: \\mathcal{S} \\rightarrow \\mathbb{R} are some fixed known functions &ndash, called the basis functions.\n\n\n\n\n\n\n\nIt is then possible to determine θ\\theta by using J̃(s;θ)\\tilde{J}(s; \\theta) in place of J*J^\\ast in the LP approach.\n\nIn particular, we optimize over θ\\theta the following program\n\n\nminimize∑s∈𝒮̃J̃(s;θ)subject toJ̃(s;θ)≥r(s,a)+γ∑s′∈𝒮p(s′|s,a)J̃(s′;θ),s∈𝒮,a∈𝒜̃(s),(19)\n\\begin{align}\n\\operatorname{minimize} & \\sum_{s \\in \\tilde{\\mathcal{S}}} \\tilde{J}(s; \\theta) \\\\\n\\text{subject to} & \\tilde{J}(s; \\theta) \\geq r(s, a) + \\gamma \\sum_{s' \\in\n\\mathcal{S}} p(s' | s, a) \\tilde{J}(s'; \\theta), \\quad s \\in \\mathcal{S}, \\;\\; a\n\\in \\tilde{\\mathcal{A}}(s),\n\\end{align}\n \\qquad(19)\n\n𝒮̃\\tilde{\\mathcal{S}} is either the state-space 𝒮\\mathcal{S} or a suitably chosen finite subset of 𝒮\\mathcal{S},\n𝒜̃\\tilde{\\mathcal{A}} is either the action-space 𝒜\\mathcal{A} or a suitably chosen finite subset of 𝒜\\mathcal{A}.\n\n\n\nOptimization Theory and Practice • Aykut C. Satici"
  }
]