[
  {
    "objectID": "06_basic_unc.html#optimization-theory-and-practice",
    "href": "06_basic_unc.html#optimization-theory-and-practice",
    "title": "06_basic_unc",
    "section": "Optimization Theory and Practice",
    "text": "Optimization Theory and Practice\n\n\nBasic Properties of Solutions and Algorithms\n\n\n\n\nInstructor: Aykut Satici, Ph.D.Â   Mechanical and Biomedical Engineering  Electrical and Computer Engineering  Boise State University, Boise, ID, USA\n\n\nTopics:  First-Order Necessary Conditions  Examples  Second-Order Conditions  Convex and Concave Functions  Minimization and Maximization of Convex Functions  Global Convergence of Descent Algorithms"
  },
  {
    "objectID": "06_basic_unc.html#feasible-and-descent-directions",
    "href": "06_basic_unc.html#feasible-and-descent-directions",
    "title": "06_basic_unc",
    "section": "Feasible and Descent Directions",
    "text": "Feasible and Descent Directions\n\n\n\n\n\nDefinition (relative minimum or local minimum).\n\n\nA point ğ±*âˆˆÎ©\\bm{x}^\\ast \\in \\Omega is said to be a relative minimum point of ff over Î©\\Omega if âˆƒÎµ>0\\exists \\varepsilon > 0 such that f(ğ±)â‰¥f(ğ±*)f(\\bm{x}) \\geq f(\\bm{x}^\\ast) for all ğ±âˆˆÎ©\\bm{x} \\in \\Omega within a distance Îµ\\varepsilon of ğ±*\\bm{x}^\\ast.\n\n\n\n\n\n\n\nDefinition (global minimum).\n\n\nA point ğ±*âˆˆÎ©\\bm{x}^\\ast \\in \\Omega is said to be a global minimum point of ff over Î©\\Omega if f(ğ±)â‰¥f(ğ±*)f(\\bm{x}) \\geq f(\\bm{x}^\\ast) for all ğ±âˆˆÎ©\\bm{x} \\in \\Omega.\n\nUsually impossible to find w/ gradient-based methods.\n\n\n\n\n\n\n\nAlong any given direction, the objective function can be regarded as a function of a single variable: the parameter defining movement in this direction.\n\n\n\n\nFeasible direction\n\n\nGiven ğ±âˆˆÎ©\\bm{x} \\in \\Omega we say that a vector ğ\\bm{d} is a feasible direction at ğ±\\bm{x} if there is an Î±â€¾>0\\bar{\\alpha} > 0 such that ğ±+Î±ğâˆˆÎ©\\bm{x} + \\alpha \\bm{d} \\in \\Omega for all Î±\\alpha with 0â‰¤Î±â‰¤Î±â€¾0 \\leq \\alpha \\leq \\bar{\\alpha}.\n\n\n\n\n\n\nDescent direction\n\n\nAn element of the set of directions with the property {ğ:âˆ‡f(ğ±)ğ<0}\\{\\bm{d}: \\nabla f(\\bm{x}) \\bm{d} < 0\\} is called a descent direction.\nIf f(ğ±)âˆˆC1f(\\bm{x}) \\in C^1, then there is Î±â€¾>0\\bar{\\alpha} > 0 such that f(ğ±+Î±ğ)<f(ğ±)f(\\bm{x} + \\alpha \\bm{d}) < f(\\bm{x}) for all Î±\\alpha with 0<Î±â‰¤Î±â€¾0 < \\alpha \\leq \\bar{\\alpha}. The direction ğâŠ¤=âˆ’âˆ‡f(ğ±)\\bm{d}^\\top = -\\nabla f(\\bm{x}) is the steepest descent one."
  },
  {
    "objectID": "06_basic_unc.html#first-order-necessary-conditions-1",
    "href": "06_basic_unc.html#first-order-necessary-conditions-1",
    "title": "06_basic_unc",
    "section": "First-Order Necessary Conditions",
    "text": "First-Order Necessary Conditions\n\n\n\nProposition (FONC).\n\n\nIf ğ±*\\bm{x}^\\ast is a relative minimum point of ff over Î©âŠ†â„n\\Omega \\subseteq \\mathbb{R}^n, then for any ğâˆˆâ„n\\bm{d} \\in \\mathbb{R}^n that is a feasible direction at ğ±*\\bm{x}^\\ast, we have âˆ‡f(ğ±*)ğâ‰¥0\\nabla f(\\bm{x}^\\ast) \\bm{d} \\geq 0.\n\n\n\n\n\n\nProof.\n\n\nFor any Î±\\alpha, 0â‰¤Î±â‰¤Î±â€¾0 \\leq \\alpha \\leq \\bar{\\alpha}, the point ğ±(Î±)=ğ±*+Î±ğâˆˆÎ©\\bm{x}(\\alpha) = \\bm{x}^\\ast + \\alpha \\bm{d} \\in \\Omega. For 0â‰¤Î±â‰¤Î±â€¾0 \\leq \\alpha \\leq \\bar{\\alpha} define the function g(Î±)=f(ğ±(Î±))g(\\alpha) = f(\\bm{x}(\\alpha)). Then gg has a relative minimum at Î±=0\\alpha = 0. By ordinary calculus, we have\ng(Î±)âˆ’g(0)=gâ€²(0)Î±+o(Î±)(1) g(\\alpha) - g(0) = g'(0)\\alpha + o(\\alpha)  \\qquad(1)\nwhere o(Î±)o(\\alpha) denotes terms that go to zero faster than Î±\\alpha. If gâ€²(0)<0g'(0) < 0, then for sufficiently small values of Î±\\alpha, the rhs of EquationÂ 1 will be negative and hence g(Î±)âˆ’g(0)<0g(\\alpha) - g(0) < 0, which contradicts the minimality of g(0)g(0). Thus gâ€²(0)=âˆ‡f(ğ±*)ğâ‰¥0g'(0) = \\nabla f(\\bm{x}^\\ast) \\bm{d} \\geq 0.\n\n\n\n\n\n\nCorollary (Unconstrained Case).\n\n\nLet Î©âŠ†â„n\\Omega \\subseteq \\mathbb{R}^n and let fâˆˆC1f \\in C^1 on Î©\\Omega. If ğ±*\\bm{x}^\\ast is a relative minimum point of ff over Î©\\Omega and if ğ±*âˆˆÎ©ÌŠ\\bm{x}^\\ast \\in \\mathring{\\Omega}, then âˆ‡f(ğ±*)=0\\nabla f(\\bm{x}^\\ast) = \\bm{0}."
  },
  {
    "objectID": "06_basic_unc.html#first-order-sufficient-conditions",
    "href": "06_basic_unc.html#first-order-sufficient-conditions",
    "title": "06_basic_unc",
    "section": "First-Order Sufficient Conditions",
    "text": "First-Order Sufficient Conditions\n\n\n\nProposition (FOSC).\n\n\nLet fâˆˆC1f \\in C^1 be a convex function on â„n\\mathbb{R}^n. If ğ±*\\bm{x}^\\ast meets the first-order conditions âˆ‡f(ğ±*)=0\\nabla f(\\bm{x}^\\ast) = \\bm{0}, ğ±*\\bm{x}^\\ast is a global minimizer of ff.\n\n\n\n\n\n\nExamples\n\n\nExample 1. Consider the problem minimizef(x1,x2)=x12âˆ’x1x2+x22âˆ’3x2. \\operatorname{minimize} f(x_1, x_2) = x_1^2 - x_1x_2 + x_2^2 - 3x_2.  There are no constraints, Î©=â„2\\Omega = \\mathbb{R}^2. Setting the partial derivatives of ff equal to zero yields\n2x1âˆ’x2=0,âˆ’x1+2x2=3. 2x_1 - x_ 2= 0, \\quad -x_1 + 2x_2 = 3. \nwhich has the unique solution x1=1x_1 = 1, x2=2x_2 = 2. This is a global minimum point of ff.\nExample 2. minimizef(x1,x2)=x12âˆ’x1+x2+x1x2,subject tox1,x2â‰¥0.\n\\begin{align}\n\\operatorname{minimize} & f(x_1, x_2) = x_1^2 - x_1 + x_2 + x_1x_2, \\\\\n\\text{subject to} & x_1, x_2 \\geq 0.\n\\end{align}\n This problem has a global minimum at x1=12x_1 = \\frac{1}{2}, x2=0x_2 = 0. At this point\nâˆ‚fâˆ‚x1=2x1âˆ’1+x2=0,âˆ‚fâˆ‚x2=1+x1=32. \\frac{\\partial f}{\\partial x_1} = 2x_1 - 1 + x_2 = 0, \\quad \\frac{\\partial\nf}{\\partial x_2} = 1 + x_1 = \\frac{3}{2}. \n\nThus the partial derivatives do not both vanish at the solution\nSince any feasible direction must have an x2x_2 component greater than or equal to zero, we have âˆ‡f(ğ±*)ğâ‰¥0,âˆ€ğâˆˆâ„2. \\nabla f(\\bm{x}^\\ast) \\bm{d} \\geq 0, \\;\\; \\forall \\bm{d} \\in \\mathbb{R}^2."
  },
  {
    "objectID": "06_basic_unc.html#example-1-logistic-regression",
    "href": "06_basic_unc.html#example-1-logistic-regression",
    "title": "06_basic_unc",
    "section": "Example 1 â€“ Logistic Regression",
    "text": "Example 1 â€“ Logistic Regression\n\nWe have vectors ğšiâˆˆâ„d\\bm{a}_i \\in \\mathbb{R}^d for i=1,2,â€¦,n1i = 1, 2, \\ldots, n_1 in a class and vectors ğ›jâˆˆâ„d\\bm{b}_j \\in \\mathbb{R}^d for j=1,2,â€¦,n2j = 1, 2, \\ldots, n_2 not in that class.\nWe wish to classify them, i.e., find a vector ğ²âˆˆâ„d\\bm{y} \\in \\mathbb{R}^d and a number Î²\\beta such that\n\nexp(ğšiâŠ¤ğ²+Î²)1+exp(ğšiâŠ¤ğ²+Î²)â‰ˆ1,âˆ€i, and exp(ğ›jâŠ¤ğ²+Î²)1+exp(ğ›jâŠ¤ğ²+Î²)â‰ˆ0,âˆ€j. \\frac{\\operatorname{exp}(\\bm{a}_i^\\top \\bm{y} + \\beta)}{1 +\n\\operatorname{exp}(\\bm{a}_i^\\top \\bm{y} + \\beta)} \\approx 1, \\;\\; \\forall i,\n\\quad \\text{ and } \\quad \\frac{\\operatorname{exp}(\\bm{b}_j^\\top \\bm{y} +\n\\beta)}{1 + \\operatorname{exp}(\\bm{b}_j^\\top \\bm{y} + \\beta)} \\approx 0, \\;\\;\n\\forall j. \n\nThis problem can be cast as an unconstrained optimization problem\n\nmaximizeğ²,Î²(âˆiexp(ğšiâŠ¤ğ²+Î²)1+exp(ğšiâŠ¤ğ²+Î²))(âˆj(1âˆ’exp(ğ›jâŠ¤ğ²+Î²)1+exp(ğ›jâŠ¤ğ²+Î²))) \\operatorname{maximize}_{\\bm{y}, \\beta} \n\\left(\\prod_i \\frac{\\operatorname{exp}(\\bm{a}_i^\\top \\bm{y} + \\beta)}{1 +\n\\operatorname{exp}(\\bm{a}_i^\\top \\bm{y} + \\beta)}\\right) \\left(\\prod_j\n\\left(1 - \\frac{\\operatorname{exp}(\\bm{b}_j^\\top \\bm{y} + \\beta)}{1 +\n\\operatorname{exp}(\\bm{b}_j^\\top \\bm{y} + \\beta)} \\right) \\right)\n\nwhich may equivalently be expressed using a log transformation as\nminimizeğ²,Î²âˆ‘ilog(1+exp(âˆ’ğšiâŠ¤ğ²âˆ’Î²))+âˆ‘jlog(1+exp(ğ›iâŠ¤ğ²+Î²)).\n\\operatorname{minimize}_{\\bm{y}, \\beta} \\sum_i \\operatorname{log}\\left(1 +\n\\operatorname{exp}(-\\bm{a}_i^\\top \\bm{y} - \\beta) \\right) + \\sum_j\n\\operatorname{log}\\left(1 + \\operatorname{exp}(\\bm{b}_i^\\top \\bm{y} + \\beta)\n\\right).\n\n\n\n\n\n\nâˆ(eğšiâŠ¤ğ²+Î²1+eğšiâŠ¤ğ²+Î²)=âˆ(11+eâˆ’ğšiâŠ¤ğ²âˆ’Î²)\n\\prod \\left( \\frac{e^{\\bm{a}_i^\\top \\bm{y} + \\beta}}{1 + e^{\\bm{a}_i^\\top\n\\bm{y} + \\beta}} \\right) = \\prod \\left( \\frac{1}{1 + e^{-\\bm{a}_i^\\top\n\\bm{y} - \\beta}} \\right)\n\n\n\n\n\n\n\n\nâˆ(1âˆ’eğ›jâŠ¤ğ²+Î²1+eğ›jâŠ¤ğ²+Î²)=âˆ(11+eğ›jâŠ¤ğ²+Î²)\n\\prod \\left( 1 - \\frac{e^{\\bm{b}_j^\\top \\bm{y} + \\beta}}{1 + e^{\\bm{b}_j^\\top\n\\bm{y} + \\beta}} \\right) = \\prod \\left( \\frac{1}{1 + e^{\\bm{b}_j^\\top\n\\bm{y} + \\beta}} \\right)"
  },
  {
    "objectID": "06_basic_unc.html#example-2-parametric-estimation-convex",
    "href": "06_basic_unc.html#example-2-parametric-estimation-convex",
    "title": "06_basic_unc",
    "section": "Example 2 â€“ Parametric Estimation (Convex)",
    "text": "Example 2 â€“ Parametric Estimation (Convex)\n\nA common use of optimization is for the purpose of function approximation.\nSuppose that through an experiment the value of a function gg is observed at mm points: x1,x2,â€¦,xmx_1, x_2, \\ldots, x_m. Thus the values g(x1),g(x2),â€¦,g(xm)g(x_1), g(x_2), \\ldots, g(x_m) are known.\nWe wish to approximate the function by a polynomial of degree n<mn < m:\n\nh(x)=a0+a1x+â‹¯+anâˆ’1xnâˆ’1+anxn. h(x) = a_0 + a_1 x + \\cdots + a_{n-1}x^{n-1} + a_nx^n. \n\nDefine the errors Îµk=g(xk)âˆ’h(xk)\\varepsilon_k = g(x_k) - h(x_k) and define the best approximation as the polynomial that minimizes the sum-of-squares of these errors\n\nminimizeğšf(ğš)=âˆ‘k=1nÎµk2=âˆ‘k=1n[g(xk)âˆ’(a0+a1xk+â‹¯+anâˆ’1xknâˆ’1+anxkn)]2. \\operatorname{minimize}_\\bm{a} f(\\bm{a}) = \\sum_{k=1}^n \\varepsilon_k^2 =\n\\sum_{k=1}^n \\left[g(x_k) - \\left(a_0 + a_1 x_k + \\cdots + a_{n-1}x_k^{n-1} +\na_nx_k^n\\right) \\right]^2.  \n\nTo find a compact representation for this objective, define\n\nqijâ‰œâˆ‘k=1mxki+j,bj=âˆ‘k=1ng(xk)xkj, and c=âˆ‘k=1ng(xk)2. q_{ij} \\triangleq \\sum_{k=1}^m x_k^{i+j}, \\quad b_j = \\sum_{k=1}^n\ng(x_k)x_k^j, \\quad \\text{ and } c = \\sum_{k=1}^n g(x_k)^2. \n\nThen after a bit of algebra, it can be shown that f(ğš)=ğšâŠ¤ğğšâˆ’2ğ›âŠ¤ğš+c\\quad f(\\bm{a}) = \\bm{a}^\\top \\bm{Qa} - 2\\bm{b}^\\top\\bm{a} + c."
  },
  {
    "objectID": "06_basic_unc.html#example-2-parametric-estimation-nonconvex",
    "href": "06_basic_unc.html#example-2-parametric-estimation-nonconvex",
    "title": "06_basic_unc",
    "section": " Example 2 â€“ Parametric Estimation (Nonconvex) ",
    "text": "Example 2 â€“ Parametric Estimation (Nonconvex) \n\n\n\nEstimating the parameters of a neural network is typically nonconvex.\nThis network has 66 layers, where the initial layer is the input vector ğ±=ğŸ0\\bm{x} = \\bm{f}^0 and the last layer is the function output ğŸ(ğ±)=ğŸ5\\bm{f}(\\bm{x}) = \\bm{f}^5.\n\n\n\n\n\n\n\n\nThe vector function ğŸâ„“\\bm{f}^\\ell, â„“=0,1,â€¦,5\\ell = 0, 1, \\ldots, 5, is defined recursively by the parameter weights between two consecutive layers wijâ„“âˆ’1w_{ij}^{\\ell-1} as a piecewise linear/affine function\n\nfjâ„“=max{0,âˆ‘iwijâ„“âˆ’1fiâ„“âˆ’1},âˆ€j. f_j^\\ell = \\operatorname{max}\\left\\{0, \\sum_i w_{ij}^{\\ell-1} f_i^{\\ell -1\n}\\right\\}, \\quad \\forall j. \n\nSimilarly, for a sequence of variable value vector ğ±k\\bm{x}^k and observed function value vector ğ (ğ±k)\\bm{g}(\\bm{x}^k),\n\nWe would like to find all weights (wijâ„“)\\left(w_{ij}^\\ell \\right)â€™s to minimize the total difference between ğŸ(ğ±k)\\bm{f}(\\bm{x}^k) and ğ (ğ±k)\\bm{g}(\\bm{x}^k) for all kk. âˆ‘k|ğŸ(ğ±k)âˆ’ğ (ğ±k)|2. \\sum_k \\left| \\bm{f}(\\bm{x}^k) - \\bm{g}(\\bm{x}^k) \\right|^2."
  },
  {
    "objectID": "06_basic_unc.html#second-order-necessary-conditions",
    "href": "06_basic_unc.html#second-order-necessary-conditions",
    "title": "06_basic_unc",
    "section": "Second-Order Necessary Conditions",
    "text": "Second-Order Necessary Conditions\n\n\n\nProposition (Second-Order Necessary Conditions)\n\n\nLet Î©âŠ†â„n\\Omega \\subseteq \\mathbb{R}^n and let fâˆˆC2f \\in C^2 on Î©\\Omega. If ğ±*\\bm{x}^\\ast is a relative minimum point of ff over Î©\\Omega, then for any ğâˆˆâ„n\\bm{d} \\in \\mathbb{R}^n that is a feasible direction at ğ±*\\bm{x}^\\ast we have\nâˆ‡f(ğ±*)ğâ‰¥0(2) \\nabla f(\\bm{x}^\\ast) \\bm{d} \\geq 0  \\qquad(2) âˆ‡f(ğ±*)ğ=0, then ğâŠ¤âˆ‡2f(ğ±*)ğ=ğâŠ¤ğ…(ğ±*)ğâ‰¥0.(3) \\nabla f(\\bm{x}^\\ast) \\bm{d} = 0, \\;\\; \\text{ then } \\;\\; \\bm{d}^\\top\n\\nabla^2 f(\\bm{x}^\\ast) \\bm{d} = \\bm{d}^\\top \\bm{F}(\\bm{x}^\\ast) \\bm{d} \\geq 0. \\qquad(3)\n\n\n\n\n\n\nProof\n\n\nThe first condition is just the FONC, and the second applies only if âˆ‡f(ğ±*)ğ=0\\nabla f(\\bm{x}^\\ast)\\bm{d} = 0. In this case, introducing ğ±(Î±)=ğ±*+Î±ğ\\bm{x}(\\alpha) = \\bm{x}^\\ast + \\alpha \\bm{d} and g(Î±)=f(ğ±(Î±))g(\\alpha) = f(\\bm{x}(\\alpha)) as before, we have, in view of gâ€²(Î±)=0g'(\\alpha) = 0,\ng(Î±)âˆ’g(0)=12gâ€³(0)Î±2+o(Î±2). g(\\alpha) - g(0) = \\frac{1}{2}g''(0)\\alpha^2 + o(\\alpha^2). \nIf gâ€³(0)<0g''(0) < 0a the rhs of the above equation is negative for sufficiently small Î±\\alpha which contradicts the relative minimum nature of g(0)g(0). Thus\ngâ€³(0)=ğâŠ¤âˆ‡2f(ğ±*)ğâ‰¥0. g''(0) = \\bm{d}^\\top \\nabla^2 f(\\bm{x}^\\ast)\\bm{d} \\geq 0. \n\n\n\n\nSee Example 2 from FOSC slide with d2=0d_2 = 0 whence ğâŠ¤âˆ‡2f(ğ±*)ğ=2d12â‰¥0\\bm{d}^\\top \\nabla^2 f(\\bm{x}^\\ast) \\bm{d} = 2d_1^2 \\geq 0, satisfying the second condition."
  },
  {
    "objectID": "06_basic_unc.html#second-order-necessary-conditions-1",
    "href": "06_basic_unc.html#second-order-necessary-conditions-1",
    "title": "06_basic_unc",
    "section": "Second-Order Necessary Conditions",
    "text": "Second-Order Necessary Conditions\n\n\n\nProposition (Second-Order Necessary Conditions â€” Unconstrained Case)\n\n\nLet ğ±*âˆˆÎ©ÌŠ\\bm{x}^\\ast \\in \\mathring{\\Omega} and suppose ğ±*\\bm{x}^\\ast is a relative minimum point over Î©\\Omega of fâˆˆC2f \\in C^2. Then\nâˆ‡f(ğ±*)=0(4) \\nabla f(\\bm{x}^\\ast) = 0  \\qquad(4) âˆ€ğ,ğâŠ¤ğ…(ğ±*)ğâ‰¥0.(5) \\forall \\bm{d},  \\;\\; \\bm{d}^\\top \\bm{F}(\\bm{x}^\\ast) \\bm{d} \\geq 0. \\qquad(5)\n\nThe second condition is equivalent to stating that the matrix ğ…(ğ±*)\\bm{F}(\\bm{x}^\\ast) is positive semidefinite.\n\n\n\n\n\n\n\n\n\nExample\n\n\nminimizef(x1,x2)=x13âˆ’x12x2+2x22subject tox1,x2â‰¥0.\n\\begin{align}\n\\operatorname{minimize} & f(x_1, x_2) = x_1^3 - x_1^2x_2 + 2x_2^2 \\\\\n\\text{subject to} & x_1, x_2 \\geq 0.\n\\end{align}\n\n\n\n\n\nIf x1,x2>0x_1, x_2 > 0, then the FONC are\n3x12âˆ’2x1x2=0,âˆ’x12+4x2=0. 3x_1^2 - 2x_1x_2 = 0, \\quad -x_1^2 + 4x_2 = 0. \nwith a solution at x1=6x_1 = 6, x2=9x_2 = 9.\n\n\n\n\n\nNote that for x1x_1 fixed at x1=6x_1 = 6, the objective attains a relative minimum w.r.t. x2x_2 at x2=9x_2 = 9.\nConversely, with x2x_2 fixed at x2=9x_2 = 9, the objective attains a relative minimum w.r.t. x1x_1 at x1=6x_1 = 6.\n\n\n\nDespite this fact, the point x1=6x_1 = 6, x2=9x_2 =9 is not a relative minimum because the Hessian matrix is\nğ…(ğ±)=[6x1âˆ’2x2âˆ’2x1âˆ’2x14];ğ…(ğ±*)=[18âˆ’12âˆ’124]â‹¡0.\n\\bm{F}(\\bm{x}) = \\begin{bmatrix} 6x_1 - 2x_2 & -2x_1 \\\\ -2x_1 & 4 \\end{bmatrix};\n\\qquad \n\\bm{F}(\\bm{x}^\\ast) = \\begin{bmatrix} 18 & -12 \\\\ -12 & 4 \\end{bmatrix} \\nsucceq  0."
  },
  {
    "objectID": "06_basic_unc.html#second-order-sufficient-conditions",
    "href": "06_basic_unc.html#second-order-sufficient-conditions",
    "title": "06_basic_unc",
    "section": "Second-Order Sufficient Conditions",
    "text": "Second-Order Sufficient Conditions\n\n\n\nProposition (Second-Order Sufficient Conditions)\n\n\nLet fâˆˆC2f \\in C^2 be a function defined on a region in which the point ğ±*\\bm{x}^\\ast is an interior point. Suppose\nâˆ‡f(ğ±*)=0(6) \\nabla f(\\bm{x}^\\ast) = 0  \\qquad(6) ğ…(ğ±*)â‰»0(7) \\bm{F}(\\bm{x}^\\ast) \\succ 0  \\qquad(7)\nThen ğ±*\\bm{x}^\\ast is a strict relative minimum of ff.\n\n\n\n\n\n\nProof\n\n\nSince ğ…(ğ±*)\\bm{F}(\\bm{x}^\\ast) is positive definite, there is an a>0a > 0 such that for all ğ\\bm{d}, ğâŠ¤ğ…(ğ±*)ğâ‰¥a|ğ|2\\bm{d}^\\top \\bm{F}(\\bm{x}^\\ast)\\bm{d} \\geq a |\\bm{d}|^2. By Taylorâ€™s Theorem (with remainder)\nf(ğ±*+ğ)âˆ’f(ğ±*)=12ğâŠ¤ğ…(ğ±*)ğ+o(|ğ|2)â‰¥a2|ğ|2+o(|ğ|2). f(\\bm{x}^\\ast + \\bm{d}) - f(\\bm{x}^\\ast) = \\frac{1}{2} \\bm{d}^\\top\n\\bm{F}(\\bm{x}^\\ast)\\bm{d} + o(|\\bm{d}|^2) \\geq \\frac{a}{2}|\\bm{d}|^2 +\no(|\\bm{d}|^2). \nFor small |ğ||\\bm{d}|, the first term on the right dominates the second, implying that both sides are positive for small ğ\\bm{d}."
  },
  {
    "objectID": "06_basic_unc.html#properties-of-convex-functions",
    "href": "06_basic_unc.html#properties-of-convex-functions",
    "title": "06_basic_unc",
    "section": "Properties of Convex Functions",
    "text": "Properties of Convex Functions\n\n\n\n\n\nProposition.\n\n\nLet f1f_1 and f2f_2 be convex functions on the convex set Î©\\Omega. Then the function f1+f2f_1 + f_2 is convex on Î©\\Omega.\n\n\n\n\n\n\nProof.\n\n\nLet ğ±1,ğ±2âˆˆÎ©\\bm{x}_1, \\bm{x}_2 \\in \\Omega, and 0<Î±<10 < \\alpha < 1. Then\nf1(Î±ğ±1+(1âˆ’Î±)ğ±2)+f2(Î±ğ±1+(1âˆ’Î±)ğ±2)â‰¤Î±[f1(ğ±1)+f2(ğ±2)]+(1âˆ’Î±)[f1(ğ±1)+f2(ğ±2)]. \n\\begin{align}\n&f_1(\\alpha \\bm{x}_1 + (1-\\alpha)\\bm{x}_2) + f_2(\\alpha \\bm{x}_1 +\n(1-\\alpha)\\bm{x}_2) \\\\ &\\leq \\alpha [f_1(\\bm{x}_1) + f_2(\\bm{x}_2)] +\n(1-\\alpha)[f_1(\\bm{x}_1) + f_2(\\bm{x}_2)]. \n\\end{align}\n\n\n\n\n\n\n\nProposition.\n\n\nLet ff be a convex function over the convex set Î©\\Omega. Then the function afaf is convex for any aâ‰¥0a \\geq 0.\n\n\n\n\n\n\nCorollary\n\n\nA conic combination of convex function a1f1+a2f2+â‹¯+amfma_1f_1 + a_2f_2 + \\cdots + a_mf_m is again convex.\n\n\n\n\n\n\n\nProposition.\n\n\nLet ff be a convex function on a convex set Î©\\Omega. The (sublevel) set Î“c={ğ±:ğ±âˆˆÎ©,f(ğ±)â‰¤c}\\Gamma_c = \\{\\bm{x}: \\bm{x} \\in \\Omega, f(\\bm{x}) \\leq c\\} is convex for every real number cc.\n\n\n\n\n\n\nProof.\n\n\nLet ğ±1,ğ±2âˆˆÎ“c\\bm{x}_1, \\bm{x}_2 \\in \\Gamma_c. Then f(ğ±1)â‰¤cf(\\bm{x}_1) \\leq c, f(ğ±2)â‰¤cf(\\bm{x}_2) \\leq c and for 0<Î±<10 < \\alpha < 1,\nf(Î±ğ±1+(1âˆ’Î±)ğ±2)â‰¤Î±f(ğ±1)+(1âˆ’Î±)f(ğ±2)â‰¤c.\nf(\\alpha \\bm{x}_1 + (1-\\alpha)\\bm{x}_2) \\leq \\alpha f(\\bm{x}_1) +\n(1-\\alpha)f(\\bm{x}_2) \\leq c.\n\nThus Î±ğ±1+(1âˆ’Î±)ğ±2âˆˆÎ“c\\alpha \\bm{x}_1 + (1-\\alpha)\\bm{x}_2 \\in \\Gamma_c.\n\n\n\n\n\n\nCorollary\n\n\nSince the intersection of convex sets is also convex, the set of points simultaneously satisfying\nf1(ğ±)â‰¤c1,f2(ğ±)â‰¤c2,â€¦,fm(ğ±)â‰¤cm,\nf_1(\\bm{x}) \\leq c_1, \\;\\; f_2(\\bm{x}) \\leq c_2, \\ldots, f_m(\\bm{x}) \\leq c_m,\n\nwhere each fif_i is a convex function, defines a convex set."
  },
  {
    "objectID": "06_basic_unc.html#properties-of-differentiable-convex-functions",
    "href": "06_basic_unc.html#properties-of-differentiable-convex-functions",
    "title": "06_basic_unc",
    "section": " Properties of Differentiable Convex Functions ",
    "text": "Properties of Differentiable Convex Functions \n\n\n\n\n\nProposition\n\n\nLet fâˆˆC1f \\in C^1. Then ff is convex over a convex set Î©\\Omega iff f(ğ²)â‰¥f(ğ±)+âˆ‡f(ğ±)(ğ²âˆ’ğ±),âˆ€ğ±,ğ²âˆˆÎ©. f(\\bm{y}) \\geq f(\\bm{x}) + \\nabla f(\\bm{x})(\\bm{y} - \\bm{x}), \\quad \\forall\n\\bm{x}, \\bm{y} \\in \\Omega. \n\n\n\n\n\n\n\n\nProof\n\n\nFirst suppose ff is convex. Then for all Î±,0â‰¤Î±â‰¤1\\alpha, 0 \\leq \\alpha \\leq 1,\nf(Î±ğ²+(1âˆ’Î±)ğ±)â‰¤Î±f(ğ²)+(1âˆ’Î±)f(ğ±). f(\\alpha \\bm{y} + (1-\\alpha)\\bm{x}) \\leq \\alpha f(\\bm{y}) +\n(1-\\alpha)f(\\bm{x}). \nThus for 0<Î±â‰¤10 < \\alpha \\leq 1\nf(ğ±+Î±(ğ²âˆ’ğ±))âˆ’f(ğ±)Î±â‰¤f(ğ²)âˆ’f(ğ±).\\frac{f(\\bm{x}+\\alpha(\\bm{y}-\\bm{x})) - f(\\bm{x})}{\\alpha} \\leq f(\\bm{y}) -\nf(\\bm{x}). \nLetting Î±â†’0\\alpha \\rightarrow 0 we obtain\nâˆ‡f(ğ±)(ğ²âˆ’ğ±)â‰¤f(ğ²)âˆ’f(ğ±). \\nabla f(\\bm{x})(\\bm{y}-\\bm{x}) \\leq f(\\bm{y}) - f(\\bm{x}). \nThis proves the â€œonly ifâ€ part.\n\n\n\n\n\n\n\n\nProof\n\n\nNow assume\nf(ğ²)â‰¥f(ğ±)+âˆ‡f(ğ±)(ğ²âˆ’ğ±)f(\\bm{y}) \\geq f(\\bm{x}) + \\nabla f(\\bm{x})(\\bm{y} - \\bm{x})\nfor all ğ±,ğ²âˆˆÎ©\\bm{x}, \\bm{y} \\in \\Omega. Fix ğ±1,ğ±2âˆˆÎ©\\bm{x}_1, \\bm{x}_2 \\in \\Omega and Î±\\alpha, 0â‰¤Î±â‰¤10 \\leq \\alpha \\leq 1. Setting ğ±=Î±ğ±1+(1âˆ’Î±)ğ±2\\bm{x} = \\alpha \\bm{x}_1 + (1-\\alpha) \\bm{x}_2 and alternatively ğ²=ğ±1\\bm{y} = \\bm{x}_1 or ğ²=ğ±2\\bm{y} = \\bm{x}_2, we have\nf(ğ±1)â‰¥f(ğ±)+âˆ‡f(ğ±)(ğ±1âˆ’ğ±)f(ğ±2)â‰¥f(ğ±)+âˆ‡f(ğ±)(ğ±2âˆ’ğ±).\n\\begin{align}\nf(\\bm{x}_1) &\\geq f(\\bm{x}) + \\nabla f(\\bm{x})(\\bm{x}_1 - \\bm{x}) \\\\\nf(\\bm{x}_2) &\\geq f(\\bm{x}) + \\nabla f(\\bm{x})(\\bm{x}_2 - \\bm{x}).\n\\end{align}\n\nMultiply the first equation by Î±\\alpha and the second equation by (1âˆ’Î±)(1-\\alpha) and add to obtain\nÎ±f(ğ±1)+(1âˆ’Î±)f(ğ±2)â‰¥f(ğ±)+âˆ‡f(ğ±)[Î±ğ±1+(1âˆ’Î±)ğ±2âˆ’ğ±]. \\alpha f(\\bm{x}_1) + (1-\\alpha)f(\\bm{x}_2) \\geq f(\\bm{x}) + \\nabla\nf(\\bm{x})[\\alpha \\bm{x}_1 + (1-\\alpha)\\bm{x}_2 - \\bm{x}]. \nBut substituting ğ±=Î±ğ±1+(1âˆ’Î±)ğ±2\\bm{x} = \\alpha \\bm{x}_1 + (1-\\alpha)\\bm{x}_2, we obtain\nÎ±f(ğ±1)+(1âˆ’Î±)f(ğ±2)â‰¥f(Î±ğ±1+(1âˆ’Î±)ğ±2). \\alpha f(\\bm{x}_1) + (1-\\alpha)f(\\bm{x}_2) \\geq f(\\alpha \\bm{x}_1 +\n(1-\\alpha)\\bm{x}_2). \n\n\n\n\n\n\n\n\n\n\n\n\nIllustration of the proposition\n\n\nThis is a dual characterization of the original definition.\n\nThe original definition states that a linear interpolation between two points overestimates the function.\nThis characterization states that linear approximation based on the local derivative underestimates the function."
  },
  {
    "objectID": "06_basic_unc.html#properties-of-differentiable-convex-functions-1",
    "href": "06_basic_unc.html#properties-of-differentiable-convex-functions-1",
    "title": "06_basic_unc",
    "section": " Properties of Differentiable Convex Functions ",
    "text": "Properties of Differentiable Convex Functions \n\n\n\n\n\nProposition\n\n\nLet fâˆˆC2f \\in C^2. Then ff is convex over a convex set Î©\\Omega containing an interior point if and only if the Hessian matrix ğ…\\bm{F} of ff is positive semidefinite throughout Î©\\Omega.\n\n\n\n\n\n\n\n\nProof\n\n\nBy Taylorâ€™s theorem we have\nf(ğ²)âˆ’f(ğ±)=âˆ‡f(ğ±)(ğ²âˆ’ğ±)+12(ğ²âˆ’ğ±)âŠ¤ğ…(ğ±+Î±(ğ²âˆ’ğ±))(ğ²âˆ’ğ±)(8)\n\\begin{align}\nf(\\bm{y}) - f(\\bm{x}) &= \\nabla f(\\bm{x})(\\bm{y}-\\bm{x}) + \\\\\n&\\frac{1}{2}(\\bm{y}-\\bm{x})^\\top \\bm{F}(\\bm{x} +\n\\alpha(\\bm{y}-\\bm{x}))(\\bm{y}-\\bm{x})\n\\end{align}\n \\qquad(8)\nfor some Î±\\alpha, 0â‰¤Î±â‰¤10 \\leq \\alpha \\leq 1. Clearly, if the Hessian is everywhere positive semidefinite, we have\nf(ğ²)â‰¥f(ğ±)+âˆ‡f(ğ±)(ğ²âˆ’ğ±),(9) f(\\bm{y}) \\geq f(\\bm{x}) + \\nabla f(\\bm{x}) (\\bm{y} - \\bm{x}),  \\qquad(9)\nwhich in view of the previous proposition implies that ff is convex.\n\n\n\n\n\n\n\n\nProof\n\n\nNow suppose the Hessian is not positive semidefinite at some point ğ±âˆˆÎ©\\bm{x} \\in \\Omega. By the continuity of the Hessian, it can be assumed w.l.o.g., that ğ±\\bm{x} is an interior point of Î©\\Omega. There is a ğ²âˆˆÎ©\\bm{y} \\in \\Omega such that (ğ²âˆ’ğ±)âŠ¤ğ…(ğ±)(ğ²âˆ’ğ±)<0(\\bm{y}-\\bm{x})^\\top \\bm{F}(\\bm{x})(\\bm{y}-\\bm{x}) < 0.\nAgain, by the continuity of the Hessian, ğ²\\bm{y} may be selected so that for all Î±\\alpha, 0â‰¤Î±â‰¤10 \\leq \\alpha \\leq 1,\n(ğ²âˆ’ğ±)âŠ¤ğ…(ğ±+Î±(ğ²âˆ’ğ±))(ğ²âˆ’ğ±)<0. (\\bm{y}-\\bm{x})^\\top \\bm{F}(\\bm{x}+\\alpha(\\bm{y}-\\bm{x}))(\\bm{y}-\\bm{x}) < 0.\n\nThis, in view of EquationÂ 8 implies that EquationÂ 9 does not hold; which in view of the previous proposition implies that ff is not convex.\n\n\n\n\n\n\n\nThe Hessian matrix is the generalization to â„n\\mathbb{R}^n of the concept of the curvature of a function.\n\npositive definiteness of the Hessian is the generalization of positive curvature.\n\nConvex functions have positive (at least nonnegative) curvature in every direction.\nWe refer to a function as being locally convex if its Hessian matrix is positive semidefinite in a neighborhood.\nThe SOSC requires that the fcn. be locally strictly convex at the point ğ±*\\bm{x}^\\ast.\nHence the local sufficiency theory is intimately related to convexity."
  },
  {
    "objectID": "06_basic_unc.html#minimization-of-and-maximization-convex-functions",
    "href": "06_basic_unc.html#minimization-of-and-maximization-convex-functions",
    "title": "06_basic_unc",
    "section": " Minimization of and Maximization Convex Functions ",
    "text": "Minimization of and Maximization Convex Functions \n\n\n\n\n\nTheorem\n\n\nLet ff be a convex function defined on the convex set Î©\\Omega. Then the set Î“\\Gamma where ff achieves its minimum is convex, and any relative minimum of ff is a global minimum.\n\n\n\n\n\n\nProof\n\n\nIf ff has no relative minima the theorem is valid by default. Assume now that c0c_0 is the minimum of ff. Then clearly Î“={ğ±:f(ğ±)â‰¤c0,ğ±âˆˆÎ©}\\Gamma = \\{\\bm{x}: f(\\bm{x}) \\leq c_0, \\; \\bm{x} \\in \\Omega \\} and this is convex by a proposition we covered earlier.\nSuppose now that ğ±*âˆˆÎ©\\bm{x}^\\ast \\in \\Omega is a relative minimum point of ff, but that there is another point ğ²âˆˆÎ©\\bm{y} \\in \\Omega with f(ğ²)<f(ğ±*)f(\\bm{y}) < f(\\bm{x}^\\ast). On the line Î±ğ²+(1âˆ’Î±)ğ±*\\alpha \\bm{y} + (1-\\alpha)\\bm{x}^\\ast, 0<Î±<10 < \\alpha < 1 we have\nf(Î±ğ²+(1âˆ’Î±)ğ±*)â‰¤Î±f(ğ²)+(1âˆ’Î±)f(ğ±*)<f(ğ±*), f(\\alpha \\bm{y} + (1-\\alpha)\\bm{x}^\\ast) \\leq \\alpha f(\\bm{y}) + (1-\\alpha)\nf(\\bm{x}^\\ast) < f(\\bm{x}^\\ast), \ncontradicting the fact that ğ±*\\bm{x}^\\ast is a relative minimum point.\n\n\n\n\nAll minimum points of a convex function are located together and are global minima.\n\n\n\n\n\nTheorem\n\n\nLet fâˆˆC1f \\in C^1 be convex on the convex set Î©\\Omega. If there is a point ğ±*âˆˆÎ©\\bm{x}^\\ast \\in \\Omega such that for all ğ²âˆˆÎ©\\bm{y} \\in \\Omega, âˆ‡f(ğ±*)(ğ²âˆ’ğ±*)â‰¥0\\nabla f(\\bm{x}^\\ast)(\\bm{y} - \\bm{x}^\\ast) \\geq 0, then ğ±*\\bm{x}^\\ast is a global minimum point of ff over Î©\\Omega.\n\n\n\n\n\n\nProof\n\n\nSince ğ²âˆ’ğ±*\\bm{y}-\\bm{x}^\\ast is a feasible direction at ğ±*\\bm{x}^\\ast, the given condition is equivalent to the FONC. The proof of the propostion is now immediate since by an earlier proposition we have\nf(ğ²)â‰¥f(ğ±*)+âˆ‡f(ğ±*)(ğ²âˆ’ğ±*)â‰¥f(ğ±*). f(\\bm{y}) \\geq f(\\bm{x}^\\ast) + \\nabla f(\\bm{x}^\\ast)(\\bm{y} - \\bm{x}^\\ast)\n\\geq f(\\bm{x}^\\ast). \n\n\n\n\n\n\nTheorem\n\n\nLet ff be a convex function defined on the bounded, closed convex set Î©\\Omega. If ff has a maximum over Î©\\Omega, it is achieved at an extreme point of Î©\\Omega.\n\n\n\n\n\n\nProof â€“ Omitted."
  },
  {
    "objectID": "06_basic_unc.html#algorithms",
    "href": "06_basic_unc.html#algorithms",
    "title": "06_basic_unc",
    "section": "Algorithms",
    "text": "Algorithms\n\nIterative: the algorithm generates a series of points, each point being calculated on the basis of the points preceding it.\nDescent: as each new point is generated by the algorithm the corresponding value of some function decreases in value.\nAn iterative algorithm is initiated by specifying a starting point.\n\nIf for arbitrary starting points, the algorithm is guaranteed to generate a sequence of points converging to a solution, then the algorithm is said to be globally convergent.\nNot all algorithms have this desirable property: indeed, many of the most important algorithms for NLP are not globally convergent.\nThey occasionally generate sequences that do not converge at all or converge to points that are not solutions."
  },
  {
    "objectID": "06_basic_unc.html#iterative-algorithms",
    "href": "06_basic_unc.html#iterative-algorithms",
    "title": "06_basic_unc",
    "section": "Iterative Algorithms",
    "text": "Iterative Algorithms\n\n\n\nWe formally define an algorithm ğ€\\bm{A} as a mapping, ğ€:Xâ†’X\\;\\; \\bm{A}: X \\rightarrow X. Operated iteratively, the algorithm ğ€\\bm{A} initiated at ğ±0âˆˆX\\bm{x}_0 \\in X would generates a sequence {ğ±k}\\{\\bm{x}_k\\} defined by\n\nğ±k+1=ğ€(ğ±k). \\bm{x}_{k+1} = \\bm{A}(\\bm{x}_k). \n\n\n\nDefinition (Algorithm â€“ a generalization)\n\n\nAn algorithm ğ€\\bm{A} is a mapping defined on a space XX that assigns to every point ğ±âˆˆX\\bm{x} \\in X a subset of XX. That is, the mapping ğ€\\bm{A} is a point-to-set mapping of XX.\n\n\n\n\nAn algorithm ğ€\\bm{A}, therefore, generates a sequence of points in the following way.\n\nGiven ğ±kâˆˆX\\bm{x}_k \\in X, the algorithm yields ğ€(ğ±k)âŠ†X\\bm{A}(\\bm{x}_k) \\subseteq X.\nFrom this subset an arbitrary element ğ±k+1\\bm{x}_{k+1} is selected.\nIn this way, given an initial point ğ±0\\bm{x}_0, the algorithm generates sequences through the iteration\n\n\nğ±k+1âˆˆğ€(ğ±k). \\bm{x}_{k+1} \\in \\bm{A}(\\bm{x}_k). \n\nThe utility of this more general definition is that it allows one to analyze the convergence of an infinite family of similar algorithms.\n\n\n\n\n\n\nExample\n\n\nSuppose for xâˆˆâ„x \\in \\mathbb{R}, we define\nA(x)=[âˆ’|x|2,|x|2] A(x) = \\left[ -\\frac{|x|}{2}, \\frac{|x|}{2} \\right ] \nso that A(x)A(x) is an interval of the real line.\n\nLet us start at x0=100x_0 = 100.\nEach of the following sequences below might be generated from an iterative application of this algorithm.\n\n{100,50,25,12,âˆ’6,âˆ’2,1,12,â€¦} \\left\\{100, 50, 25, 12, -6, -2, 1, \\frac{1}{2}, \\ldots \\right\\}  {100,âˆ’40,20,âˆ’5,âˆ’2,1,14,18,â€¦} \\left\\{100, -40, 20, -5, -2, 1, \\frac{1}{4}, \\frac{1}{8}, \\ldots \\right\\}  {100,10,âˆ’1,116,1100,âˆ’11000,110000,â€¦} \\left\\{100, 10, -1, \\frac{1}{16}, \\frac{1}{100}, -\\frac{1}{1000}, \\frac{1}{10000}, \\ldots \\right\\}"
  },
  {
    "objectID": "06_basic_unc.html#descent",
    "href": "06_basic_unc.html#descent",
    "title": "06_basic_unc",
    "section": "Descent",
    "text": "Descent\n\n\n\nDefinition (Descent)\n\n\nLet Î“âŠ†X\\Gamma \\subseteq X be a given solution set and let ğ€\\bm{A} be an algorithm on XX. A continuous real-valued function ZZ on XX is said to be a descent function for Î“\\Gamma and ğ€\\bm{A} if it satisfies\n\nif ğ±âˆ‰Î“\\bm{x} \\notin \\Gamma and ğ²âˆˆğ€(ğ±)\\bm{y} \\in \\bm{A}(\\bm{x}), then Z(ğ²)<Z(ğ±)Z(\\bm{y}) < Z(\\bm{x}),\nif ğ±âˆˆÎ“\\bm{x} \\in \\Gamma and ğ²âˆˆğ€(ğ±)\\bm{y} \\in \\bm{A}(\\bm{x}), then Z(ğ²)â‰¤Z(ğ±)Z(\\bm{y}) \\leq Z(\\bm{x}).\n\n\n\n\n\nThere are a number of ways a solution set, algorithm, and descent function can be defined.\nA natural set-up for the problem\n\nminimizef(ğ±)subject toğ±âˆˆÎ©\n\\begin{align}\n\\operatorname{minimize} & f(\\bm{x}) \\\\\n\\text{subject to} & \\bm{x} \\in \\Omega\n\\end{align}\n\n\nÎ“\\Gamma be the set of minimizing points; define an algorithm ğ€\\bm{A} on Î©\\Omega in such a way that ff decreases at each step (descent function).\nAnother possibility: Î“\\Gamma is the set of points ğ±\\bm{x} satisfying âˆ‡f(ğ±)=0\\nabla f(\\bm{x}) = 0. |âˆ‡f(ğ±)||\\nabla f(\\bm{x})| might then serve as a descent function."
  },
  {
    "objectID": "06_basic_unc.html#global-convergence-theorem",
    "href": "06_basic_unc.html#global-convergence-theorem",
    "title": "06_basic_unc",
    "section": "Global Convergence Theorem",
    "text": "Global Convergence Theorem\n\n\n\n\n\nThe set-up\n\n\n\nThere is a solution set Î“\\Gamma\nPoints are generated according to ğ±k+1âˆˆğ€(ğ±k)\\bm{x}_{k+1} \\in \\bm{A}(\\bm{x}_k)\nEach new point strictly decreases a descent function ğ™\\bm{Z} unless the solution set Î“\\Gamma is reached.\n\n\n\n\n\n\n\nGlobal Convergence Theorem\n\n\nLet ğ€\\bm{A} be an algorithm on XX, and suppose that, given ğ±0\\bm{x}_0, the sequence {ğ±k}k=0âˆ\\left\\{\\bm{x}_k\\right\\}_{k=0}^\\infty is generated satisfying\nğ±k+1âˆˆğ€(ğ±k). \\bm{x}_{k+1} \\in \\bm{A}(\\bm{x}_k). \nLet a solution set Î“âŠ†X\\Gamma \\subseteq X be given, and suppose\n\nall points ğ±k\\bm{x}_k are contained in a compact set SâŠ†XS \\subseteq X,\nthere is a continuous function ğ™\\bm{Z} on XX such that\n\nif ğ±âˆ‰Î“\\bm{x} \\notin \\Gamma and ğ²âˆˆğ€(ğ±)\\bm{y} \\in \\bm{A}(\\bm{x}), then Z(ğ²)<Z(ğ±)Z(\\bm{y}) < Z(\\bm{x}),\nif ğ±âˆˆÎ“\\bm{x} \\in \\Gamma and ğ²âˆˆğ€(ğ±)\\bm{y} \\in \\bm{A}(\\bm{x}), then Z(ğ²)â‰¤Z(ğ±)Z(\\bm{y}) \\leq Z(\\bm{x}).\n\nthe mapping ğ€\\bm{A} is closed at points outside Î“\\Gamma.\n\nThen the limit of any convergent subsequence of {ğ±k}\\{\\bm{x}_k\\} is a solution.\n\n\n\n\n\n\n\n\nExample\n\n\n\n\nConsider the point-to-set algorithm ğ€\\bm{A} defined by the graph in the figure on the right.\nThis is given explicitly on X=[0,1]X = [0, 1] by\nğ€(x)={[0,x)n0<xâ‰¤10x=0, \n\\bm{A}(x) = \n\\begin{cases}\n  [0, x)  & n 0 < x \\leq 1 \\\\\n  0 & x = 0,\n\\end{cases}\n\nLetting Î“={0}\\Gamma = \\{0\\}, the function Z(x)=xZ(x) = x serves as a descent function because for xâ‰ 0x \\neq 0 all points in ğ€(x)\\bm{A}(x) are less than xx.\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptimization Theory and Practice â€¢ Aykut C. Satici"
  }
]