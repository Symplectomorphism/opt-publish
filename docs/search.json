[
  {
    "objectID": "06_basic_unc.html#optimization-theory-and-practice",
    "href": "06_basic_unc.html#optimization-theory-and-practice",
    "title": "06_basic_unc",
    "section": "Optimization Theory and Practice",
    "text": "Optimization Theory and Practice\n\n\nBasic Properties of Solutions and Algorithms\n\n\n\n\nInstructor: Aykut Satici, Ph.D.   Mechanical and Biomedical Engineering  Electrical and Computer Engineering  Boise State University, Boise, ID, USA\n\n\nTopics:  First-Order Necessary Conditions  Examples  Second-Order Conditions  Convex and Concave Functions  Minimization and Maximization of Convex Functions  Global Convergence of Descent Algorithms"
  },
  {
    "objectID": "06_basic_unc.html#feasible-and-descent-directions",
    "href": "06_basic_unc.html#feasible-and-descent-directions",
    "title": "06_basic_unc",
    "section": "Feasible and Descent Directions",
    "text": "Feasible and Descent Directions\n\n\n\n\n\nDefinition (relative minimum or local minimum).\n\n\nA point 𝐱*∈Ω\\bm{x}^\\ast \\in \\Omega is said to be a relative minimum point of ff over Ω\\Omega if ∃ε>0\\exists \\varepsilon > 0 such that f(𝐱)≥f(𝐱*)f(\\bm{x}) \\geq f(\\bm{x}^\\ast) for all 𝐱∈Ω\\bm{x} \\in \\Omega within a distance ε\\varepsilon of 𝐱*\\bm{x}^\\ast.\n\n\n\n\n\n\n\nDefinition (global minimum).\n\n\nA point 𝐱*∈Ω\\bm{x}^\\ast \\in \\Omega is said to be a global minimum point of ff over Ω\\Omega if f(𝐱)≥f(𝐱*)f(\\bm{x}) \\geq f(\\bm{x}^\\ast) for all 𝐱∈Ω\\bm{x} \\in \\Omega.\n\nUsually impossible to find w/ gradient-based methods.\n\n\n\n\n\n\n\nAlong any given direction, the objective function can be regarded as a function of a single variable: the parameter defining movement in this direction.\n\n\n\n\nFeasible direction\n\n\nGiven 𝐱∈Ω\\bm{x} \\in \\Omega we say that a vector 𝐝\\bm{d} is a feasible direction at 𝐱\\bm{x} if there is an α‾>0\\bar{\\alpha} > 0 such that 𝐱+α𝐝∈Ω\\bm{x} + \\alpha \\bm{d} \\in \\Omega for all α\\alpha with 0≤α≤α‾0 \\leq \\alpha \\leq \\bar{\\alpha}.\n\n\n\n\n\n\nDescent direction\n\n\nAn element of the set of directions with the property {𝐝:∇f(𝐱)𝐝<0}\\{\\bm{d}: \\nabla f(\\bm{x}) \\bm{d} < 0\\} is called a descent direction.\nIf f(𝐱)∈C1f(\\bm{x}) \\in C^1, then there is α‾>0\\bar{\\alpha} > 0 such that f(𝐱+α𝐝)<f(𝐱)f(\\bm{x} + \\alpha \\bm{d}) < f(\\bm{x}) for all α\\alpha with 0<α≤α‾0 < \\alpha \\leq \\bar{\\alpha}. The direction 𝐝⊤=−∇f(𝐱)\\bm{d}^\\top = -\\nabla f(\\bm{x}) is the steepest descent one."
  },
  {
    "objectID": "06_basic_unc.html#first-order-necessary-conditions-1",
    "href": "06_basic_unc.html#first-order-necessary-conditions-1",
    "title": "06_basic_unc",
    "section": "First-Order Necessary Conditions",
    "text": "First-Order Necessary Conditions\n\n\n\nProposition (FONC).\n\n\nIf 𝐱*\\bm{x}^\\ast is a relative minimum point of ff over Ω⊆ℝn\\Omega \\subseteq \\mathbb{R}^n, then for any 𝐝∈ℝn\\bm{d} \\in \\mathbb{R}^n that is a feasible direction at 𝐱*\\bm{x}^\\ast, we have ∇f(𝐱*)𝐝≥0\\nabla f(\\bm{x}^\\ast) \\bm{d} \\geq 0.\n\n\n\n\n\n\nProof.\n\n\nFor any α\\alpha, 0≤α≤α‾0 \\leq \\alpha \\leq \\bar{\\alpha}, the point 𝐱(α)=𝐱*+α𝐝∈Ω\\bm{x}(\\alpha) = \\bm{x}^\\ast + \\alpha \\bm{d} \\in \\Omega. For 0≤α≤α‾0 \\leq \\alpha \\leq \\bar{\\alpha} define the function g(α)=f(𝐱(α))g(\\alpha) = f(\\bm{x}(\\alpha)). Then gg has a relative minimum at α=0\\alpha = 0. By ordinary calculus, we have\ng(α)−g(0)=g′(0)α+o(α)(1) g(\\alpha) - g(0) = g'(0)\\alpha + o(\\alpha)  \\qquad(1)\nwhere o(α)o(\\alpha) denotes terms that go to zero faster than α\\alpha. If g′(0)<0g'(0) < 0, then for sufficiently small values of α\\alpha, the rhs of Equation 1 will be negative and hence g(α)−g(0)<0g(\\alpha) - g(0) < 0, which contradicts the minimality of g(0)g(0). Thus g′(0)=∇f(𝐱*)𝐝≥0g'(0) = \\nabla f(\\bm{x}^\\ast) \\bm{d} \\geq 0.\n\n\n\n\n\n\nCorollary (Unconstrained Case).\n\n\nLet Ω⊆ℝn\\Omega \\subseteq \\mathbb{R}^n and let f∈C1f \\in C^1 on Ω\\Omega. If 𝐱*\\bm{x}^\\ast is a relative minimum point of ff over Ω\\Omega and if 𝐱*∈Ω̊\\bm{x}^\\ast \\in \\mathring{\\Omega}, then ∇f(𝐱*)=0\\nabla f(\\bm{x}^\\ast) = \\bm{0}."
  },
  {
    "objectID": "06_basic_unc.html#first-order-sufficient-conditions",
    "href": "06_basic_unc.html#first-order-sufficient-conditions",
    "title": "06_basic_unc",
    "section": "First-Order Sufficient Conditions",
    "text": "First-Order Sufficient Conditions\n\n\n\nProposition (FOSC).\n\n\nLet f∈C1f \\in C^1 be a convex function on ℝn\\mathbb{R}^n. If 𝐱*\\bm{x}^\\ast meets the first-order conditions ∇f(𝐱*)=0\\nabla f(\\bm{x}^\\ast) = \\bm{0}, 𝐱*\\bm{x}^\\ast is a global minimizer of ff.\n\n\n\n\n\n\nExamples\n\n\nExample 1. Consider the problem minimizef(x1,x2)=x12−x1x2+x22−3x2. \\operatorname{minimize} f(x_1, x_2) = x_1^2 - x_1x_2 + x_2^2 - 3x_2.  There are no constraints, Ω=ℝ2\\Omega = \\mathbb{R}^2. Setting the partial derivatives of ff equal to zero yields\n2x1−x2=0,−x1+2x2=3. 2x_1 - x_ 2= 0, \\quad -x_1 + 2x_2 = 3. \nwhich has the unique solution x1=1x_1 = 1, x2=2x_2 = 2. This is a global minimum point of ff.\nExample 2. minimizef(x1,x2)=x12−x1+x2+x1x2,subject tox1,x2≥0.\n\\begin{align}\n\\operatorname{minimize} & f(x_1, x_2) = x_1^2 - x_1 + x_2 + x_1x_2, \\\\\n\\text{subject to} & x_1, x_2 \\geq 0.\n\\end{align}\n This problem has a global minimum at x1=12x_1 = \\frac{1}{2}, x2=0x_2 = 0. At this point\n∂f∂x1=2x1−1+x2=0,∂f∂x2=1+x1=32. \\frac{\\partial f}{\\partial x_1} = 2x_1 - 1 + x_2 = 0, \\quad \\frac{\\partial\nf}{\\partial x_2} = 1 + x_1 = \\frac{3}{2}. \n\nThus the partial derivatives do not both vanish at the solution\nSince any feasible direction must have an x2x_2 component greater than or equal to zero, we have ∇f(𝐱*)𝐝≥0,∀𝐝∈ℝ2. \\nabla f(\\bm{x}^\\ast) \\bm{d} \\geq 0, \\;\\; \\forall \\bm{d} \\in \\mathbb{R}^2."
  },
  {
    "objectID": "06_basic_unc.html#example-1-logistic-regression",
    "href": "06_basic_unc.html#example-1-logistic-regression",
    "title": "06_basic_unc",
    "section": "Example 1 – Logistic Regression",
    "text": "Example 1 – Logistic Regression\n\nWe have vectors 𝐚i∈ℝd\\bm{a}_i \\in \\mathbb{R}^d for i=1,2,…,n1i = 1, 2, \\ldots, n_1 in a class and vectors 𝐛j∈ℝd\\bm{b}_j \\in \\mathbb{R}^d for j=1,2,…,n2j = 1, 2, \\ldots, n_2 not in that class.\nWe wish to classify them, i.e., find a vector 𝐲∈ℝd\\bm{y} \\in \\mathbb{R}^d and a number β\\beta such that\n\nexp(𝐚i⊤𝐲+β)1+exp(𝐚i⊤𝐲+β)≈1,∀i, and exp(𝐛j⊤𝐲+β)1+exp(𝐛j⊤𝐲+β)≈0,∀j. \\frac{\\operatorname{exp}(\\bm{a}_i^\\top \\bm{y} + \\beta)}{1 +\n\\operatorname{exp}(\\bm{a}_i^\\top \\bm{y} + \\beta)} \\approx 1, \\;\\; \\forall i,\n\\quad \\text{ and } \\quad \\frac{\\operatorname{exp}(\\bm{b}_j^\\top \\bm{y} +\n\\beta)}{1 + \\operatorname{exp}(\\bm{b}_j^\\top \\bm{y} + \\beta)} \\approx 0, \\;\\;\n\\forall j. \n\nThis problem can be cast as an unconstrained optimization problem\n\nmaximize𝐲,β(∏iexp(𝐚i⊤𝐲+β)1+exp(𝐚i⊤𝐲+β))(∏j(1−exp(𝐛j⊤𝐲+β)1+exp(𝐛j⊤𝐲+β))) \\operatorname{maximize}_{\\bm{y}, \\beta} \n\\left(\\prod_i \\frac{\\operatorname{exp}(\\bm{a}_i^\\top \\bm{y} + \\beta)}{1 +\n\\operatorname{exp}(\\bm{a}_i^\\top \\bm{y} + \\beta)}\\right) \\left(\\prod_j\n\\left(1 - \\frac{\\operatorname{exp}(\\bm{b}_j^\\top \\bm{y} + \\beta)}{1 +\n\\operatorname{exp}(\\bm{b}_j^\\top \\bm{y} + \\beta)} \\right) \\right)\n\nwhich may equivalently be expressed using a log transformation as\nminimize𝐲,β∑ilog(1+exp(−𝐚i⊤𝐲−β))+∑jlog(1+exp(𝐛i⊤𝐲+β)).\n\\operatorname{minimize}_{\\bm{y}, \\beta} \\sum_i \\operatorname{log}\\left(1 +\n\\operatorname{exp}(-\\bm{a}_i^\\top \\bm{y} - \\beta) \\right) + \\sum_j\n\\operatorname{log}\\left(1 + \\operatorname{exp}(\\bm{b}_i^\\top \\bm{y} + \\beta)\n\\right).\n\n\n\n\n\n\n∏(e𝐚i⊤𝐲+β1+e𝐚i⊤𝐲+β)=∏(11+e−𝐚i⊤𝐲−β)\n\\prod \\left( \\frac{e^{\\bm{a}_i^\\top \\bm{y} + \\beta}}{1 + e^{\\bm{a}_i^\\top\n\\bm{y} + \\beta}} \\right) = \\prod \\left( \\frac{1}{1 + e^{-\\bm{a}_i^\\top\n\\bm{y} - \\beta}} \\right)\n\n\n\n\n\n\n\n\n∏(1−e𝐛j⊤𝐲+β1+e𝐛j⊤𝐲+β)=∏(11+e𝐛j⊤𝐲+β)\n\\prod \\left( 1 - \\frac{e^{\\bm{b}_j^\\top \\bm{y} + \\beta}}{1 + e^{\\bm{b}_j^\\top\n\\bm{y} + \\beta}} \\right) = \\prod \\left( \\frac{1}{1 + e^{\\bm{b}_j^\\top\n\\bm{y} + \\beta}} \\right)"
  },
  {
    "objectID": "06_basic_unc.html#example-2-parametric-estimation-convex",
    "href": "06_basic_unc.html#example-2-parametric-estimation-convex",
    "title": "06_basic_unc",
    "section": "Example 2 – Parametric Estimation (Convex)",
    "text": "Example 2 – Parametric Estimation (Convex)\n\nA common use of optimization is for the purpose of function approximation.\nSuppose that through an experiment the value of a function gg is observed at mm points: x1,x2,…,xmx_1, x_2, \\ldots, x_m. Thus the values g(x1),g(x2),…,g(xm)g(x_1), g(x_2), \\ldots, g(x_m) are known.\nWe wish to approximate the function by a polynomial of degree n<mn < m:\n\nh(x)=a0+a1x+⋯+an−1xn−1+anxn. h(x) = a_0 + a_1 x + \\cdots + a_{n-1}x^{n-1} + a_nx^n. \n\nDefine the errors εk=g(xk)−h(xk)\\varepsilon_k = g(x_k) - h(x_k) and define the best approximation as the polynomial that minimizes the sum-of-squares of these errors\n\nminimize𝐚f(𝐚)=∑k=1nεk2=∑k=1n[g(xk)−(a0+a1xk+⋯+an−1xkn−1+anxkn)]2. \\operatorname{minimize}_\\bm{a} f(\\bm{a}) = \\sum_{k=1}^n \\varepsilon_k^2 =\n\\sum_{k=1}^n \\left[g(x_k) - \\left(a_0 + a_1 x_k + \\cdots + a_{n-1}x_k^{n-1} +\na_nx_k^n\\right) \\right]^2.  \n\nTo find a compact representation for this objective, define\n\nqij≜∑k=1mxki+j,bj=∑k=1ng(xk)xkj, and c=∑k=1ng(xk)2. q_{ij} \\triangleq \\sum_{k=1}^m x_k^{i+j}, \\quad b_j = \\sum_{k=1}^n\ng(x_k)x_k^j, \\quad \\text{ and } c = \\sum_{k=1}^n g(x_k)^2. \n\nThen after a bit of algebra, it can be shown that f(𝐚)=𝐚⊤𝐐𝐚−2𝐛⊤𝐚+c\\quad f(\\bm{a}) = \\bm{a}^\\top \\bm{Qa} - 2\\bm{b}^\\top\\bm{a} + c."
  },
  {
    "objectID": "06_basic_unc.html#example-2-parametric-estimation-nonconvex",
    "href": "06_basic_unc.html#example-2-parametric-estimation-nonconvex",
    "title": "06_basic_unc",
    "section": " Example 2 – Parametric Estimation (Nonconvex) ",
    "text": "Example 2 – Parametric Estimation (Nonconvex) \n\n\n\nEstimating the parameters of a neural network is typically nonconvex.\nThis network has 66 layers, where the initial layer is the input vector 𝐱=𝐟0\\bm{x} = \\bm{f}^0 and the last layer is the function output 𝐟(𝐱)=𝐟5\\bm{f}(\\bm{x}) = \\bm{f}^5.\n\n\n\n\n\n\n\n\nThe vector function 𝐟ℓ\\bm{f}^\\ell, ℓ=0,1,…,5\\ell = 0, 1, \\ldots, 5, is defined recursively by the parameter weights between two consecutive layers wijℓ−1w_{ij}^{\\ell-1} as a piecewise linear/affine function\n\nfjℓ=max{0,∑iwijℓ−1fiℓ−1},∀j. f_j^\\ell = \\operatorname{max}\\left\\{0, \\sum_i w_{ij}^{\\ell-1} f_i^{\\ell -1\n}\\right\\}, \\quad \\forall j. \n\nSimilarly, for a sequence of variable value vector 𝐱k\\bm{x}^k and observed function value vector 𝐠(𝐱k)\\bm{g}(\\bm{x}^k),\n\nWe would like to find all weights (wijℓ)\\left(w_{ij}^\\ell \\right)’s to minimize the total difference between 𝐟(𝐱k)\\bm{f}(\\bm{x}^k) and 𝐠(𝐱k)\\bm{g}(\\bm{x}^k) for all kk. ∑k|𝐟(𝐱k)−𝐠(𝐱k)|2. \\sum_k \\left| \\bm{f}(\\bm{x}^k) - \\bm{g}(\\bm{x}^k) \\right|^2."
  },
  {
    "objectID": "06_basic_unc.html#second-order-necessary-conditions",
    "href": "06_basic_unc.html#second-order-necessary-conditions",
    "title": "06_basic_unc",
    "section": "Second-Order Necessary Conditions",
    "text": "Second-Order Necessary Conditions\n\n\n\nProposition (Second-Order Necessary Conditions)\n\n\nLet Ω⊆ℝn\\Omega \\subseteq \\mathbb{R}^n and let f∈C2f \\in C^2 on Ω\\Omega. If 𝐱*\\bm{x}^\\ast is a relative minimum point of ff over Ω\\Omega, then for any 𝐝∈ℝn\\bm{d} \\in \\mathbb{R}^n that is a feasible direction at 𝐱*\\bm{x}^\\ast we have\n∇f(𝐱*)𝐝≥0(2) \\nabla f(\\bm{x}^\\ast) \\bm{d} \\geq 0  \\qquad(2) ∇f(𝐱*)𝐝=0, then 𝐝⊤∇2f(𝐱*)𝐝=𝐝⊤𝐅(𝐱*)𝐝≥0.(3) \\nabla f(\\bm{x}^\\ast) \\bm{d} = 0, \\;\\; \\text{ then } \\;\\; \\bm{d}^\\top\n\\nabla^2 f(\\bm{x}^\\ast) \\bm{d} = \\bm{d}^\\top \\bm{F}(\\bm{x}^\\ast) \\bm{d} \\geq 0. \\qquad(3)\n\n\n\n\n\n\nProof\n\n\nThe first condition is just the FONC, and the second applies only if ∇f(𝐱*)𝐝=0\\nabla f(\\bm{x}^\\ast)\\bm{d} = 0. In this case, introducing 𝐱(α)=𝐱*+α𝐝\\bm{x}(\\alpha) = \\bm{x}^\\ast + \\alpha \\bm{d} and g(α)=f(𝐱(α))g(\\alpha) = f(\\bm{x}(\\alpha)) as before, we have, in view of g′(α)=0g'(\\alpha) = 0,\ng(α)−g(0)=12g″(0)α2+o(α2). g(\\alpha) - g(0) = \\frac{1}{2}g''(0)\\alpha^2 + o(\\alpha^2). \nIf g″(0)<0g''(0) < 0a the rhs of the above equation is negative for sufficiently small α\\alpha which contradicts the relative minimum nature of g(0)g(0). Thus\ng″(0)=𝐝⊤∇2f(𝐱*)𝐝≥0. g''(0) = \\bm{d}^\\top \\nabla^2 f(\\bm{x}^\\ast)\\bm{d} \\geq 0. \n\n\n\n\nSee Example 2 from FOSC slide with d2=0d_2 = 0 whence 𝐝⊤∇2f(𝐱*)𝐝=2d12≥0\\bm{d}^\\top \\nabla^2 f(\\bm{x}^\\ast) \\bm{d} = 2d_1^2 \\geq 0, satisfying the second condition."
  },
  {
    "objectID": "06_basic_unc.html#second-order-necessary-conditions-1",
    "href": "06_basic_unc.html#second-order-necessary-conditions-1",
    "title": "06_basic_unc",
    "section": "Second-Order Necessary Conditions",
    "text": "Second-Order Necessary Conditions\n\n\n\nProposition (Second-Order Necessary Conditions — Unconstrained Case)\n\n\nLet 𝐱*∈Ω̊\\bm{x}^\\ast \\in \\mathring{\\Omega} and suppose 𝐱*\\bm{x}^\\ast is a relative minimum point over Ω\\Omega of f∈C2f \\in C^2. Then\n∇f(𝐱*)=0(4) \\nabla f(\\bm{x}^\\ast) = 0  \\qquad(4) ∀𝐝,𝐝⊤𝐅(𝐱*)𝐝≥0.(5) \\forall \\bm{d},  \\;\\; \\bm{d}^\\top \\bm{F}(\\bm{x}^\\ast) \\bm{d} \\geq 0. \\qquad(5)\n\nThe second condition is equivalent to stating that the matrix 𝐅(𝐱*)\\bm{F}(\\bm{x}^\\ast) is positive semidefinite.\n\n\n\n\n\n\n\n\n\nExample\n\n\nminimizef(x1,x2)=x13−x12x2+2x22subject tox1,x2≥0.\n\\begin{align}\n\\operatorname{minimize} & f(x_1, x_2) = x_1^3 - x_1^2x_2 + 2x_2^2 \\\\\n\\text{subject to} & x_1, x_2 \\geq 0.\n\\end{align}\n\n\n\n\n\nIf x1,x2>0x_1, x_2 > 0, then the FONC are\n3x12−2x1x2=0,−x12+4x2=0. 3x_1^2 - 2x_1x_2 = 0, \\quad -x_1^2 + 4x_2 = 0. \nwith a solution at x1=6x_1 = 6, x2=9x_2 = 9.\n\n\n\n\n\nNote that for x1x_1 fixed at x1=6x_1 = 6, the objective attains a relative minimum w.r.t. x2x_2 at x2=9x_2 = 9.\nConversely, with x2x_2 fixed at x2=9x_2 = 9, the objective attains a relative minimum w.r.t. x1x_1 at x1=6x_1 = 6.\n\n\n\nDespite this fact, the point x1=6x_1 = 6, x2=9x_2 =9 is not a relative minimum because the Hessian matrix is\n𝐅(𝐱)=[6x1−2x2−2x1−2x14];𝐅(𝐱*)=[18−12−124]⋡0.\n\\bm{F}(\\bm{x}) = \\begin{bmatrix} 6x_1 - 2x_2 & -2x_1 \\\\ -2x_1 & 4 \\end{bmatrix};\n\\qquad \n\\bm{F}(\\bm{x}^\\ast) = \\begin{bmatrix} 18 & -12 \\\\ -12 & 4 \\end{bmatrix} \\nsucceq  0."
  },
  {
    "objectID": "06_basic_unc.html#second-order-sufficient-conditions",
    "href": "06_basic_unc.html#second-order-sufficient-conditions",
    "title": "06_basic_unc",
    "section": "Second-Order Sufficient Conditions",
    "text": "Second-Order Sufficient Conditions\n\n\n\nProposition (Second-Order Sufficient Conditions)\n\n\nLet f∈C2f \\in C^2 be a function defined on a region in which the point 𝐱*\\bm{x}^\\ast is an interior point. Suppose\n∇f(𝐱*)=0(6) \\nabla f(\\bm{x}^\\ast) = 0  \\qquad(6) 𝐅(𝐱*)≻0(7) \\bm{F}(\\bm{x}^\\ast) \\succ 0  \\qquad(7)\nThen 𝐱*\\bm{x}^\\ast is a strict relative minimum of ff.\n\n\n\n\n\n\nProof\n\n\nSince 𝐅(𝐱*)\\bm{F}(\\bm{x}^\\ast) is positive definite, there is an a>0a > 0 such that for all 𝐝\\bm{d}, 𝐝⊤𝐅(𝐱*)𝐝≥a|𝐝|2\\bm{d}^\\top \\bm{F}(\\bm{x}^\\ast)\\bm{d} \\geq a |\\bm{d}|^2. By Taylor’s Theorem (with remainder)\nf(𝐱*+𝐝)−f(𝐱*)=12𝐝⊤𝐅(𝐱*)𝐝+o(|𝐝|2)≥a2|𝐝|2+o(|𝐝|2). f(\\bm{x}^\\ast + \\bm{d}) - f(\\bm{x}^\\ast) = \\frac{1}{2} \\bm{d}^\\top\n\\bm{F}(\\bm{x}^\\ast)\\bm{d} + o(|\\bm{d}|^2) \\geq \\frac{a}{2}|\\bm{d}|^2 +\no(|\\bm{d}|^2). \nFor small |𝐝||\\bm{d}|, the first term on the right dominates the second, implying that both sides are positive for small 𝐝\\bm{d}."
  },
  {
    "objectID": "06_basic_unc.html#properties-of-convex-functions",
    "href": "06_basic_unc.html#properties-of-convex-functions",
    "title": "06_basic_unc",
    "section": "Properties of Convex Functions",
    "text": "Properties of Convex Functions\n\n\n\n\n\nProposition.\n\n\nLet f1f_1 and f2f_2 be convex functions on the convex set Ω\\Omega. Then the function f1+f2f_1 + f_2 is convex on Ω\\Omega.\n\n\n\n\n\n\nProof.\n\n\nLet 𝐱1,𝐱2∈Ω\\bm{x}_1, \\bm{x}_2 \\in \\Omega, and 0<α<10 < \\alpha < 1. Then\nf1(α𝐱1+(1−α)𝐱2)+f2(α𝐱1+(1−α)𝐱2)≤α[f1(𝐱1)+f2(𝐱2)]+(1−α)[f1(𝐱1)+f2(𝐱2)]. \n\\begin{align}\n&f_1(\\alpha \\bm{x}_1 + (1-\\alpha)\\bm{x}_2) + f_2(\\alpha \\bm{x}_1 +\n(1-\\alpha)\\bm{x}_2) \\\\ &\\leq \\alpha [f_1(\\bm{x}_1) + f_2(\\bm{x}_2)] +\n(1-\\alpha)[f_1(\\bm{x}_1) + f_2(\\bm{x}_2)]. \n\\end{align}\n\n\n\n\n\n\n\nProposition.\n\n\nLet ff be a convex function over the convex set Ω\\Omega. Then the function afaf is convex for any a≥0a \\geq 0.\n\n\n\n\n\n\nCorollary\n\n\nA conic combination of convex function a1f1+a2f2+⋯+amfma_1f_1 + a_2f_2 + \\cdots + a_mf_m is again convex.\n\n\n\n\n\n\n\nProposition.\n\n\nLet ff be a convex function on a convex set Ω\\Omega. The (sublevel) set Γc={𝐱:𝐱∈Ω,f(𝐱)≤c}\\Gamma_c = \\{\\bm{x}: \\bm{x} \\in \\Omega, f(\\bm{x}) \\leq c\\} is convex for every real number cc.\n\n\n\n\n\n\nProof.\n\n\nLet 𝐱1,𝐱2∈Γc\\bm{x}_1, \\bm{x}_2 \\in \\Gamma_c. Then f(𝐱1)≤cf(\\bm{x}_1) \\leq c, f(𝐱2)≤cf(\\bm{x}_2) \\leq c and for 0<α<10 < \\alpha < 1,\nf(α𝐱1+(1−α)𝐱2)≤αf(𝐱1)+(1−α)f(𝐱2)≤c.\nf(\\alpha \\bm{x}_1 + (1-\\alpha)\\bm{x}_2) \\leq \\alpha f(\\bm{x}_1) +\n(1-\\alpha)f(\\bm{x}_2) \\leq c.\n\nThus α𝐱1+(1−α)𝐱2∈Γc\\alpha \\bm{x}_1 + (1-\\alpha)\\bm{x}_2 \\in \\Gamma_c.\n\n\n\n\n\n\nCorollary\n\n\nSince the intersection of convex sets is also convex, the set of points simultaneously satisfying\nf1(𝐱)≤c1,f2(𝐱)≤c2,…,fm(𝐱)≤cm,\nf_1(\\bm{x}) \\leq c_1, \\;\\; f_2(\\bm{x}) \\leq c_2, \\ldots, f_m(\\bm{x}) \\leq c_m,\n\nwhere each fif_i is a convex function, defines a convex set."
  },
  {
    "objectID": "06_basic_unc.html#properties-of-differentiable-convex-functions",
    "href": "06_basic_unc.html#properties-of-differentiable-convex-functions",
    "title": "06_basic_unc",
    "section": " Properties of Differentiable Convex Functions ",
    "text": "Properties of Differentiable Convex Functions \n\n\n\n\n\nProposition\n\n\nLet f∈C1f \\in C^1. Then ff is convex over a convex set Ω\\Omega iff f(𝐲)≥f(𝐱)+∇f(𝐱)(𝐲−𝐱),∀𝐱,𝐲∈Ω. f(\\bm{y}) \\geq f(\\bm{x}) + \\nabla f(\\bm{x})(\\bm{y} - \\bm{x}), \\quad \\forall\n\\bm{x}, \\bm{y} \\in \\Omega. \n\n\n\n\n\n\n\n\nProof\n\n\nFirst suppose ff is convex. Then for all α,0≤α≤1\\alpha, 0 \\leq \\alpha \\leq 1,\nf(α𝐲+(1−α)𝐱)≤αf(𝐲)+(1−α)f(𝐱). f(\\alpha \\bm{y} + (1-\\alpha)\\bm{x}) \\leq \\alpha f(\\bm{y}) +\n(1-\\alpha)f(\\bm{x}). \nThus for 0<α≤10 < \\alpha \\leq 1\nf(𝐱+α(𝐲−𝐱))−f(𝐱)α≤f(𝐲)−f(𝐱).\\frac{f(\\bm{x}+\\alpha(\\bm{y}-\\bm{x})) - f(\\bm{x})}{\\alpha} \\leq f(\\bm{y}) -\nf(\\bm{x}). \nLetting α→0\\alpha \\rightarrow 0 we obtain\n∇f(𝐱)(𝐲−𝐱)≤f(𝐲)−f(𝐱). \\nabla f(\\bm{x})(\\bm{y}-\\bm{x}) \\leq f(\\bm{y}) - f(\\bm{x}). \nThis proves the “only if” part.\n\n\n\n\n\n\n\n\nProof\n\n\nNow assume\nf(𝐲)≥f(𝐱)+∇f(𝐱)(𝐲−𝐱)f(\\bm{y}) \\geq f(\\bm{x}) + \\nabla f(\\bm{x})(\\bm{y} - \\bm{x})\nfor all 𝐱,𝐲∈Ω\\bm{x}, \\bm{y} \\in \\Omega. Fix 𝐱1,𝐱2∈Ω\\bm{x}_1, \\bm{x}_2 \\in \\Omega and α\\alpha, 0≤α≤10 \\leq \\alpha \\leq 1. Setting 𝐱=α𝐱1+(1−α)𝐱2\\bm{x} = \\alpha \\bm{x}_1 + (1-\\alpha) \\bm{x}_2 and alternatively 𝐲=𝐱1\\bm{y} = \\bm{x}_1 or 𝐲=𝐱2\\bm{y} = \\bm{x}_2, we have\nf(𝐱1)≥f(𝐱)+∇f(𝐱)(𝐱1−𝐱)f(𝐱2)≥f(𝐱)+∇f(𝐱)(𝐱2−𝐱).\n\\begin{align}\nf(\\bm{x}_1) &\\geq f(\\bm{x}) + \\nabla f(\\bm{x})(\\bm{x}_1 - \\bm{x}) \\\\\nf(\\bm{x}_2) &\\geq f(\\bm{x}) + \\nabla f(\\bm{x})(\\bm{x}_2 - \\bm{x}).\n\\end{align}\n\nMultiply the first equation by α\\alpha and the second equation by (1−α)(1-\\alpha) and add to obtain\nαf(𝐱1)+(1−α)f(𝐱2)≥f(𝐱)+∇f(𝐱)[α𝐱1+(1−α)𝐱2−𝐱]. \\alpha f(\\bm{x}_1) + (1-\\alpha)f(\\bm{x}_2) \\geq f(\\bm{x}) + \\nabla\nf(\\bm{x})[\\alpha \\bm{x}_1 + (1-\\alpha)\\bm{x}_2 - \\bm{x}]. \nBut substituting 𝐱=α𝐱1+(1−α)𝐱2\\bm{x} = \\alpha \\bm{x}_1 + (1-\\alpha)\\bm{x}_2, we obtain\nαf(𝐱1)+(1−α)f(𝐱2)≥f(α𝐱1+(1−α)𝐱2). \\alpha f(\\bm{x}_1) + (1-\\alpha)f(\\bm{x}_2) \\geq f(\\alpha \\bm{x}_1 +\n(1-\\alpha)\\bm{x}_2). \n\n\n\n\n\n\n\n\n\n\n\n\nIllustration of the proposition\n\n\nThis is a dual characterization of the original definition.\n\nThe original definition states that a linear interpolation between two points overestimates the function.\nThis characterization states that linear approximation based on the local derivative underestimates the function."
  },
  {
    "objectID": "06_basic_unc.html#properties-of-differentiable-convex-functions-1",
    "href": "06_basic_unc.html#properties-of-differentiable-convex-functions-1",
    "title": "06_basic_unc",
    "section": " Properties of Differentiable Convex Functions ",
    "text": "Properties of Differentiable Convex Functions \n\n\n\n\n\nProposition\n\n\nLet f∈C2f \\in C^2. Then ff is convex over a convex set Ω\\Omega containing an interior point if and only if the Hessian matrix 𝐅\\bm{F} of ff is positive semidefinite throughout Ω\\Omega.\n\n\n\n\n\n\n\n\nProof\n\n\nBy Taylor’s theorem we have\nf(𝐲)−f(𝐱)=∇f(𝐱)(𝐲−𝐱)+12(𝐲−𝐱)⊤𝐅(𝐱+α(𝐲−𝐱))(𝐲−𝐱)(8)\n\\begin{align}\nf(\\bm{y}) - f(\\bm{x}) &= \\nabla f(\\bm{x})(\\bm{y}-\\bm{x}) + \\\\\n&\\frac{1}{2}(\\bm{y}-\\bm{x})^\\top \\bm{F}(\\bm{x} +\n\\alpha(\\bm{y}-\\bm{x}))(\\bm{y}-\\bm{x})\n\\end{align}\n \\qquad(8)\nfor some α\\alpha, 0≤α≤10 \\leq \\alpha \\leq 1. Clearly, if the Hessian is everywhere positive semidefinite, we have\nf(𝐲)≥f(𝐱)+∇f(𝐱)(𝐲−𝐱),(9) f(\\bm{y}) \\geq f(\\bm{x}) + \\nabla f(\\bm{x}) (\\bm{y} - \\bm{x}),  \\qquad(9)\nwhich in view of the previous proposition implies that ff is convex.\n\n\n\n\n\n\n\n\nProof\n\n\nNow suppose the Hessian is not positive semidefinite at some point 𝐱∈Ω\\bm{x} \\in \\Omega. By the continuity of the Hessian, it can be assumed w.l.o.g., that 𝐱\\bm{x} is an interior point of Ω\\Omega. There is a 𝐲∈Ω\\bm{y} \\in \\Omega such that (𝐲−𝐱)⊤𝐅(𝐱)(𝐲−𝐱)<0(\\bm{y}-\\bm{x})^\\top \\bm{F}(\\bm{x})(\\bm{y}-\\bm{x}) < 0.\nAgain, by the continuity of the Hessian, 𝐲\\bm{y} may be selected so that for all α\\alpha, 0≤α≤10 \\leq \\alpha \\leq 1,\n(𝐲−𝐱)⊤𝐅(𝐱+α(𝐲−𝐱))(𝐲−𝐱)<0. (\\bm{y}-\\bm{x})^\\top \\bm{F}(\\bm{x}+\\alpha(\\bm{y}-\\bm{x}))(\\bm{y}-\\bm{x}) < 0.\n\nThis, in view of Equation 8 implies that Equation 9 does not hold; which in view of the previous proposition implies that ff is not convex.\n\n\n\n\n\n\n\nThe Hessian matrix is the generalization to ℝn\\mathbb{R}^n of the concept of the curvature of a function.\n\npositive definiteness of the Hessian is the generalization of positive curvature.\n\nConvex functions have positive (at least nonnegative) curvature in every direction.\nWe refer to a function as being locally convex if its Hessian matrix is positive semidefinite in a neighborhood.\nThe SOSC requires that the fcn. be locally strictly convex at the point 𝐱*\\bm{x}^\\ast.\nHence the local sufficiency theory is intimately related to convexity."
  },
  {
    "objectID": "06_basic_unc.html#minimization-of-and-maximization-convex-functions",
    "href": "06_basic_unc.html#minimization-of-and-maximization-convex-functions",
    "title": "06_basic_unc",
    "section": " Minimization of and Maximization Convex Functions ",
    "text": "Minimization of and Maximization Convex Functions \n\n\n\n\n\nTheorem\n\n\nLet ff be a convex function defined on the convex set Ω\\Omega. Then the set Γ\\Gamma where ff achieves its minimum is convex, and any relative minimum of ff is a global minimum.\n\n\n\n\n\n\nProof\n\n\nIf ff has no relative minima the theorem is valid by default. Assume now that c0c_0 is the minimum of ff. Then clearly Γ={𝐱:f(𝐱)≤c0,𝐱∈Ω}\\Gamma = \\{\\bm{x}: f(\\bm{x}) \\leq c_0, \\; \\bm{x} \\in \\Omega \\} and this is convex by a proposition we covered earlier.\nSuppose now that 𝐱*∈Ω\\bm{x}^\\ast \\in \\Omega is a relative minimum point of ff, but that there is another point 𝐲∈Ω\\bm{y} \\in \\Omega with f(𝐲)<f(𝐱*)f(\\bm{y}) < f(\\bm{x}^\\ast). On the line α𝐲+(1−α)𝐱*\\alpha \\bm{y} + (1-\\alpha)\\bm{x}^\\ast, 0<α<10 < \\alpha < 1 we have\nf(α𝐲+(1−α)𝐱*)≤αf(𝐲)+(1−α)f(𝐱*)<f(𝐱*), f(\\alpha \\bm{y} + (1-\\alpha)\\bm{x}^\\ast) \\leq \\alpha f(\\bm{y}) + (1-\\alpha)\nf(\\bm{x}^\\ast) < f(\\bm{x}^\\ast), \ncontradicting the fact that 𝐱*\\bm{x}^\\ast is a relative minimum point.\n\n\n\n\nAll minimum points of a convex function are located together and are global minima.\n\n\n\n\n\nTheorem\n\n\nLet f∈C1f \\in C^1 be convex on the convex set Ω\\Omega. If there is a point 𝐱*∈Ω\\bm{x}^\\ast \\in \\Omega such that for all 𝐲∈Ω\\bm{y} \\in \\Omega, ∇f(𝐱*)(𝐲−𝐱*)≥0\\nabla f(\\bm{x}^\\ast)(\\bm{y} - \\bm{x}^\\ast) \\geq 0, then 𝐱*\\bm{x}^\\ast is a global minimum point of ff over Ω\\Omega.\n\n\n\n\n\n\nProof\n\n\nSince 𝐲−𝐱*\\bm{y}-\\bm{x}^\\ast is a feasible direction at 𝐱*\\bm{x}^\\ast, the given condition is equivalent to the FONC. The proof of the propostion is now immediate since by an earlier proposition we have\nf(𝐲)≥f(𝐱*)+∇f(𝐱*)(𝐲−𝐱*)≥f(𝐱*). f(\\bm{y}) \\geq f(\\bm{x}^\\ast) + \\nabla f(\\bm{x}^\\ast)(\\bm{y} - \\bm{x}^\\ast)\n\\geq f(\\bm{x}^\\ast). \n\n\n\n\n\n\nTheorem\n\n\nLet ff be a convex function defined on the bounded, closed convex set Ω\\Omega. If ff has a maximum over Ω\\Omega, it is achieved at an extreme point of Ω\\Omega.\n\n\n\n\n\n\nProof – Omitted."
  },
  {
    "objectID": "06_basic_unc.html#algorithms",
    "href": "06_basic_unc.html#algorithms",
    "title": "06_basic_unc",
    "section": "Algorithms",
    "text": "Algorithms\n\nIterative: the algorithm generates a series of points, each point being calculated on the basis of the points preceding it.\nDescent: as each new point is generated by the algorithm the corresponding value of some function decreases in value.\nAn iterative algorithm is initiated by specifying a starting point.\n\nIf for arbitrary starting points, the algorithm is guaranteed to generate a sequence of points converging to a solution, then the algorithm is said to be globally convergent.\nNot all algorithms have this desirable property: indeed, many of the most important algorithms for NLP are not globally convergent.\nThey occasionally generate sequences that do not converge at all or converge to points that are not solutions."
  },
  {
    "objectID": "06_basic_unc.html#iterative-algorithms",
    "href": "06_basic_unc.html#iterative-algorithms",
    "title": "06_basic_unc",
    "section": "Iterative Algorithms",
    "text": "Iterative Algorithms\n\n\n\nWe formally define an algorithm 𝐀\\bm{A} as a mapping, 𝐀:X→X\\;\\; \\bm{A}: X \\rightarrow X. Operated iteratively, the algorithm 𝐀\\bm{A} initiated at 𝐱0∈X\\bm{x}_0 \\in X would generates a sequence {𝐱k}\\{\\bm{x}_k\\} defined by\n\n𝐱k+1=𝐀(𝐱k). \\bm{x}_{k+1} = \\bm{A}(\\bm{x}_k). \n\n\n\nDefinition (Algorithm – a generalization)\n\n\nAn algorithm 𝐀\\bm{A} is a mapping defined on a space XX that assigns to every point 𝐱∈X\\bm{x} \\in X a subset of XX. That is, the mapping 𝐀\\bm{A} is a point-to-set mapping of XX.\n\n\n\n\nAn algorithm 𝐀\\bm{A}, therefore, generates a sequence of points in the following way.\n\nGiven 𝐱k∈X\\bm{x}_k \\in X, the algorithm yields 𝐀(𝐱k)⊆X\\bm{A}(\\bm{x}_k) \\subseteq X.\nFrom this subset an arbitrary element 𝐱k+1\\bm{x}_{k+1} is selected.\nIn this way, given an initial point 𝐱0\\bm{x}_0, the algorithm generates sequences through the iteration\n\n\n𝐱k+1∈𝐀(𝐱k). \\bm{x}_{k+1} \\in \\bm{A}(\\bm{x}_k). \n\nThe utility of this more general definition is that it allows one to analyze the convergence of an infinite family of similar algorithms.\n\n\n\n\n\n\nExample\n\n\nSuppose for x∈ℝx \\in \\mathbb{R}, we define\nA(x)=[−|x|2,|x|2] A(x) = \\left[ -\\frac{|x|}{2}, \\frac{|x|}{2} \\right ] \nso that A(x)A(x) is an interval of the real line.\n\nLet us start at x0=100x_0 = 100.\nEach of the following sequences below might be generated from an iterative application of this algorithm.\n\n{100,50,25,12,−6,−2,1,12,…} \\left\\{100, 50, 25, 12, -6, -2, 1, \\frac{1}{2}, \\ldots \\right\\}  {100,−40,20,−5,−2,1,14,18,…} \\left\\{100, -40, 20, -5, -2, 1, \\frac{1}{4}, \\frac{1}{8}, \\ldots \\right\\}  {100,10,−1,116,1100,−11000,110000,…} \\left\\{100, 10, -1, \\frac{1}{16}, \\frac{1}{100}, -\\frac{1}{1000}, \\frac{1}{10000}, \\ldots \\right\\}"
  },
  {
    "objectID": "06_basic_unc.html#descent",
    "href": "06_basic_unc.html#descent",
    "title": "06_basic_unc",
    "section": "Descent",
    "text": "Descent\n\n\n\nDefinition (Descent)\n\n\nLet Γ⊆X\\Gamma \\subseteq X be a given solution set and let 𝐀\\bm{A} be an algorithm on XX. A continuous real-valued function ZZ on XX is said to be a descent function for Γ\\Gamma and 𝐀\\bm{A} if it satisfies\n\nif 𝐱∉Γ\\bm{x} \\notin \\Gamma and 𝐲∈𝐀(𝐱)\\bm{y} \\in \\bm{A}(\\bm{x}), then Z(𝐲)<Z(𝐱)Z(\\bm{y}) < Z(\\bm{x}),\nif 𝐱∈Γ\\bm{x} \\in \\Gamma and 𝐲∈𝐀(𝐱)\\bm{y} \\in \\bm{A}(\\bm{x}), then Z(𝐲)≤Z(𝐱)Z(\\bm{y}) \\leq Z(\\bm{x}).\n\n\n\n\n\nThere are a number of ways a solution set, algorithm, and descent function can be defined.\nA natural set-up for the problem\n\nminimizef(𝐱)subject to𝐱∈Ω\n\\begin{align}\n\\operatorname{minimize} & f(\\bm{x}) \\\\\n\\text{subject to} & \\bm{x} \\in \\Omega\n\\end{align}\n\n\nΓ\\Gamma be the set of minimizing points; define an algorithm 𝐀\\bm{A} on Ω\\Omega in such a way that ff decreases at each step (descent function).\nAnother possibility: Γ\\Gamma is the set of points 𝐱\\bm{x} satisfying ∇f(𝐱)=0\\nabla f(\\bm{x}) = 0. |∇f(𝐱)||\\nabla f(\\bm{x})| might then serve as a descent function."
  },
  {
    "objectID": "06_basic_unc.html#global-convergence-theorem",
    "href": "06_basic_unc.html#global-convergence-theorem",
    "title": "06_basic_unc",
    "section": "Global Convergence Theorem",
    "text": "Global Convergence Theorem\n\n\n\n\n\nThe set-up\n\n\n\nThere is a solution set Γ\\Gamma\nPoints are generated according to 𝐱k+1∈𝐀(𝐱k)\\bm{x}_{k+1} \\in \\bm{A}(\\bm{x}_k)\nEach new point strictly decreases a descent function 𝐙\\bm{Z} unless the solution set Γ\\Gamma is reached.\n\n\n\n\n\n\n\nGlobal Convergence Theorem\n\n\nLet 𝐀\\bm{A} be an algorithm on XX, and suppose that, given 𝐱0\\bm{x}_0, the sequence {𝐱k}k=0∞\\left\\{\\bm{x}_k\\right\\}_{k=0}^\\infty is generated satisfying\n𝐱k+1∈𝐀(𝐱k). \\bm{x}_{k+1} \\in \\bm{A}(\\bm{x}_k). \nLet a solution set Γ⊆X\\Gamma \\subseteq X be given, and suppose\n\nall points 𝐱k\\bm{x}_k are contained in a compact set S⊆XS \\subseteq X,\nthere is a continuous function 𝐙\\bm{Z} on XX such that\n\nif 𝐱∉Γ\\bm{x} \\notin \\Gamma and 𝐲∈𝐀(𝐱)\\bm{y} \\in \\bm{A}(\\bm{x}), then Z(𝐲)<Z(𝐱)Z(\\bm{y}) < Z(\\bm{x}),\nif 𝐱∈Γ\\bm{x} \\in \\Gamma and 𝐲∈𝐀(𝐱)\\bm{y} \\in \\bm{A}(\\bm{x}), then Z(𝐲)≤Z(𝐱)Z(\\bm{y}) \\leq Z(\\bm{x}).\n\nthe mapping 𝐀\\bm{A} is closed at points outside Γ\\Gamma.\n\nThen the limit of any convergent subsequence of {𝐱k}\\{\\bm{x}_k\\} is a solution.\n\n\n\n\n\n\n\n\nExample\n\n\n\n\nConsider the point-to-set algorithm 𝐀\\bm{A} defined by the graph in the figure on the right.\nThis is given explicitly on X=[0,1]X = [0, 1] by\n𝐀(x)={[0,x)n0<x≤10x=0, \n\\bm{A}(x) = \n\\begin{cases}\n  [0, x)  & n 0 < x \\leq 1 \\\\\n  0 & x = 0,\n\\end{cases}\n\nLetting Γ={0}\\Gamma = \\{0\\}, the function Z(x)=xZ(x) = x serves as a descent function because for x≠0x \\neq 0 all points in 𝐀(x)\\bm{A}(x) are less than xx.\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptimization Theory and Practice • Aykut C. Satici"
  }
]