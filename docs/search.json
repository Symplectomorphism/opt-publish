[
  {
    "objectID": "07_basic_descent.html#optimization-theory-and-practice",
    "href": "07_basic_descent.html#optimization-theory-and-practice",
    "title": "07_basic_descent",
    "section": "Optimization Theory and Practice",
    "text": "Optimization Theory and Practice\n\n\nBasic Descent Methods\n\n\n\n\nInstructor: Aykut Satici, Ph.D.Â   Mechanical and Biomedical Engineering  Electrical and Computer Engineering  Boise State University, Boise, ID, USA\n\n\nTopics:  Line Search Algorithms  The Method of Steepest Descent  Newtonâ€™s Method: Second-Order   Coordinate and Stochastic Gradient Descent"
  },
  {
    "objectID": "07_basic_descent.html#th-order-method-golden-search",
    "href": "07_basic_descent.html#th-order-method-golden-search",
    "title": "07_basic_descent",
    "section": "00th-Order Method: Golden Search",
    "text": "00th-Order Method: Golden Search\n\n\n\n\n\nIf we evaluate ff at only one intermediate point of the interval, we cannot narrow the range within which we know the minimizer is located.\n\n\n\n\n\n\n\n\n\nWe have to evaluate ff at two intermediate points.\n\nWe choose the intermediate points in such a way that the reduction in the range is symmetric, i.e., a1âˆ’a0=b0âˆ’b1=Ï(b0âˆ’a0),Ï&lt;12. a_1 - a_0 = b_0 - b_1 = \\rho(b_0 - a_0), \\qquad \\rho &lt; \\frac{1}{2}. \nWe then evaluate ff at the intermediate points.\n\nIf f(a1)&lt;f(b1)f(a_1) &lt; f(b_1) then the minimizer must lie in the range [a0,b1][a_0, b_1].\nIf, on the other hand f(a1)â‰¥f(b1)f(a_1) \\geq f(b_1), then the minimizer is located at [a1,b0][a_1, b_0].\n\n\n\n\n\n\n\n\n\n\n\n\n\nStarting with the reduced range of uncertainty, we can repeat the process to find two new points: a2a_2 and b2b_2.\nBut this is unnecessary: we know x*âˆˆ[a0,b1]x^\\ast \\in [a_0, b_1]\n\nSince a1âˆˆ[a0,b1]a_1 \\in [a_0, b_1], we can set a1=b2a_1 = b_2.\nOnly one function evaluation of ff at a2a_2 is necessary.\n\nHow do we find the value of Ï\\rho that results only in one new evaluation of ff?"
  },
  {
    "objectID": "07_basic_descent.html#th-order-method-golden-search-1",
    "href": "07_basic_descent.html#th-order-method-golden-search-1",
    "title": "07_basic_descent",
    "section": "00th-Order Method: Golden Search",
    "text": "00th-Order Method: Golden Search\n\n\n\n\n\n\n\n\nFinding Ï\\rho\n\n\nWe choose Ï\\rho so that: 2345Ï(b1âˆ’a0)=b1âˆ’b2=1âˆ’2Ï\\phantom{2345} \\rho (b_1 - a_0) = b_1 - b_2 = 1-2\\rho.\nThis has the two solutions: 123Ï1,2=12(3Â±5)\\phantom{123} \\rho_{1,2} = \\frac{1}{2}(3 \\pm \\sqrt{5}).\n\nSince we require that Ï&lt;12\\rho &lt; \\frac{1}{2}, we must take Ï=3âˆ’52â‰ˆ0.382.\\rho =\n\\frac{3-\\sqrt{5}}{2} \\approx 0.382. \nObserve that 1âˆ’Ï=5âˆ’12â‰ˆ0.618031-\\rho = \\frac{\\sqrt{5}-1}{2} \\approx 0.61803 and Ï1âˆ’Ï=1âˆ’Ï1\\frac{\\rho}{1-\\rho} = \\frac{1-\\rho}{1}.\n\nDividing a range in the ratio of Ï\\rho to 1âˆ’Ï1-\\rho has the effect that ratio of the shorter segment to the longer equals the ratio of the longer to the sum of the two.\nThis rule is referred to as the golden section.\n\nThe uncertainty range reduction is 1âˆ’Ï1-\\rho at each stage.\n\nNN steps of the method reduces the range by factor (1âˆ’Ï)N(1-\\rho)^N.\n\n\n\n\n\n\n\n\n\nExample: locate xx to within the range 310\\frac{3}{10}\n\n\nf(x)=x4âˆ’14x3+60x2âˆ’70x,xâˆˆ[0,2]. f(x) = x^4 - 14x^3 + 60x^2 - 70x, \\quad x \\in [0, 2]. \n\n(1âˆ’Ï)Nâ‰¤12310â‡’N&gt;3.94(1-\\rho)^N \\leq \\frac{1}{2}\\frac{3}{10} \\; \\Rightarrow \\; N &gt; 3.94 so N=4N=4.\n\nIteration 1. Evaluate ff at two intermediate points. We have a1=a0+Ï(b0âˆ’a0)=0.7639,f(a1)=âˆ’24.36,b1=a0+(1âˆ’Ï)(b0âˆ’a0)=1.236f(b1)=âˆ’18.96. \n\\begin{align}\na_1 &= a_0 + \\rho(b_0 - a_0) = 0.7639, & f(a_1) = -24.36, \\\\ \nb_1 &= a_0 + (1-\\rho)(b_0 - a_0) = 1.236 & f(b_1) = -18.96. \n\\end{align}\n Since f(a1)&lt;f(b1)f(a_1) &lt; f(b_1), the uncertainty interval is reduced to [a0,b1]=[0,1.236]. [a_0, b_1] = [0, 1.236]. \nIteration 2. We choose b2=a1b_2 = a_1 and so the new point is a2=a0+Ï(b1âˆ’a0)=0.4721,f(a2)=âˆ’21.10,b2=a1=0.7639,f(b2)=âˆ’24.36.\n\\begin{align}\na_2 &= a_0 + \\rho(b_1 - a_0) = 0.4721, & f(a_2) = -21.10, \\\\\nb_2 &= a_1 = 0.7639, & f(b_2) = -24.36.\n\\end{align}\n Since f(a2)&gt;f(b2)f(a_2) &gt; f(b_2) the uncertainty interval is [a2,b1]=[0.4721,1.236]. [a_2, b_1] = [0.4721, 1.236]. \nIteration 3. We choose a3=b2a_3 = b_2 and continue â€¦\nIteration 4. Final iteration results in [a4,b3]=[0.6525,0.9443]. [a_4, b_3] = [0.6525, 0.9443]."
  },
  {
    "objectID": "07_basic_descent.html#st-order-method-bisection-method",
    "href": "07_basic_descent.html#st-order-method-bisection-method",
    "title": "07_basic_descent",
    "section": "11st-Order Method: Bisection Method",
    "text": "11st-Order Method: Bisection Method\n\n\n\nAssumptions: ff is unimodal and continuously differentiable.\n\n\n\n\n\n\n\n\nAlgorithm\n\n\n\nLet x0=12(a0+b0)x_0 = \\frac{1}{2}(a_0 + b_0), the midpoint of the initial uncertainty interval.\nEvaluate fâ€²(x0)f'(x_0).\n\nIf fâ€²(x0)&gt;0f'(x_0) &gt; 0, deduce that the minimizer lies to the left of x0x_0.\n\nReduce the uncertainty interval to [a0,x0][a_0, x_0].\n\nIf fâ€²(x0)&lt;0f'(x_0) &lt; 0, deduce that the minimizer lies to the right of x0x_0.\n\nReduce the uncertainty interval to [x0,b0][x_0, b_0].\n\nIf fâ€²(x0)=0f'(x_0) = 0, declare x0x_0 the minimizer and terminate the search.\n\nWith the new uncertainty interval computed, repeat the process iteratively.\n\nCompute the midpoint xkx_k and check the sign of fâ€²(xk)f'(x_k) and reduce the uncertainty to the left or right of xkx_k.\nDeclare xkx_k the minimizer if fâ€²(xk)=0f'(x_k) = 0.\n\n\n\n\n\n\n\n\n\nSalient features\n\n\n\nInstead of using the values of ff, the bisection method uses the values of fâ€²f'.\nAt each iteration, the length of the uncertaintly interval is reduced by a factor of 12\\frac{1}{2}.\n\nAfter NN steps, the range is reduced by a factor of (12)N(\\frac{1}{2})^N.\nThis factor is smaller than in the golden search or Fibonacci methods.\n\n\n\n\n\n\n\n\nExample\n\n\nf(x)=x4âˆ’14x3+60x2âˆ’70x,xâˆˆ[0,2]. f(x) = x^4 - 14x^3 + 60x^2 - 70x, \\quad x \\in [0, 2]. \n\nIf we want a precision of 0.30.3, then we need N&gt;1âˆ’log2(310)N &gt; 1 - \\operatorname{log}_2(\\frac{3}{10}) iterations, i.e., N=3N = 3."
  },
  {
    "objectID": "07_basic_descent.html#nd-order-method-newtons-method",
    "href": "07_basic_descent.html#nd-order-method-newtons-method",
    "title": "07_basic_descent",
    "section": "22nd-Order Method: Newtonâ€™s Method",
    "text": "22nd-Order Method: Newtonâ€™s Method\n\n\n\n\n\n\nConstruct a quadratic function qq which at xkx_k agrees with ff up to second derivatives, that is\n\nq(x)=f(xk)+fâ€²(xk)(xâˆ’xk)+12fâ€³(xk)(xâˆ’xk)2. q(x) = f(x_k) + f'(x_k)(x-x_k) + \\frac{1}{2}f''(x_k)(x-x_k)^2. \n\nCalculate an estimate xk+1x_{k+1} of the minimum point of ff by finding the point where the derivative of qq vanishes.\n\n0=qâ€²(xk+1)=fâ€²(xk)+fâ€³(xk)(xk+1âˆ’xk)â‡’xk+1=xkâˆ’fâ€²(xk)fâ€³(xk).(1) \n\\begin{align}\n0 &= q'(x_{k+1}) = f'(x_k) + f''(x_k)(x_{k+1}-x_k) \\\\ \n&\\Rightarrow x_{k+1} = x_k - \\frac{f'(x_k)}{f''(x_k)}. \n\\end{align}\n \\qquad(1)\n\n\n\n\nxk+1x_{k+1} resulting from Newtonâ€™s method does not depend on the value f(xk)f(x_k).\n\n\n\n\n\n\n\n\n\nNewtonâ€™s method may be viewed as a technique for iteratively solving equations of the form\n\ng(x)=0, g(x) = 0, \nwhere, when applied to minimization, we put g(x)=fâ€²(x)g(x) = f'(x).\n\n\n\n\n\n\n\n\n\nProposition\n\n\nLet x*x^\\ast satisfy g(x*)=0g(x^\\ast) = 0, gâ€²(x*)â‰ 0g'(x^\\ast) \\neq 0. Then, provided x0x_0 is sufficiently close to x*x^\\ast, the sequence {xk}k=0âˆ\\{x_k\\}_{k=0}^\\infty generated by Newtonâ€™s method EquationÂ 1 converges to x*x^\\ast with an order of convergence at least two."
  },
  {
    "objectID": "07_basic_descent.html#example-newtons-method",
    "href": "07_basic_descent.html#example-newtons-method",
    "title": "07_basic_descent",
    "section": "Example â€“ Newtonâ€™s Method",
    "text": "Example â€“ Newtonâ€™s Method\n\n\n\nWe want to find the minimizer of\n\nf(x)=12x2âˆ’sinx,x0=12. f(x) = \\frac{1}{2}x^2 - \\sin{x}, \\quad x_0 = \\frac{1}{2}. \n\nWe want an accuracy of Îµ=10âˆ’5\\varepsilon = 10^{-5}, i.e., stop when\n\n|xk+1âˆ’xk|&lt;Îµ. |x_{k+1} - x_k | &lt; \\varepsilon. \n\nWe compute\n\nfâ€²(x)=xâˆ’cosx,fâ€³(x)=1+sinx. f'(x) = x - \\cos{x}, \\quad f''(x) = 1 + \\sin{x}. \n\n  x1=12âˆ’12âˆ’cos121+sin12=0.7552,x2=x1âˆ’fâ€²(x1)fâ€³(x1)=x1âˆ’0.027101.685=0.7391,x3=x2âˆ’fâ€²(x2)fâ€³(x2)=x2âˆ’9.461Ã—10âˆ’51.673=0.7390,x4=x3âˆ’fâ€²(x3)fâ€³(x3)=x3âˆ’1.17Ã—10âˆ’91.673=0.7390.\n\\begin{align}\nx_1 &= \\frac{1}{2} - \\frac{\\frac{1}{2} - \\cos{\\frac{1}{2}}}{1 +\n\\sin{\\frac{1}{2}}} = 0.7552, \\\\\nx_2 &= x_1 - \\frac{f'(x_1)}{f''(x_1)} = x_1 - \\frac{0.02710}{1.685} = 0.7391, \\\\\nx_3 &= x_2 - \\frac{f'(x_2)}{f''(x_2)} = x_2 - \\frac{9.461 \\times 10^{-5}}{1.673}\n= 0.7390, \\\\\nx_4 &= x_3 - \\frac{f'(x_3)}{f''(x_3)} = x_3 - \\frac{1.17 \\times 10^{-9}}{1.673}\n= 0.7390.\n\\end{align}"
  },
  {
    "objectID": "07_basic_descent.html#inaccurate-line-search",
    "href": "07_basic_descent.html#inaccurate-line-search",
    "title": "07_basic_descent",
    "section": "Inaccurate Line Search",
    "text": "Inaccurate Line Search\n\n\n\n\nIn practice, we do not bend over backwards to find the exact minimum when performing line search.\n\nIt is often desirable to sacrifice accuracy in the line search routine in order to conserve overall computation time.\nMost functions to not attain their minimum along the line weâ€™re searching at a particular iteration anyway!\n\nInaccuracy is introduced in a line search algorithm by simply terminating the search before it has converged.\n\n\n\n\n\n\n\n\n\nArmijoâ€™s Rule\n\n\n\nA practical and popular criterion for terminating a line search is Armijoâ€™s rule.\nThe idea is that the rule should first guarantee that the selected Î±\\alpha is not too large, and next it should not be too small. Ï•(Î±)=f(ğ±k+Î±ğk). \\phi(\\alpha) = f(\\bm{x}_k + \\alpha \\bm{d}_k). \nConsider the function Ï•(0)+ÎµÏ•â€²(0)Î±\\phi(0) + \\varepsilon \\phi'(0)\\alpha, for fixed Îµ\\varepsilon, 0&lt;Îµ&lt;10 &lt; \\varepsilon &lt; 1.\nA value of Î±\\alpha is consered to be not too large if the corresponding function value lies below the dashed line; that is, if Ï•(Î±)â‰¤Ï•(0)+ÎµÏ•â€²(0)Î±.(2) \\phi(\\alpha) \\leq \\phi(0) + \\varepsilon \\phi'(0)\\alpha.  \\qquad(2)\nTo ensure that Î±\\alpha is not too small, a value Î·&gt;1\\eta &gt; 1 is selected, and Î±\\alpha is then considered to be not too small if Ï•(Î·Î±)&gt;Ï•(0)+ÎµÏ•â€²(0)Î·Î±. \\phi(\\eta \\alpha) &gt; \\phi(0) + \\varepsilon \\phi'(0)\\eta \\alpha. \nThis means that if Î±\\alpha is increased by a factor Î±\\alpha, it will fail to meet the test EquationÂ 2.\n\n\n\n\n\n\n\n\n\n\n\nThe acceptable region defined by the Armijo rule when Î·=2\\eta = 2."
  },
  {
    "objectID": "07_basic_descent.html#the-method-and-convergence",
    "href": "07_basic_descent.html#the-method-and-convergence",
    "title": "07_basic_descent",
    "section": "The Method and Convergence",
    "text": "The Method and Convergence\n\n\n\n\n\n\nThe gradient âˆ‡f(ğ±)\\nabla f(\\bm{x}) is defined as a nn-dim. row vector.\nWe define the nn-dim. column vector g(ğ±)=âˆ‡f(ğ±)âŠ¤g(\\bm{x}) = \\nabla f(\\bm{x})^\\top.\n\n\n\n\n\n\n\nThe Method of Steepest Descent (SDM)\n\n\nThe iterative algorithm is\nğ±k+1=ğ±kâˆ’Î±kğ k, \\bm{x}_{k+1} = \\bm{x}_k - \\alpha_k \\bm{g}_k, \nwhere the stepsize Î±k\\alpha_k is a nonnegative scalar possible minimizing f(ğ±kâˆ’Î±ğ k)f(\\bm{x}_k - \\alpha \\bm{g}_k).\n\n\n\n\n\n\nThe Algorithm\n\n\nDefine the mapping 1ğ’:â„2nâ†’â„n\\phantom{1} \\bm{S}: \\mathbb{R}^{2n} \\rightarrow \\mathbb{R}^n by ğ’(ğ±,ğ)={ğ²:ğ²=ğ±+Î±ğ,Î±â‰¥0,f(ğ²)=min0â‰¤Î±&lt;âˆf(ğ±+Î±ğ)}. \\bm{S}(\\bm{x}, \\bm{d}) = \\{\\bm{y}: \\bm{y} = \\bm{x} + \\alpha\n\\bm{d}, \\;\\; \\alpha \\geq 0, \\; f(\\bm{y}) =\n\\operatorname{min}_{0 \\leq \\alpha &lt; \\infty} f(\\bm{x} + \\alpha \\bm{d}) \\}. \nThis is a closed map if ğâ‰ ğŸ\\bm{d} \\neq \\bm{0}.\nThe overall algorithm is ğ€:â„nâ†’â„n\\; \\bm{A}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n which gives ğ±k+1âˆˆğ€(ğ±k)\\bm{x}_{k+1} \\in \\bm{A}(\\bm{x}_k) can be decomposed in the form\nğ€=ğ’ğ†,ğ†(ğ±)=(ğ±,âˆ’ğ (ğ±)). \\bm{A} = \\bm{SG}, \\quad \\bm{G}(\\bm{x}) = (\\bm{x}, -\\bm{g}(\\bm{x})). \n\n\n\n\n\n\n\nGlobal Convergence\n\n\n\nDefine the solution set Î“={ğ±âˆˆâ„n:âˆ‡f(ğ±)=ğŸ}\\Gamma = \\{\\bm{x} \\in \\mathbb{R}^n: \\nabla f(\\bm{x}) = \\bm{0}\\}.\nZ(ğ±)=f(ğ±)Z(\\bm{x}) = f(\\bm{x}) is a descent function for ğ€\\bm{A}, since for âˆ‡f(ğ±)â‰ ğŸ\\nabla f(\\bm{x}) \\neq \\bm{0} min0â‰¤Î±&lt;âˆf(ğ±âˆ’Î±ğ (ğ±))&lt;f(ğ±). \\operatorname{min}_{0 \\leq \\alpha &lt; \\infty} f(\\bm{x} - \\alpha \\bm{g}(\\bm{x}))\n&lt; f(\\bm{x}). \nThus by the Global Convergence Theorem, if the sequence {ğ±k}\\{\\bm{x}_k\\} is bounded, it will have limit points and each of these is a solution.\n\n\n\n\n\n\n\nDefinition (First-order Î²\\beta-Lipschitz Function)\n\n\nFor any two points ğ±\\bm{x} and ğ²\\bm{y}, |âˆ‡f(ğ²)âˆ’âˆ‡f(ğ±)|â‰¤Î²|ğ²âˆ’ğ±||\\nabla f(\\bm{y}) - \\nabla f(\\bm{x})| \\leq \\beta |\\bm{y} - \\bm{x}|, for a positive real number Î²\\beta.\n\n\n\n\n\n\nConvergence Speed for fixed stepsize Î±k=1Î²\\alpha_k = \\frac{1}{\\beta} is arithmetic.\n\n\nWe may not know 1Î²\\frac{1}{\\beta} so we employ a backtracking line search: start with a guess of Î²\\beta; if sufficient obj. reduction is achieved, halve Î²\\beta; otherwise double Î²\\beta. Stop when the process is reversed."
  },
  {
    "objectID": "07_basic_descent.html#steepest-descent-analysis",
    "href": "07_basic_descent.html#steepest-descent-analysis",
    "title": "07_basic_descent",
    "section": "Steepest Descent Analysis",
    "text": "Steepest Descent Analysis\n\n\n\n\n\nLemma. Let f(ğ±)f(\\bm{x}) be differentiable everywhere and satisfy the (first-order) Î²\\beta-Lipschitz condition. Then, for any two points ğ±\\bm{x} and ğ²\\bm{y} f(ğ²)âˆ’f(ğ±)âˆ’âˆ‡f(ğ±)(ğ²âˆ’ğ±)â‰¤Î²2|ğ²âˆ’ğ±|2. f(\\bm{y}) - f(\\bm{x}) - \\nabla f(\\bm{x})(\\bm{y} - \\bm{x}) \\leq\n\\frac{\\beta}{2}|\\bm{y} - \\bm{x}|^2. \n\n\n\n\n\n\nSteepest Descent â€” Lipschitz Convex Case.\n\n\nLet f(ğ±)f(\\bm{x}) be convex and differentiable everywhere, satisfy the (first-order) Î²\\beta-Lipschitz condition, and admit a minimizer ğ±*\\bm{x}^\\ast. Then, the method of steepest descent ğ±k+1=ğ±kâˆ’1Î²ğ k=ğ±kâˆ’1Î²âˆ‡f(ğ±k)âŠ¤ \\bm{x}_{k+1} = \\bm{x}_k - \\frac{1}{\\beta}\\bm{g}_k = \\bm{x}_k -\n\\frac{1}{\\beta}\\nabla f(\\bm{x}_k)^\\top  generates a sequence of solutions ğ±k\\bm{x}_k such that |âˆ‡f(ğ±k)|â‰¤Î²k(k+1)|ğ±0âˆ’ğ±*|, |\\nabla f(\\bm{x}_k)| \\leq \\frac{\\beta}{\\sqrt{k(k+1)}}|\\bm{x}_0 -\n\\bm{x}^\\ast|,  and f(ğ±k)âˆ’f*â‰¤Î²2(k+1)|ğ±0âˆ’ğ±*|2. f(\\bm{x}_k) - f^\\ast \\leq \\frac{\\beta}{2(k+1)}|\\bm{x}_0 - \\bm{x}^\\ast|^2. \n\n\n\n\n\n\nProof\n\n\nConsider the function gğ±(ğ²)=f(ğ²)âˆ’âˆ‡f(ğ±)ğ²g_\\bm{x}(\\bm{y}) = f(\\bm{y}) - \\nabla f(\\bm{x})\\bm{y} for any given ğ±\\bm{x}.\n\n\n\n\n\n\n\nNote that gğ±g_\\bm{x} is convex and satisfies the Î²\\beta-Lipschitz condition. Moreover ğ±\\bm{x} is the minimizer of gğ±(ğ²)g_\\bm{x}(\\bm{y}) and âˆ‡gğ±(ğ²)=âˆ‡f(ğ²)âˆ’âˆ‡f(ğ±)\\nabla g_\\bm{x}(\\bm{y}) = \\nabla f(\\bm{y}) - \\nabla f(\\bm{x}).\nApplying the Lemma to gğ±g_\\bm{x} and noting the relations of gğ±g_\\bm{x} and f(ğ±)f(\\bm{x}), we have\nf(ğ±)âˆ’f(ğ²)âˆ’âˆ‡f(ğ±)(ğ±âˆ’ğ²)=gğ±(ğ±)âˆ’gğ±(ğ²)123â‰¤gğ±(ğ²âˆ’1Î²âˆ‡gğ±(ğ²)âŠ¤)âˆ’gğ±(ğ²)123â‰¤âˆ‡gğ±(ğ²)(âˆ’1Î²âˆ‡gğ±(ğ²)âŠ¤)+Î²21Î²2|âˆ‡gğ±(ğ²)|2123=âˆ’12Î²|âˆ‡gğ±(ğ²)|2=âˆ’12Î²|âˆ‡f(ğ±)âˆ’âˆ‡f(ğ²)|2.(3)\n\\begin{align}\n&f(\\bm{x}) - f(\\bm{y}) - \\nabla f(\\bm{x})(\\bm{x} - \\bm{y}) = g_\\bm{x}(\\bm{x}) -\ng_\\bm{x}(\\bm{y}) \\\\\n&\\phantom{123} \\leq g_\\bm{x}(\\bm{y} - \\frac{1}{\\beta}\\nabla\ng_\\bm{x}(\\bm{y})^\\top) -\ng_\\bm{x}(\\bm{y}) \\\\\n&\\phantom{123}\\leq \\nabla g_{\\bm{x}}(\\bm{y})(-\\frac{1}{\\beta}\\nabla g_\\bm{x}(\\bm{y})^\\top) +\n\\frac{\\beta}{2}\\frac{1}{\\beta^2}|\\nabla g_\\bm{x}(\\bm{y})|^2 \\\\\n&\\phantom{123} = -\\frac{1}{2\\beta}|\\nabla g_{\\bm{x}}(\\bm{y})|^2 = -\\frac{1}{2\\beta}|\\nabla\nf(\\bm{x}) - \\nabla f(\\bm{y})|^2.\n\\end{align}\n \\qquad(3)\nSimilarly, we have\nf(ğ²)âˆ’f(ğ±)âˆ’âˆ‡f(ğ²)(ğ²âˆ’ğ±)â‰¤âˆ’12Î²|âˆ‡f(ğ±)âˆ’âˆ‡f(ğ²)|2. f(\\bm{y}) - f(\\bm{x}) - \\nabla f(\\bm{y})(\\bm{y} - \\bm{x}) \\leq\n-\\frac{1}{2\\beta}|\\nabla f(\\bm{x}) - \\nabla f(\\bm{y})|^2. \nAdding the above two derived inequalities, we have for any ğ±\\bm{x} and ğ²\\bm{y}\n(âˆ‡f(ğ±)âˆ’âˆ‡f(ğ²))(ğ±âˆ’ğ²)â‰¥1Î²|âˆ‡f(ğ±)âˆ’âˆ‡f(ğ²)|2.(4)\n(\\nabla f(\\bm{x}) - \\nabla f(\\bm{y}))(\\bm{x} - \\bm{y}) \\geq\n\\frac{1}{\\beta}|\\nabla f(\\bm{x}) - \\nabla f(\\bm{y})|^2.\n \\qquad(4)\nFor simplicity, let ğk=ğ±kâˆ’ğ±*\\bm{d}_k = \\bm{x}_k - \\bm{x}^\\ast and Î´k=f(ğ±k)âˆ’f(ğ±*)â‰¥0\\delta_k = f(\\bm{x}_k) - f(\\bm{x}^\\ast) \\geq 0."
  },
  {
    "objectID": "07_basic_descent.html#steepest-descent-analysis-1",
    "href": "07_basic_descent.html#steepest-descent-analysis-1",
    "title": "07_basic_descent",
    "section": "Steepest Descent Analysis",
    "text": "Steepest Descent Analysis\n\n\n\n\n\nProof â€“ Continued â€“\n\n\nNow let ğ±=ğ±k+1\\bm{x} = \\bm{x}_{k+1} and ğ²=ğ±k\\bm{y} = \\bm{x}_k in EquationÂ 4. Then\n âˆ’1Î²ğ kâŠ¤(ğ k+1âˆ’ğ k)=(ğ±k+1âˆ’ğ±k)âŠ¤(ğ k+1âˆ’ğ k)â‰¥1Î²|ğ k+1âˆ’ğ k|2, -\\frac{1}{\\beta} \\bm{g}_k^\\top (\\bm{g}_{k+1} - \\bm{g}_k) = (\\bm{x}_{k+1} -\n\\bm{x}_k)^\\top (\\bm{g}_{k+1} - \\bm{g}_k) \\geq \\frac{1}{\\beta}|\\bm{g}_{k+1} -\n\\bm{g}_k|^2,  \nwhich leads to\n|ğ k+1|2â‰¤ğ k+1âŠ¤ğ kâ‰¤|ğ k+1||ğ k|,i.e.,|ğ k+1|â‰¤|ğ k|.(5)\n|\\bm{g}_{k+1}|^2 \\leq \\bm{g}_{k+1}^\\top \\bm{g}_k \\leq |\\bm{g}_{k+1}||\\bm{g}_k|,\n\\;\\; \\text{i.e.,} \\;\\; |\\bm{g}_{k+1}| \\leq |\\bm{g}_k|.\n \\qquad(5)\nThis inequality implies that |ğ k|=|âˆ‡f(ğ±k)||\\bm{g}_k| = |\\nabla f(\\bm{x}_k)| is monotonically decreasing.\nApplying inequality EquationÂ 3 for ğ±=ğ±k\\bm{x} = \\bm{x}_k and ğ²=ğ±*\\bm{y} = \\bm{x}^\\ast and noting ğ *=ğŸ\\bm{g}^\\ast = \\bm{0} we have\nÎ´kâ‰¤ğ kâŠ¤ğkâˆ’12Î²|ğ k|2=âˆ’Î²(ğ±k+1âˆ’ğ±k)ğkâˆ’Î²2|ğ±k+1âˆ’ğ±k|2=âˆ’Î²2(|ğ±k+1âˆ’ğ±k|2+2(ğ±k+1âˆ’ğ±k)âŠ¤ğk)=âˆ’Î²2(|ğk+1âˆ’ğk|2+2(ğk+1âˆ’ğk)âŠ¤ğk)=Î²2(|ğk|2âˆ’|ğk+1|2).(6)\n\\begin{align}\n\\delta_k &\\leq \\bm{g}_k^\\top \\bm{d}_k - \\frac{1}{2\\beta}|\\bm{g}_k|^2 \\\\\n&= -\\beta(\\bm{x}_{k+1} - \\bm{x}_k)\\bm{d}_k - \\frac{\\beta}{2}|\\bm{x}_{k+1} -\n\\bm{x}_k|^2 \\\\\n&= -\\frac{\\beta}{2}(|\\bm{x}_{k+1} - \\bm{x}_k|^2 + 2(\\bm{x}_{k+1} -\n\\bm{x}_k)^\\top \\bm{d}_k) \\\\\n&= -\\frac{\\beta}{2}(|\\bm{d}_{k+1} - \\bm{d}_k|^2 + 2(\\bm{d}_{k+1} - \\bm{d}_k)^\\top\n\\bm{d}_k) \\\\\n&= \\frac{\\beta}{2}(|\\bm{d}_k|^2 - |\\bm{d}_{k+1}|^2).\n\\end{align}\n \\qquad(6)\n\n\n\n\n\n\n\nSumming up EquationÂ 6 from 00 to kk, we have\nâˆ‘l=0kÎ´lâ‰¤Î²2(|ğ0|2âˆ’|ğk+1|2)â‰¤Î²2|ğ0|2.(7)\n\\sum_{l=0}^k \\delta_l \\leq \\frac{\\beta}{2}(|\\bm{d}_0|^2 - |\\bm{d}_{k+1}|^2) \\leq\n\\frac{\\beta}{2}|\\bm{d}_0|^2.\n \\qquad(7)\nUsing EquationÂ 3 again for ğ±=ğ±k+1\\bm{x} = \\bm{x}_{k+1} and ğ²=ğ±k\\bm{y} = \\bm{x}_k and noting the SD rule we have\nÎ´k+1âˆ’Î´k=f(ğ±k+1)âˆ’f(ğ±k)â‰¤ğ k+1âŠ¤(âˆ’1Î²ğ k)âˆ’12Î²|ğ k+1âˆ’ğ k|2=âˆ’12Î²(|ğ k+1|2+|ğ k|2).(8)\n\\begin{align}\n\\delta_{k+1} - \\delta_k &= f(\\bm{x}_{k+1}) - f(\\bm{x}_k) \\\\\n&\\leq \\bm{g}_{k+1}^\\top (-\\frac{1}{\\beta}\\bm{g}_k) -\n\\frac{1}{2\\beta}|\\bm{g}_{k+1} - \\bm{g}_k|^2 \\\\\n&= -\\frac{1}{2\\beta}(|\\bm{g}_{k+1}|^2 + |\\bm{g}_k|^2).\n\\end{align}\n \\qquad(8)\nNoting that EquationÂ 8 holds for all kk, we have\n âˆ‘l=0kÎ´l=âˆ‘l=0kÎ´l(l+1âˆ’l)=âˆ‘l=0kÎ´l(l+1)âˆ’âˆ‘l=0kÎ´ll=âˆ‘l=1k+1Î´lâˆ’1lâˆ’âˆ‘l=1kÎ´ll=Î´k(k+1)+âˆ‘l=1k(Î´lâˆ’1âˆ’Î´l)lâ‰¥Î´k(k+1)+âˆ‘l=1kl2Î²(|ğ l2|+|ğ lâˆ’1|2)â‰¥Î´k(k+1)+k(k+1)2Î²|ğ k|2,\n\\begin{align}\n\\sum_{l=0}^k \\delta_l &= \\sum_{l=0}^k \\delta_l (l + 1 - l) = \\sum_{l=0}^k\n\\delta_l(l+1) - \\sum_{l=0}^k \\delta_l l \\\\\n&= \\sum_{l=1}^{k+1} \\delta_{l-1}l - \\sum_{l=1}^k \\delta_ll = \\delta_k (k+1) +\n\\sum_{l=1}^k (\\delta_{l-1} - \\delta_l)l \\\\\n&\\geq \\delta_k(k+1) + \\sum_{l=1}^k \\frac{l}{2\\beta}(|\\bm{g}_l^2| + |\\bm{g}_{l-1}|^2) \\\\\n&\\geq \\delta_k (k+1) + \\frac{k(k+1)}{2\\beta}|\\bm{g}_k|^2,\n\\end{align}\n \nwhere the last inequality comes from the fact that |ğ k|=|âˆ‡f(ğ±k)||\\bm{g}_k| = |\\nabla f(\\bm{x}_k)| is monotonically decreasing."
  },
  {
    "objectID": "07_basic_descent.html#steepest-descent-analysis-2",
    "href": "07_basic_descent.html#steepest-descent-analysis-2",
    "title": "07_basic_descent",
    "section": "Steepest Descent Analysis",
    "text": "Steepest Descent Analysis\n\n\n\nProof â€“ Continued â€“\n\n\nUsing EquationÂ 7 we finally have\n(k+1)Î´k+k(k+1)2Î²|ğ k|2â‰¤Î²2|ğ0|2(9)\n(k+1)\\delta_k + \\frac{k(k+1)}{2\\beta} |\\bm{g}_k|^2 \\leq\n\\frac{\\beta}{2}|\\bm{d}_0|^2\n \\qquad(9)\nInequality EquationÂ 9, from Î´k=f(ğ±k)âˆ’f(ğ±*)â‰¥0\\delta_k = f(\\bm{x}_k) - f(\\bm{x}^\\ast) \\geq 0 and ğ0=ğ±0âˆ’ğ±*\\bm{d}_0 = \\bm{x}_0 - \\bm{x}^\\ast, proves the desired bounds."
  },
  {
    "objectID": "07_basic_descent.html#the-quadratic-case",
    "href": "07_basic_descent.html#the-quadratic-case",
    "title": "07_basic_descent",
    "section": "The Quadratic Case",
    "text": "The Quadratic Case\n\n\n\nWhen f(ğ±)f(\\bm{x}) is strongly convex, the convergence speed can be increased from arithmetic to geometric or linear convergence.\n\n\n\n\n\n\n\n\nWe focus on the quadratic problem\nf(ğ±)=12ğ±âŠ¤ğğ±âˆ’ğ±âŠ¤ğ›,ğâ‰»ğŸ.(10) f(\\bm{x}) = \\frac{1}{2}\\bm{x}^\\top \\bm{Q} \\bm{x} - \\bm{x}^\\top \\bm{b}, \\;\\;\n\\bm{Q} \\succ \\bm{0}.  \\qquad(10)\n\nThe unique minimum of ff can be found directly by setting the gradient to zero: ğğ±*=ğ›. \\bm{Q}\\bm{x}^\\ast = \\bm{b}. \nLetâ€™s introduce the function E(ğ±)=12(ğ±âˆ’ğ±*)ğ(ğ±âˆ’ğ±*), E(\\bm{x}) = \\frac{1}{2}(\\bm{x} -\n\\bm{x}^\\ast)\\bm{Q}(\\bm{x} - \\bm{x}^\\ast),  so that we have E(ğ±)=f(ğ±)+12ğ±*ğğ±*E(\\bm{x}) = f(\\bm{x}) + \\frac{1}{2}\\bm{x}^\\ast \\bm{Q} \\bm{x}^\\ast. Hence minfâ‡”minE\\operatorname{min} f \\; \\Leftrightarrow \\; \\operatorname{min} E.\nThe method of SD can be expressed as ğ±k+1=ğ±kâˆ’Î±kğ k\\bm{x}_{k+1} = \\bm{x}_k - \\alpha_k \\bm{g}_k, where ğ k=ğğ±kâˆ’ğ›\\bm{g}_k = \\bm{Q}\\bm{x}_k - \\bm{b} and Î±k\\alpha_k minimizes f(ğ±kâˆ’Î±ğ k)f(\\bm{x}_k - \\alpha \\bm{g}_k).\nWe have by definition EquationÂ 10 f(ğ±kâˆ’Î±ğ k)=12(ğ±kâˆ’Î±ğ k)âŠ¤ğ(ğ±kâˆ’Î±ğ k)âˆ’(ğ±kâˆ’Î±ğ k)âŠ¤ğ›. f(\\bm{x}_k -\\alpha \\bm{g}_k) = \\frac{1}{2}(\\bm{x}_k - \\alpha \\bm{g}_k)^\\top\n\\bm{Q} (\\bm{x}_k - \\alpha \\bm{g}_k) - (\\bm{x}_k - \\alpha \\bm{g}_k)^\\top \\bm{b}.\n\n\nThis is minimized at Î±k=ğ kâŠ¤ğ kğ kâŠ¤ğğ k. \\alpha_k = \\frac{\\bm{g}_k^\\top \\bm{g}_k}{\\bm{g}_k^\\top\n\\bm{Q} \\bm{g}_k}. \n\n\n\n\n\n\n\n\n\n\n\nThe function ff and the SD process is illustrated by showing the contours of constant values of ff and a typical sequence developed by the process.\nThe contours of ff are nn-dim. ellipsoids with axes in the directions of the nn-mutually orthogonal eigenvectors of ğ\\bm{Q}.\nThe axis corresponding to the ithi^{\\text{th}} eigenvector has length proportional to 1Î»i\\frac{1}{\\lambda_i}."
  },
  {
    "objectID": "07_basic_descent.html#nonquadratic-case",
    "href": "07_basic_descent.html#nonquadratic-case",
    "title": "07_basic_descent",
    "section": "Nonquadratic Case",
    "text": "Nonquadratic Case\n\n\n\nFor nonquadratic functions, SD still does well if the condition number is modest. To establish estimates, assume that the Hessian matrix is positive definite: Î±ğˆâ‰¤ğ…(ğ±â€¾)â‰¤Ağˆ\\alpha \\bm{I} \\leq \\bm{F}(\\bar{\\bm{x}}) \\leq A\\bm{I}.\n\n\n\n\n\n\n\n\nExact Line Search\n\n\nf(ğ±kâˆ’Î±g(ğ±k))â‰¤f(ğ±k)âˆ’Î±ğ (ğ±k)âŠ¤ğ (ğ±k)+AÎ±22ğ (ğ±k)âŠ¤ğ (ğ±k). f(\\bm{x}_k - \\alpha g(\\bm{x}_k)) \\leq f(\\bm{x}_k) - \\alpha\n\\bm{g}(\\bm{x}_k)^\\top \\bm{g}(\\bm{x}_k) +\n\\frac{A\\alpha^2}{2}\\bm{g}(\\bm{x}_k)^\\top \\bm{g}(\\bm{x}_k). \nMinimizing both sides w.r.t. Î±\\alpha yields f(ğ±k+1)â‰¤f(ğ±k)âˆ’12A|ğ (ğ±k)|2. f(\\bm{x}_{k+1}) \\leq f(\\bm{x}_k) - \\frac{1}{2A}|\\bm{g}(\\bm{x}_k)|^2. \nSubtracting the optimal value f*=f(ğ±*)f^\\ast = f(\\bm{x}^\\ast) from both sides f(ğ±k+1)âˆ’f*â‰¤f(ğ±k)âˆ’f*âˆ’12A|ğ (ğ±k)|2.(11) f(\\bm{x}_{k+1}) - f^\\ast \\leq f(\\bm{x}_k) - f^\\ast -\n\\frac{1}{2A}|\\bm{g}(\\bm{x}_k)|^2.  \\qquad(11)\nSimilarly, for any ğ±\\bm{x} there holds f(ğ±)â‰¥f(ğ±k)+ğ (ğ±k)âŠ¤(ğ±âˆ’ğ±k)+a2|ğ±âˆ’ğ±k|2. f(\\bm{x}) \\geq f(\\bm{x}_k) + \\bm{g}(\\bm{x}_k)^\\top (\\bm{x} - \\bm{x}_k) +\n\\frac{a}{2} | \\bm{x} - \\bm{x}_k |^2. \n\n\n\n\n\n\n\nAgain, minimize both sides: the left-hand side is minimized at f*f^\\ast and the right-hand side is minimized at ğ±â€¾=ğ±kâˆ’ğ (ğ±k)a\\bar{\\bm{x}} = \\bm{x}_k - \\frac{\\bm{g}(\\bm{x}_k)}{a}. Subsituting this ğ±â€¾\\bar{\\bm{x}} in the right-hand side gives f*â‰¥f(ğ±k)âˆ’12a|ğ (ğ±k)|2.(12) f^\\ast \\geq f(\\bm{x}_k) - \\frac{1}{2a}|\\bm{g}(\\bm{x}_k)|^2.  \\qquad(12)\nFrom EquationÂ 12, we have âˆ’|ğ (ğ±k)|2â‰¤2a[f*âˆ’f(ğ±k)]. - |\\bm{g}(\\bm{x}_k)|^2 \\leq 2a [f^\\ast - f(\\bm{x}_k)]. \nSubstituting this in EquationÂ 11 gives f(ğ±k+1)âˆ’f*â‰¤(1âˆ’aA)[f(ğ±kâˆ’f*)]. f(\\bm{x}_{k+1}) - f^\\ast \\leq (1-\\frac{a}{A})[f(\\bm{x}_k - f^\\ast)]. \nThis shows that SD makes progress even when it is not close to the solution.\n\n\n\n\n\n\n\n\nTheorem. Suppose that the Hessian matrix ğ…(ğ±*)\\bm{F}(\\bm{x}^\\ast) of ff at a relative minimum ğ±*\\bm{x}^\\ast has the smallest and largest eigenvalues a,A&gt;0a, A &gt; 0, respectively. If {ğ±k}\\{\\bm{x}_k\\} is a sequence generated by the method of steepest descent that converges to ğ±*\\bm{x}^\\ast, then the sequence of objective values {f(ğ±k)}\\{f(\\bm{x}_k)\\} converges to f(ğ±*)f(\\bm{x}^\\ast) linearly with a convergence ratio no greater than (Aâˆ’aA+a)2(\\frac{A-a}{A+a})^2."
  },
  {
    "objectID": "07_basic_descent.html#accelerated-steepest-descent",
    "href": "07_basic_descent.html#accelerated-steepest-descent",
    "title": "07_basic_descent",
    "section": "Accelerated Steepest Descent",
    "text": "Accelerated Steepest Descent\n\n\n\n\nThere is an accelerated steepest descent method that works as follows:\nÎ»0=0,Î»k+1=12(1+1+4Î»k2),Î±k=1âˆ’Î»kÎ»k+1,ğ±Ìƒk+1=ğ±kâˆ’1Î²âˆ‡f(ğ±k)âŠ¤,ğ±k+1=(1âˆ’Î±k)ğ±Ìƒk+1+Î±kğ±Ìƒk.\n\\begin{align}\n&\\lambda^0 = 0, & \\lambda_{k+1} = \\frac{1}{2}(1 + \\sqrt{1+4\\lambda_k^2}), &\n\\alpha_k = \\frac{1 - \\lambda_k}{\\lambda_{k+1}}, & \\tilde{\\bm{x}}_{k+1} =\n\\bm{x}_k - \\frac{1}{\\beta}\\nabla f(\\bm{x}_k)^\\top, & \\bm{x}_{k+1} = (1 -\n\\alpha_k) \\tilde{\\bm{x}}_{k+1} + \\alpha_k \\tilde{\\bm{x}}_k.\n\\end{align}\n\nNote that Î»k2=Î»k+1(Î»k+1âˆ’1)\\lambda_k^2 = \\lambda_{k+1}(\\lambda_{k+1} -1), Î»k&gt;k2\\lambda_k &gt; \\frac{k}{2}, and Î±kâ‰¤0\\alpha_k \\leq 0.\n\n\n\n\n\n\nTheorem (Accelerated Steepest Descent)\n\n\nLet f(ğ±)f(\\bm{x}) be convex and differentiable everywhere, satisfies the (first-order) Î²\\beta-Lipschitz condition, and admits a minimizer ğ±*\\bm{x}^\\ast. Then, the method of accelerated steepest descent generates a sequence of solutions such that\nf(ğ±Ìƒk+1)âˆ’f(ğ±*)â‰¤2Î²k2|ğ±0âˆ’ğ±*|2,âˆ€kâ‰¥1.\nf(\\tilde{\\bm{x}}_{k+1}) - f(\\bm{x}^\\ast) \\leq \\frac{2\\beta}{k^2}|\\bm{x}_0 -\n\\bm{x}^\\ast|^2, \\quad \\forall k \\geq 1."
  },
  {
    "objectID": "07_basic_descent.html#order-two-convergence",
    "href": "07_basic_descent.html#order-two-convergence",
    "title": "07_basic_descent",
    "section": "Order Two Convergence",
    "text": "Order Two Convergence\n\n\n\n\nTheorem (Newtonâ€™s Method)\n\n\nLet fâˆˆC3f \\in C^3 on â„n\\mathbb{R}^n and assume that at the local minimum point ğ±*\\bm{x}^\\ast, the Hessian ğ…(ğ±*)\\bm{F}(\\bm{x}^\\ast) is positive definite. Then if started sufficiently close to ğ±*\\bm{x}^\\ast, the points generated by Newtonâ€™s method converge to ğ±*\\bm{x}^\\ast. The order of convergence is at least two.\n\n\n\n\n\n\nProof\n\n\nThere are Ï,Î²1,Î²2&gt;0\\rho, \\beta_1, \\beta_2 &gt; 0 such that for all ğ±\\bm{x} with |ğ±âˆ’ğ±*&lt;Ï|\\bm{x} - \\bm{x}^\\ast &lt; \\rho, there holds |ğ…(ğ±)âˆ’1|&lt;Î²1|\\bm{F}(\\bm{x})^{-1}| &lt; \\beta_1 and |âˆ‡f(ğ±*)âŠ¤âˆ’âˆ‡f(ğ±)âŠ¤âˆ’ğ…(ğ±)(ğ±*âˆ’ğ±)|â‰¤Î²2|ğ±âˆ’ğ±*|2|\\nabla f(\\bm{x^\\ast})^\\top - \\nabla f(\\bm{x})^\\top - \\bm{F}(\\bm{x})(\\bm{x}^\\ast - \\bm{x})| \\leq \\beta_2 |\\bm{x} - \\bm{x}^\\ast|^2. now suppose ğ±k\\bm{x}_k is selected with Î²1Î²2|ğ±kâˆ’ğ±*|&lt;1\\beta_1\\beta_2|\\bm{x}_k - \\bm{x}^\\ast| &lt; 1 and |ğ±kâˆ’ğ±*|&lt;Ï|\\bm{x}_k - \\bm{x}^\\ast| &lt; \\rho. Then\n|ğ±k+1âˆ’ğ±*|=|ğ±kâˆ’ğ±*âˆ’ğ…(ğ±k)âˆ’1âˆ‡f(ğ±k)âŠ¤|=|ğ…(ğ±k)âˆ’1[âˆ‡f(ğ±*)âŠ¤âˆ’âˆ‡f(ğ±k)âˆ’ğ…(ğ±k)(ğ±*âˆ’ğ±k)]|â‰¤|ğŸ(ğ±k)âˆ’1|Î²2|ğ±kâˆ’ğ±*|2â‰¤Î²1Î²2|ğ±kâˆ’ğ±*|2&lt;|ğ±kâˆ’ğ±*|.\n\\begin{align}\n|\\bm{x}_{k+1} - \\bm{x}^\\ast| &= |\\bm{x}_k - \\bm{x}^\\ast -\n\\bm{F}(\\bm{x}_k)^{-1}\\nabla f(\\bm{x}_k)^\\top| = |\\bm{F}(\\bm{x}_k)^{-1}[\\nabla\nf(\\bm{x}^\\ast)^\\top - \\nabla f(\\bm{x}_k) - \\bm{F}(\\bm{x}_k)(\\bm{x}^\\ast -\n\\bm{x}_k)]| \\\\ &\\leq |\\bm{f}(\\bm{x}_k)^{-1}|\\beta_2|\\bm{x}_k - \\bm{x}^\\ast|^2\n\\leq \\beta_1\\beta_2|\\bm{x}_k - \\bm{x}^\\ast|^2 &lt; |\\bm{x}_k - \\bm{x}^\\ast|.\n\\end{align}\n\nThe final inequality shows that the new point is closer to ğ±*\\bm{x}^\\ast than the old point, and hence all conditions apply again to ğ±k+1\\bm{x}_{k+1}. The previous inequality establishes that the convergence is second order."
  },
  {
    "objectID": "07_basic_descent.html#modifications",
    "href": "07_basic_descent.html#modifications",
    "title": "07_basic_descent",
    "section": "Modifications",
    "text": "Modifications\n\n\n\nAlthough Newtonâ€™s method is very attractive in terms of its convergence properties near the solution, it requires modification before it can be used at points that are remote from the solution.\n\n\n\n\n\n\n\n\nDamping\n\n\nA search parameter Î±\\alpha is introduced ğ±k+1=ğ±kâˆ’Î±kğ…(ğ±k)âˆ’1âˆ‡f(ğ±k)âŠ¤, \\bm{x}_{k+1} = \\bm{x}_k - \\alpha_k \\bm{F}(\\bm{x}_k)^{-1}\\nabla\nf(\\bm{x}_k)^\\top,  where Î±k\\alpha_k is selected to minimize ff.\n\n\n\n\n\n\nPositive Definiteness and Scaling\n\n\nGeneral class of algorithms is given by ğ±k+1=ğ±k+Î±ğk=ğ±kâˆ’Î±ğŒkğ k,(14) \\bm{x}_{k+1} = \\bm{x}_k + \\alpha \\bm{d}_k = \\bm{x}_k - \\alpha \\bm{M}_k \\bm{g}_k,  \\qquad(14)\n\nSD: ğŒk=ğˆ\\bm{M}_k = \\bm{I}, Newton: ğŒk=ğ…(ğ±k)âˆ’1\\bm{M}_k = \\bm{F}(\\bm{x}_k)^{-1}.\n\nFor small Î±\\alpha, it can be shown that f(ğ±k+1)=f(ğ±k)âˆ’Î±ğ kâŠ¤ğŒkğ k+O(Î±2). f(\\bm{x}_{k+1}) = f(\\bm{x}_k) -\n\\alpha \\bm{g}_k^\\top \\bm{M}_k \\bm{g}_k + O(\\alpha^2). \n\nAs Î±â†’0\\alpha \\rightarrow 0, the second term on the rhs dominates the third.\nTo guarantee a descrese in ff, we must have ğ kâŠ¤ğŒkğ k&gt;0\\bm{g}_k^\\top \\bm{M}_k \\bm{g}_k &gt; 0.\n\nSimplest way to ensure this is to require ğŒkâ‰»ğŸ\\bm{M}_k \\succ \\bm{0}.\n\n\n\n\n\n\n\n\n\nGeneral Problems\n\n\n\nIn practice, Newtonâ€™s method must be modified to accommodate the possible nonpositive definiteness at regions remote from the solution.\nCommon approach: ğŒk=[Î¼kğˆ+ğ…(ğ±k)]âˆ’1\\bm{M}_k = [\\mu_k\\bm{I} + \\bm{F}(\\bm{x}_k)]^{-1} for some Î¼k&gt;0\\mu_k &gt; 0.\nThis can be regarded as a compromise between SD (Î¼k\\mu_k very large) and Newtonâ€™s method (Î¼k=0\\mu_k = 0).\n\nLevenberg-Marquardt performs Cholesky factorization for a given value of Î¼k\\mu_k as follows Î¼kğˆ+ğ…(ğ±k)=ğ†ğ†âŠ¤.\\mu_k \\bm{I} + \\bm{F}(\\bm{x}_k) = \\bm{G}\\bm{G}^\\top. \nThis checks for positive definiteness (not positive definite if factorization fails).\nIf the factorization breaks down Î¼k\\mu_k is increased.\nStep direction is found by solving ğ†ğ†âŠ¤ğk=âˆ’ğ k\\bm{G}\\bm{G}^\\top \\bm{d}_k = -\\bm{g}_k."
  },
  {
    "objectID": "07_basic_descent.html#coordinate-descent",
    "href": "07_basic_descent.html#coordinate-descent",
    "title": "07_basic_descent",
    "section": "Coordinate Descent",
    "text": "Coordinate Descent\n\n\n\nGiven a point ğ±=(x1,x2,â€¦,xn)\\bm{x} = (x_1, x_2, \\ldots, x_n), descent w.r.t. the coordinate xix_i (ii fixed) means that one solves\nminimizexif(x1,x2,â€¦,xn). \\operatorname{minimize}_{x_i} \\quad f(x_1, x_2, \\ldots, x_n). \n\n\n\n\nOnly changes in the single component xix_i are allowed in seeking a new and better vector ğ±\\bm{x}.\n\nOne can also consider ğ±i\\bm{x}_i the ithi^{\\text{th}} block of variables (block coordinate method).\n\nIn our general terminology, each such descent can be regarded as a descent in the direction ği\\bm{e}_i.\n\n\n\n\n\n\nCyclic methods\n\n\n\nminx1,x2,â€¦,xn\\operatorname{min}\\;\\; x_1, x_2, \\ldots, x_n \\quad then minxnâˆ’1,xnâˆ’2,â€¦,x1\\quad \\operatorname{min}\\;\\; x_{n-1}, x_{n-2}, \\ldots, x_1\nminx1,x2,â€¦,xn\\operatorname{min}\\;\\; x_1, x_2, \\ldots, x_n \\quad then minx1,x2,â€¦,xn\\quad \\operatorname{min}\\;\\; x_{1}, x_{2}, \\ldots, x_n\n\nThey have the advantage of not requiring any information about âˆ‡f\\nabla f to determine descent directions.\n\n\n\n\n\n\n\nGauss-Southwell Method\n\n\n\nIf the gradient of ff is available, then it is possible to select the order of descent coordinates on the basis of the gradient.\nAt each stage, the coordinate corresponding to the largest component of the gradient vector is selected for descent."
  },
  {
    "objectID": "07_basic_descent.html#stochastic-gradient-descent-sgd-method",
    "href": "07_basic_descent.html#stochastic-gradient-descent-sgd-method",
    "title": "07_basic_descent",
    "section": "Stochastic Gradient Descent (SGD) Method",
    "text": "Stochastic Gradient Descent (SGD) Method\n\n\n\nImagine we are solving a stochastic optimization problem or its simple average approximation\nf(ğ±)=ğ”¼[Ï•(ğ±,ğ›)]orf(ğ±)=1Mâˆ‘i=1MÏ•(ğ±,ğ›ğ¢), f(\\bm{x}) = \\mathbb{E}[\\phi(\\bm{x}, \\bm{\\xi})] \\qquad \\text{or} \\qquad\nf(\\bm{x}) = \\frac{1}{M} \\sum_{i=1}^M \\phi(\\bm{x}, \\bm{\\xi_i}), \nwhere ğ›\\bm{\\xi} is a random parameter and Î¾i\\xi_i is a randomly chosen sample.\n\nIf we simply apply the SD, the evaluation of the gradient vector would be costly, involving a large sum computation.\nThe SGD would, at the current iterate ğ±k\\bm{x}_k, randomly select a sample point Î¾k\\xi_k and compute its (sub)gradient vector ğ k:=ğ (ğ±k,Î¾k)\\bm{g}_k := \\bm{g}(\\bm{x}_k, \\xi_k), which satisfies, in expectation, ğ”¼[ğ k|ğ±k]âˆˆâˆ‚f(ğ±k)\\mathbb{E}[\\bm{g}_k | \\bm{x}_k] \\in \\partial f(\\bm{x}_k).\nThen the method would update, starting from an initial solution ğ±0\\bm{x}_0,\n\nğ±k+1=ğ±kâˆ’Î±kğ k, \\bm{x}_{k+1} = \\bm{x}_k - \\alpha_k \\bm{g}_k, \nuntil k=Kâˆ’1k = K-1 and return the average solution: ğ±â€¾=1Kâˆ‘k=0Kâˆ’1ğ±k\\bar{\\bm{x}} = \\frac{1}{K}\\sum_{k=0}^{K-1} \\bm{x}_k.\n\n\n\n\n\n\nTheorem (SGD)\n\n\nLet f(ğ±)f(\\bm{x}) be a convex function that admits a minimizer ğ±*\\bm{x}^\\ast. Assume the following two conditions hold:\n\nThe sample (sub)gradients at ğ±k\\bm{x}_k satisfy |ğ k|â‰¤Î²(&gt;0)|\\bm{g}_k| \\leq \\beta (&gt;0) with probability 11 for all k=0,â€¦,Kâˆ’1k = 0, \\ldots, K-1.\nThe initial solution satisfies, for simplicity, |ğ±0âˆ’ğ±*|â‰¤1|\\bm{x}_0 - \\bm{x}^\\ast| \\leq 1.\n\nThen, with (fixed) stepsize Î±k=Î±=1Î²K\\alpha_k = \\alpha = \\frac{1}{\\beta\\sqrt{K}}, the returned solution ğ±â€¾\\bar{\\bm{x}} satisfies: ğ”¼[f(ğ±â€¾)âˆ’f(ğ±*)]â‰¤Î²K\\qquad \\mathbb{E}[f(\\bar{\\bm{x}}) - f(\\bm{x}^\\ast)] \\leq \\frac{\\beta}{\\sqrt{K}}.\n\n\n\n\n\nOptimization Theory and Practice â€¢ Aykut C. Satici"
  }
]