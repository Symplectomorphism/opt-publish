[
  {
    "objectID": "06_basic_unc.html#optimization-theory-and-practice",
    "href": "06_basic_unc.html#optimization-theory-and-practice",
    "title": "06_basic_unc",
    "section": "Optimization Theory and Practice",
    "text": "Optimization Theory and Practice\n\n\nBasic Properties of Solutions and Algorithms\n\n\n\n\nInstructor: Aykut Satici, Ph.D.   Mechanical and Biomedical Engineering  Electrical and Computer Engineering  Boise State University, Boise, ID, USA\n\n\nTopics:  First-Order Necessary Conditions  Examples  Second-Order Conditions  Convex and Concave Functions  Minimization and Maximization of Convex Functions  Global Convergence of Descent Algorithms"
  },
  {
    "objectID": "06_basic_unc.html#feasible-and-descent-directions",
    "href": "06_basic_unc.html#feasible-and-descent-directions",
    "title": "06_basic_unc",
    "section": "Feasible and Descent Directions",
    "text": "Feasible and Descent Directions\n\n\n\n\n\nDefinition (relative minimum or local minimum).\n\n\nA point 𝐱*∈Ω\\bm{x}^\\ast \\in \\Omega is said to be a relative minimum point of ff over Ω\\Omega if ∃ε&gt;0\\exists \\varepsilon &gt; 0 such that f(𝐱)≥f(𝐱*)f(\\bm{x}) \\geq f(\\bm{x}^\\ast) for all 𝐱∈Ω\\bm{x} \\in \\Omega within a distance ε\\varepsilon of 𝐱*\\bm{x}^\\ast.\n\n\n\n\n\n\n\nDefinition (global minimum).\n\n\nA point 𝐱*∈Ω\\bm{x}^\\ast \\in \\Omega is said to be a global minimum point of ff over Ω\\Omega if f(𝐱)≥f(𝐱*)f(\\bm{x}) \\geq f(\\bm{x}^\\ast) for all 𝐱∈Ω\\bm{x} \\in \\Omega.\n\nUsually impossible to find w/ gradient-based methods.\n\n\n\n\n\n\n\nAlong any given direction, the objective function can be regarded as a function of a single variable: the parameter defining movement in this direction.\n\n\n\n\nFeasible direction\n\n\nGiven 𝐱∈Ω\\bm{x} \\in \\Omega we say that a vector 𝐝\\bm{d} is a feasible direction at 𝐱\\bm{x} if there is an α‾&gt;0\\bar{\\alpha} &gt; 0 such that 𝐱+α𝐝∈Ω\\bm{x} + \\alpha \\bm{d} \\in \\Omega for all α\\alpha with 0≤α≤α‾0 \\leq \\alpha \\leq \\bar{\\alpha}.\n\n\n\n\n\n\nDescent direction\n\n\nAn element of the set of directions with the property {𝐝:∇f(𝐱)𝐝&lt;0}\\{\\bm{d}: \\nabla f(\\bm{x}) \\bm{d} &lt; 0\\} is called a descent direction.\nIf f(𝐱)∈C1f(\\bm{x}) \\in C^1, then there is α‾&gt;0\\bar{\\alpha} &gt; 0 such that f(𝐱+α𝐝)&lt;f(𝐱)f(\\bm{x} + \\alpha \\bm{d}) &lt; f(\\bm{x}) for all α\\alpha with 0&lt;α≤α‾0 &lt; \\alpha \\leq \\bar{\\alpha}. The direction 𝐝⊤=−∇f(𝐱)\\bm{d}^\\top = -\\nabla f(\\bm{x}) is the steepest descent one."
  },
  {
    "objectID": "06_basic_unc.html#first-order-necessary-conditions-1",
    "href": "06_basic_unc.html#first-order-necessary-conditions-1",
    "title": "06_basic_unc",
    "section": "First-Order Necessary Conditions",
    "text": "First-Order Necessary Conditions\n\n\n\nProposition (FONC).\n\n\nIf 𝐱*\\bm{x}^\\ast is a relative minimum point of ff over Ω⊆ℝn\\Omega \\subseteq \\mathbb{R}^n, then for any 𝐝∈ℝn\\bm{d} \\in \\mathbb{R}^n that is a feasible direction at 𝐱*\\bm{x}^\\ast, we have ∇f(𝐱*)𝐝≥0\\nabla f(\\bm{x}^\\ast) \\bm{d} \\geq 0.\n\n\n\n\n\n\nProof.\n\n\nFor any α\\alpha, 0≤α≤α‾0 \\leq \\alpha \\leq \\bar{\\alpha}, the point 𝐱(α)=𝐱*+α𝐝∈Ω\\bm{x}(\\alpha) = \\bm{x}^\\ast + \\alpha \\bm{d} \\in \\Omega. For 0≤α≤α‾0 \\leq \\alpha \\leq \\bar{\\alpha} define the function g(α)=f(𝐱(α))g(\\alpha) = f(\\bm{x}(\\alpha)). Then gg has a relative minimum at α=0\\alpha = 0. By ordinary calculus, we have\ng(α)−g(0)=g′(0)α+o(α)(1) g(\\alpha) - g(0) = g'(0)\\alpha + o(\\alpha)  \\qquad(1)\nwhere o(α)o(\\alpha) denotes terms that go to zero faster than α\\alpha. If g′(0)&lt;0g'(0) &lt; 0, then for sufficiently small values of α\\alpha, the rhs of Equation 1 will be negative and hence g(α)−g(0)&lt;0g(\\alpha) - g(0) &lt; 0, which contradicts the minimality of g(0)g(0). Thus g′(0)=∇f(𝐱*)𝐝≥0g'(0) = \\nabla f(\\bm{x}^\\ast) \\bm{d} \\geq 0.\n\n\n\n\n\n\nCorollary (Unconstrained Case).\n\n\nLet Ω⊆ℝn\\Omega \\subseteq \\mathbb{R}^n and let f∈C1f \\in C^1 on Ω\\Omega. If 𝐱*\\bm{x}^\\ast is a relative minimum point of ff over Ω\\Omega and if 𝐱*∈Ω̊\\bm{x}^\\ast \\in \\mathring{\\Omega}, then ∇f(𝐱*)=𝟎\\nabla f(\\bm{x}^\\ast) = \\bm{0}."
  },
  {
    "objectID": "06_basic_unc.html#first-order-sufficient-conditions",
    "href": "06_basic_unc.html#first-order-sufficient-conditions",
    "title": "06_basic_unc",
    "section": "First-Order Sufficient Conditions",
    "text": "First-Order Sufficient Conditions\n\n\n\nProposition (FOSC).\n\n\nLet f∈C1f \\in C^1 be a convex function on ℝn\\mathbb{R}^n. If 𝐱*\\bm{x}^\\ast meets the first-order conditions ∇f(𝐱*)=𝟎\\nabla f(\\bm{x}^\\ast) = \\bm{0}, 𝐱*\\bm{x}^\\ast is a global minimizer of ff.\n\n\n\n\n\n\nExamples\n\n\nExample 1. Consider the problem minimizef(x1,x2)=x12−x1x2+x22−3x2. \\operatorname{minimize} f(x_1, x_2) = x_1^2 - x_1x_2 + x_2^2 - 3x_2.  There are no constraints, Ω=ℝ2\\Omega = \\mathbb{R}^2. Setting the partial derivatives of ff equal to zero yields\n2x1−x2=0,−x1+2x2=3. 2x_1 - x_ 2= 0, \\quad -x_1 + 2x_2 = 3. \nwhich has the unique solution x1=1x_1 = 1, x2=2x_2 = 2. This is a global minimum point of ff.\nExample 2. minimizef(x1,x2)=x12−x1+x2+x1x2,subject tox1,x2≥0.\n\\begin{align}\n\\operatorname{minimize} & f(x_1, x_2) = x_1^2 - x_1 + x_2 + x_1x_2, \\\\\n\\text{subject to} & x_1, x_2 \\geq 0.\n\\end{align}\n This problem has a global minimum at x1=12x_1 = \\frac{1}{2}, x2=0x_2 = 0. At this point\n∂f∂x1=2x1−1+x2=0,∂f∂x2=1+x1=32. \\frac{\\partial f}{\\partial x_1} = 2x_1 - 1 + x_2 = 0, \\quad \\frac{\\partial\nf}{\\partial x_2} = 1 + x_1 = \\frac{3}{2}. \n\nThus the partial derivatives do not both vanish at the solution\nSince any feasible direction must have an x2x_2 component greater than or equal to zero, we have ∇f(𝐱*)𝐝≥0,∀𝐝∈ℝ2. \\nabla f(\\bm{x}^\\ast) \\bm{d} \\geq 0, \\;\\; \\forall \\bm{d} \\in \\mathbb{R}^2."
  },
  {
    "objectID": "06_basic_unc.html#example-1-logistic-regression",
    "href": "06_basic_unc.html#example-1-logistic-regression",
    "title": "06_basic_unc",
    "section": "Example 1 – Logistic Regression",
    "text": "Example 1 – Logistic Regression\n\nWe have vectors 𝐚i∈ℝd\\bm{a}_i \\in \\mathbb{R}^d for i=1,2,…,n1i = 1, 2, \\ldots, n_1 in a class and vectors 𝐛j∈ℝd\\bm{b}_j \\in \\mathbb{R}^d for j=1,2,…,n2j = 1, 2, \\ldots, n_2 not in that class.\nWe wish to classify them, i.e., find a vector 𝐲∈ℝd\\bm{y} \\in \\mathbb{R}^d and a number β\\beta such that\n\nexp(𝐚i⊤𝐲+β)1+exp(𝐚i⊤𝐲+β)≈1,∀i, and exp(𝐛j⊤𝐲+β)1+exp(𝐛j⊤𝐲+β)≈0,∀j. \\frac{\\operatorname{exp}(\\bm{a}_i^\\top \\bm{y} + \\beta)}{1 +\n\\operatorname{exp}(\\bm{a}_i^\\top \\bm{y} + \\beta)} \\approx 1, \\;\\; \\forall i,\n\\quad \\text{ and } \\quad \\frac{\\operatorname{exp}(\\bm{b}_j^\\top \\bm{y} +\n\\beta)}{1 + \\operatorname{exp}(\\bm{b}_j^\\top \\bm{y} + \\beta)} \\approx 0, \\;\\;\n\\forall j. \n\nThis problem can be cast as an unconstrained optimization problem\n\nmaximize𝐲,β(∏iexp(𝐚i⊤𝐲+β)1+exp(𝐚i⊤𝐲+β))(∏j(1−exp(𝐛j⊤𝐲+β)1+exp(𝐛j⊤𝐲+β))) \\operatorname{maximize}_{\\bm{y}, \\beta} \n\\left(\\prod_i \\frac{\\operatorname{exp}(\\bm{a}_i^\\top \\bm{y} + \\beta)}{1 +\n\\operatorname{exp}(\\bm{a}_i^\\top \\bm{y} + \\beta)}\\right) \\left(\\prod_j\n\\left(1 - \\frac{\\operatorname{exp}(\\bm{b}_j^\\top \\bm{y} + \\beta)}{1 +\n\\operatorname{exp}(\\bm{b}_j^\\top \\bm{y} + \\beta)} \\right) \\right)\n\nwhich may equivalently be expressed using a log transformation as\nminimize𝐲,β∑ilog(1+exp(−𝐚i⊤𝐲−β))+∑jlog(1+exp(𝐛i⊤𝐲+β)).\n\\operatorname{minimize}_{\\bm{y}, \\beta} \\sum_i \\operatorname{log}\\left(1 +\n\\operatorname{exp}(-\\bm{a}_i^\\top \\bm{y} - \\beta) \\right) + \\sum_j\n\\operatorname{log}\\left(1 + \\operatorname{exp}(\\bm{b}_i^\\top \\bm{y} + \\beta)\n\\right)."
  },
  {
    "objectID": "06_basic_unc.html#example-2-parametric-estimation-convex",
    "href": "06_basic_unc.html#example-2-parametric-estimation-convex",
    "title": "06_basic_unc",
    "section": "Example 2 – Parametric Estimation (Convex)",
    "text": "Example 2 – Parametric Estimation (Convex)\n\nA common use of optimization is for the purpose of function approximation.\nSuppose that through an experiment the value of a function gg is observed at mm points: x1,x2,…,xmx_1, x_2, \\ldots, x_m. Thus the values g(x1),g(x2),…,g(xm)g(x_1), g(x_2), \\ldots, g(x_m) are known.\nWe wish to approximate the function by a polynomial of degree n&lt;mn &lt; m:\n\nh(x)=a0+a1x+⋯+an−1xn−1+anxn. h(x) = a_0 + a_1 x + \\cdots + a_{n-1}x^{n-1} + a_nx^n. \n\nDefine the errors εk=g(xk)−h(xk)\\varepsilon_k = g(x_k) - h(x_k) and define the best approximation as the polynomial that minimizes the sum-of-squares of these errors\n\nminimize𝐚f(𝐚)=∑k=1nεk2=∑k=1n[g(xk)−(a0+a1x+⋯+an−1xn−1+anxn)]2. \\operatorname{minimize}_\\bm{a} f(\\bm{a}) = \\sum_{k=1}^n \\varepsilon_k^2 =\n\\sum_{k=1}^n \\left[g(x_k) - \\left(a_0 + a_1 x + \\cdots + a_{n-1}x^{n-1} +\na_nx^n\\right) \\right]^2.  \n\nTo find a compact representation for this objective, define\n\nqij≜∑k=1mxki+j,bj=∑k=1ng(xk)xkj, and c=∑k=1ng(xk)2. q_{ij} \\triangleq \\sum_{k=1}^m x_k^{i+j}, \\quad b_j = \\sum_{k=1}^n\ng(x_k)x_k^j, \\quad \\text{ and } c = \\sum_{k=1}^n g(x_k)^2. \n\nThen after a bit of algebra, it can be shown that f(𝐚)=𝐚⊤𝐐𝐚−2𝐛⊤𝐚+c\\quad f(\\bm{a}) = \\bm{a}^\\top \\bm{Qa} - 2\\bm{b}^\\top\\bm{a} + c."
  },
  {
    "objectID": "06_basic_unc.html#example-2-parametric-estimation-nonconvex",
    "href": "06_basic_unc.html#example-2-parametric-estimation-nonconvex",
    "title": "06_basic_unc",
    "section": " Example 2 – Parametric Estimation (Nonconvex) ",
    "text": "Example 2 – Parametric Estimation (Nonconvex) \n\n\n\nEstimating the parameters of a neural network is typically nonconvex.\nThis network has 66 layers, where the initial layer is the input vector 𝐱=𝐟0\\bm{x} = \\bm{f}^0 and the last layer is the function output 𝐟(𝐱)=𝐟5\\bm{f}(\\bm{x}) = \\bm{f}^5.\n\n\n\n\n\n\n\n\nThe vector function 𝐟ℓ\\bm{f}^\\ell, ℓ=0,1,…,5\\ell = 0, 1, \\ldots, 5, is defined recursively by the parameter weights between two consecutive layers wijℓ−1w_{ij}^{\\ell-1} as a piecewise linear/affine function\n\nfjℓ=max{0,∑iwijℓ−1fiℓ−1},∀j. f_j^\\ell = \\operatorname{max}\\left\\{0, \\sum_i w_{ij}^{\\ell-1} f_i^{\\ell -1\n}\\right\\}, \\quad \\forall j. \n\nSimilarly, for a sequence of variable value vector 𝐱k\\bm{x}^k and observed function value vector 𝐠(𝐱k)\\bm{g}(\\bm{x}^k),\n\nWe would like to find all weights (wijℓ)\\left(w_{ij}^\\ell \\right)’s to minimize the total difference between 𝐟(𝐱k)\\bm{f}(\\bm{x}^k) and 𝐠(𝐱k)\\bm{g}(\\bm{x}^k) for all kk. ∑k|𝐟(𝐱k)−𝐠(𝐱k)|2. \\sum_k \\left| \\bm{f}(\\bm{x}^k) - \\bm{g}(\\bm{x}^k) \\right|^2."
  },
  {
    "objectID": "06_basic_unc.html#second-order-necessary-conditions",
    "href": "06_basic_unc.html#second-order-necessary-conditions",
    "title": "06_basic_unc",
    "section": "Second-Order Necessary Conditions",
    "text": "Second-Order Necessary Conditions\n\n\n\nProposition (Second-Order Necessary Conditions)\n\n\nLet Ω⊆ℝn\\Omega \\subseteq \\mathbb{R}^n and let f∈C2f \\in C^2 on Ω\\Omega. If 𝐱*\\bm{x}^\\ast is a relative minimum point of ff over Ω\\Omega, then for any 𝐝∈ℝn\\bm{d} \\in \\mathbb{R}^n that is a feasible direction at 𝐱*\\bm{x}^\\ast we have\n∇f(𝐱*)𝐝≥0(2) \\nabla f(\\bm{x}^\\ast) \\bm{d} \\geq 0  \\qquad(2) ∇f(𝐱*)𝐝=0, then 𝐝⊤∇2f(𝐱*)𝐝=𝐝⊤𝐅(𝐱*)𝐝≥0.(3) \\nabla f(\\bm{x}^\\ast) \\bm{d} = 0, \\;\\; \\text{ then } \\;\\; \\bm{d}^\\top\n\\nabla^2 f(\\bm{x}^\\ast) \\bm{d} = \\bm{d}^\\top \\bm{F}(\\bm{x}^\\ast) \\bm{d} \\geq 0. \\qquad(3)\n\n\n\n\n\n\nProof\n\n\nThe first condition is just the FONC, and the second applies only if ∇f(𝐱*)𝐝=0\\nabla f(\\bm{x}^\\ast)\\bm{d} = 0. In this case, introducing 𝐱(α)=𝐱*+α𝐝\\bm{x}(\\alpha) = \\bm{x}^\\ast + \\alpha \\bm{d} and g(α)=f(𝐱(α))g(\\alpha) = f(\\bm{x}(\\alpha)) as before, we have, in view of g′(α)=0g'(\\alpha) = 0,\ng(α)−g(0)=12g″(0)α2+o(α2). g(\\alpha) - g(0) = \\frac{1}{2}g''(0)\\alpha^2 + o(\\alpha^2). \nIf g″(0)&lt;0g''(0) &lt; 0a the rhs of the above equation is negative for sufficiently small α\\alpha which contradicts the relative minimum nature of g(0)g(0). Thus\ng″(0)=𝐝⊤∇2f(𝐱*)𝐝≥0. g''(0) = \\bm{d}^\\top \\nabla^2 f(\\bm{x}^\\ast)\\bm{d} \\geq 0. \n\n\n\n\nSee Example 2 from FOSC slide with d2=0d_2 = 0 whence 𝐝⊤∇2f(𝐱*)𝐝=2d12≥0\\bm{d}^\\top \\nabla^2 f(\\bm{x}^\\ast) \\bm{d} = 2d_1^2 \\geq 0, satisfying the second condition."
  },
  {
    "objectID": "06_basic_unc.html#second-order-necessary-conditions-1",
    "href": "06_basic_unc.html#second-order-necessary-conditions-1",
    "title": "06_basic_unc",
    "section": "Second-Order Necessary Conditions",
    "text": "Second-Order Necessary Conditions\n\n\n\nProposition (Second-Order Necessary Conditions — Unconstrained Case)\n\n\nLet 𝐱*∈Ω̊\\bm{x}^\\ast \\in \\mathring{\\Omega} and suppose 𝐱*\\bm{x}^\\ast is a relative minimum point over Ω\\Omega of f∈C2f \\in C^2. Then\n∇f(𝐱*)=0(4) \\nabla f(\\bm{x}^\\ast) = 0  \\qquad(4) ∀𝐝,𝐝⊤𝐅(𝐱*)𝐝≥0.(5) \\forall \\bm{d},  \\;\\; \\bm{d}^\\top \\bm{F}(\\bm{x}^\\ast) \\bm{d} \\geq 0. \\qquad(5)\n\nThe second condition is equivalent to stating that the matrix 𝐅(𝐱*)\\bm{F}(\\bm{x}^\\ast) is positive semidefinite.\n\n\n\n\n\n\n\n\n\nExample\n\n\nminimizef(x1,x2)=x13−x12x2+2x22subject tox1,x2≥0.\n\\begin{align}\n\\operatorname{minimize} & f(x_1, x_2) = x_1^3 - x_1^2x_2 + 2x_2^2 \\\\\n\\text{subject to} & x_1, x_2 \\geq 0.\n\\end{align}\n\n\n\n\n\nIf x1,x2&gt;0x_1, x_2 &gt; 0, then the FONC are\n3x12−2x1x2=0,−x12+4x2=0. 3x_1^2 - 2x_1x_2 = 0, \\quad -x_1^2 + 4x_2 = 0. \nwith a solution at x1=6x_1 = 6, x2=9x_2 = 9.\n\n\n\n\n\nNote that for x1x_1 fixed at x1=6x_1 = 6, the objective attains a relative minimum w.r.t. x2x_2 at $x_2 = 9.\nConversely, with x2x_2 fixed at x2=9x_2 = 9, the objective attains a relative minimum w.r.t. x1x_1 at x1=6x_1 = 6.\n\n\n\nDespite this fact, the point x1=6x_1 = 6, x2=9x_2 =9 is not a relative minimum because the Hessian matrix is\n𝐅(𝐱)=[6x1−2x2−2x1−2x14];𝐅(𝐱*)=[18−12−124]⋡0.\n\\bm{F}(\\bm{x}) = \\begin{bmatrix} 6x_1 - 2x_2 & -2x_1 \\\\ -2x_1 & 4 \\end{bmatrix};\n\\qquad \n\\bm{F}(\\bm{x}^\\ast) = \\begin{bmatrix} 18 & -12 \\\\ -12 & 4 \\end{bmatrix} \\nsucceq  0."
  },
  {
    "objectID": "06_basic_unc.html#second-order-sufficient-conditions",
    "href": "06_basic_unc.html#second-order-sufficient-conditions",
    "title": "06_basic_unc",
    "section": "Second-Order Sufficient Conditions",
    "text": "Second-Order Sufficient Conditions\n\n\n\nProposition (Second-Order Sufficient Conditions)\n\n\nLet f∈C2f \\in C^2 be a function defined on a region in which the point 𝐱*\\bm{x}^\\ast is an interior point. Suppose\n∇f(𝐱*)=0(6) \\nabla f(\\bm{x}^\\ast) = 0  \\qquad(6) 𝐅(𝐱*)≻0(7) \\bm{F}(\\bm{x}^\\ast) \\succ 0  \\qquad(7)\nThen 𝐱*\\bm{x}^\\ast is a strict relative minimum of ff.\n\n\n\n\n\n\nProof\n\n\nSince 𝐅(𝐱*)\\bm{F}(\\bm{x}^\\ast) is positive definite, there is an a&gt;0a &gt; 0 such that for all 𝐝\\bm{d}, 𝐝⊤𝐅(𝐱*)𝐝≥a|𝐝|2\\bm{d}^\\top \\bm{F}(\\bm{x}^\\ast)\\bm{d} \\geq a |\\bm{d}|^2. By Taylor’s Theorem (with remainder)\nf(𝐱*+𝐝)−f(𝐱*)=12𝐝⊤𝐅(𝐱*)𝐝+o(|𝐝|2)≥a2|𝐝|2+o(|𝐝|2). f(\\bm{x}^\\ast + \\bm{d}) - f(\\bm{x}^\\ast) = \\frac{1}{2} \\bm{d}^\\top\n\\bm{F}(\\bm{x}^\\ast)\\bm{d} + o(|\\bm{d}|^2) \\geq \\frac{a}{2}|\\bm{d}|^2 +\no(|\\bm{d}|^2). \nFor small |𝐝||\\bm{d}|, the first term on the right dominates the second, implying that both sides are positive for small 𝐝\\bm{d}."
  }
]