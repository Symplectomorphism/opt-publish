[
  {
    "objectID": "06_basic_unc.html#optimization-theory-and-practice",
    "href": "06_basic_unc.html#optimization-theory-and-practice",
    "title": "06_basic_unc",
    "section": "Optimization Theory and Practice",
    "text": "Optimization Theory and Practice\n\n\nBasic Properties of Solutions and Algorithms\n\n\n\n\nInstructor: Aykut Satici, Ph.D.Â   Mechanical and Biomedical Engineering  Electrical and Computer Engineering  Boise State University, Boise, ID, USA\n\n\nTopics:  First-Order Necessary Conditions  Examples  Second-Order Conditions  Convex and Concave Functions  Minimization and Maximization of Convex Functions  Global Convergence of Descent Algorithms"
  },
  {
    "objectID": "06_basic_unc.html#feasible-and-descent-directions",
    "href": "06_basic_unc.html#feasible-and-descent-directions",
    "title": "06_basic_unc",
    "section": "Feasible and Descent Directions",
    "text": "Feasible and Descent Directions\n\n\n\n\n\nDefinition (relative minimum or local minimum).\n\n\nA point ğ±*âˆˆÎ©\\bm{x}^\\ast \\in \\Omega is said to be a relative minimum point of ff over Î©\\Omega if âˆƒÎµ&gt;0\\exists \\varepsilon &gt; 0 such that f(ğ±)â‰¥f(ğ±*)f(\\bm{x}) \\geq f(\\bm{x}^\\ast) for all ğ±âˆˆÎ©\\bm{x} \\in \\Omega within a distance Îµ\\varepsilon of ğ±*\\bm{x}^\\ast.\n\n\n\n\n\n\n\nDefinition (global minimum).\n\n\nA point ğ±*âˆˆÎ©\\bm{x}^\\ast \\in \\Omega is said to be a global minimum point of ff over Î©\\Omega if f(ğ±)â‰¥f(ğ±*)f(\\bm{x}) \\geq f(\\bm{x}^\\ast) for all ğ±âˆˆÎ©\\bm{x} \\in \\Omega.\n\nUsually impossible to find w/ gradient-based methods.\n\n\n\n\n\n\n\nAlong any given direction, the objective function can be regarded as a function of a single variable: the parameter defining movement in this direction.\n\n\n\n\nFeasible direction\n\n\nGiven ğ±âˆˆÎ©\\bm{x} \\in \\Omega we say that a vector ğ\\bm{d} is a feasible direction at ğ±\\bm{x} if there is an Î±â€¾&gt;0\\bar{\\alpha} &gt; 0 such that ğ±+Î±ğâˆˆÎ©\\bm{x} + \\alpha \\bm{d} \\in \\Omega for all Î±\\alpha with 0â‰¤Î±â‰¤Î±â€¾0 \\leq \\alpha \\leq \\bar{\\alpha}.\n\n\n\n\n\n\nDescent direction\n\n\nAn element of the set of directions with the property {ğ:âˆ‡f(ğ±)ğ&lt;0}\\{\\bm{d}: \\nabla f(\\bm{x}) \\bm{d} &lt; 0\\} is called a descent direction.\nIf f(ğ±)âˆˆC1f(\\bm{x}) \\in C^1, then there is Î±â€¾&gt;0\\bar{\\alpha} &gt; 0 such that f(ğ±+Î±ğ)&lt;f(ğ±)f(\\bm{x} + \\alpha \\bm{d}) &lt; f(\\bm{x}) for all Î±\\alpha with 0&lt;Î±â‰¤Î±â€¾0 &lt; \\alpha \\leq \\bar{\\alpha}. The direction ğâŠ¤=âˆ’âˆ‡f(ğ±)\\bm{d}^\\top = -\\nabla f(\\bm{x}) is the steepest descent one."
  },
  {
    "objectID": "06_basic_unc.html#first-order-necessary-conditions-1",
    "href": "06_basic_unc.html#first-order-necessary-conditions-1",
    "title": "06_basic_unc",
    "section": "First-Order Necessary Conditions",
    "text": "First-Order Necessary Conditions\n\n\n\nProposition (FONC).\n\n\nIf ğ±*\\bm{x}^\\ast is a relative minimum point of ff over Î©âŠ†â„n\\Omega \\subseteq \\mathbb{R}^n, then for any ğâˆˆâ„n\\bm{d} \\in \\mathbb{R}^n that is a feasible direction at ğ±*\\bm{x}^\\ast, we have âˆ‡f(ğ±*)ğâ‰¥0\\nabla f(\\bm{x}^\\ast) \\bm{d} \\geq 0.\n\n\n\n\n\n\nProof.\n\n\nFor any Î±\\alpha, 0â‰¤Î±â‰¤Î±â€¾0 \\leq \\alpha \\leq \\bar{\\alpha}, the point ğ±(Î±)=ğ±*+Î±ğâˆˆÎ©\\bm{x}(\\alpha) = \\bm{x}^\\ast + \\alpha \\bm{d} \\in \\Omega. For 0â‰¤Î±â‰¤Î±â€¾0 \\leq \\alpha \\leq \\bar{\\alpha} define the function g(Î±)=f(ğ±(Î±))g(\\alpha) = f(\\bm{x}(\\alpha)). Then gg has a relative minimum at Î±=0\\alpha = 0. By ordinary calculus, we have\ng(Î±)âˆ’g(0)=gâ€²(0)Î±+o(Î±)(1) g(\\alpha) - g(0) = g'(0)\\alpha + o(\\alpha)  \\qquad(1)\nwhere o(Î±)o(\\alpha) denotes terms that go to zero faster than Î±\\alpha. If gâ€²(0)&lt;0g'(0) &lt; 0, then for sufficiently small values of Î±\\alpha, the rhs of EquationÂ 1 will be negative and hence g(Î±)âˆ’g(0)&lt;0g(\\alpha) - g(0) &lt; 0, which contradicts the minimality of g(0)g(0). Thus gâ€²(0)=âˆ‡f(ğ±*)ğâ‰¥0g'(0) = \\nabla f(\\bm{x}^\\ast) \\bm{d} \\geq 0.\n\n\n\n\n\n\nCorollary (Unconstrained Case).\n\n\nLet Î©âŠ†â„n\\Omega \\subseteq \\mathbb{R}^n and let fâˆˆC1f \\in C^1 on Î©\\Omega. If ğ±*\\bm{x}^\\ast is a relative minimum point of ff over Î©\\Omega and if ğ±*âˆˆÎ©ÌŠ\\bm{x}^\\ast \\in \\mathring{\\Omega}, then âˆ‡f(ğ±*)=ğŸ\\nabla f(\\bm{x}^\\ast) = \\bm{0}."
  },
  {
    "objectID": "06_basic_unc.html#first-order-sufficient-conditions",
    "href": "06_basic_unc.html#first-order-sufficient-conditions",
    "title": "06_basic_unc",
    "section": "First-Order Sufficient Conditions",
    "text": "First-Order Sufficient Conditions\n\n\n\nProposition (FOSC).\n\n\nLet fâˆˆC1f \\in C^1 be a convex function on â„n\\mathbb{R}^n. If ğ±*\\bm{x}^\\ast meets the first-order conditions âˆ‡f(ğ±*)=ğŸ\\nabla f(\\bm{x}^\\ast) = \\bm{0}, ğ±*\\bm{x}^\\ast is a global minimizer of ff.\n\n\n\n\n\n\nExamples\n\n\nExample 1. Consider the problem minimizef(x1,x2)=x12âˆ’x1x2+x22âˆ’3x2. \\operatorname{minimize} f(x_1, x_2) = x_1^2 - x_1x_2 + x_2^2 - 3x_2.  There are no constraints, Î©=â„2\\Omega = \\mathbb{R}^2. Setting the partial derivatives of ff equal to zero yields\n2x1âˆ’x2=0,âˆ’x1+2x2=3. 2x_1 - x_ 2= 0, \\quad -x_1 + 2x_2 = 3. \nwhich has the unique solution x1=1x_1 = 1, x2=2x_2 = 2. This is a global minimum point of ff.\nExample 2. minimizef(x1,x2)=x12âˆ’x1+x2+x1x2,subject tox1,x2â‰¥0.\n\\begin{align}\n\\operatorname{minimize} & f(x_1, x_2) = x_1^2 - x_1 + x_2 + x_1x_2, \\\\\n\\text{subject to} & x_1, x_2 \\geq 0.\n\\end{align}\n This problem has a global minimum at x1=12x_1 = \\frac{1}{2}, x2=0x_2 = 0. At this point\nâˆ‚fâˆ‚x1=2x1âˆ’1+x2=0,âˆ‚fâˆ‚x2=1+x1=32. \\frac{\\partial f}{\\partial x_1} = 2x_1 - 1 + x_2 = 0, \\quad \\frac{\\partial\nf}{\\partial x_2} = 1 + x_1 = \\frac{3}{2}. \n\nThus the partial derivatives do not both vanish at the solution\nSince any feasible direction must have an x2x_2 component greater than or equal to zero, we have âˆ‡f(ğ±*)ğâ‰¥0,âˆ€ğâˆˆâ„2. \\nabla f(\\bm{x}^\\ast) \\bm{d} \\geq 0, \\;\\; \\forall \\bm{d} \\in \\mathbb{R}^2."
  },
  {
    "objectID": "06_basic_unc.html#example-1-logistic-regression",
    "href": "06_basic_unc.html#example-1-logistic-regression",
    "title": "06_basic_unc",
    "section": "Example 1 â€“ Logistic Regression",
    "text": "Example 1 â€“ Logistic Regression\n\nWe have vectors ğšiâˆˆâ„d\\bm{a}_i \\in \\mathbb{R}^d for i=1,2,â€¦,n1i = 1, 2, \\ldots, n_1 in a class and vectors ğ›jâˆˆâ„d\\bm{b}_j \\in \\mathbb{R}^d for j=1,2,â€¦,n2j = 1, 2, \\ldots, n_2 not in that class.\nWe wish to classify them, i.e., find a vector ğ²âˆˆâ„d\\bm{y} \\in \\mathbb{R}^d and a number Î²\\beta such that\n\nexp(ğšiâŠ¤ğ²+Î²)1+exp(ğšiâŠ¤ğ²+Î²)â‰ˆ1,âˆ€i, and exp(ğ›jâŠ¤ğ²+Î²)1+exp(ğ›jâŠ¤ğ²+Î²)â‰ˆ0,âˆ€j. \\frac{\\operatorname{exp}(\\bm{a}_i^\\top \\bm{y} + \\beta)}{1 +\n\\operatorname{exp}(\\bm{a}_i^\\top \\bm{y} + \\beta)} \\approx 1, \\;\\; \\forall i,\n\\quad \\text{ and } \\quad \\frac{\\operatorname{exp}(\\bm{b}_j^\\top \\bm{y} +\n\\beta)}{1 + \\operatorname{exp}(\\bm{b}_j^\\top \\bm{y} + \\beta)} \\approx 0, \\;\\;\n\\forall j. \n\nThis problem can be cast as an unconstrained optimization problem\n\nmaximizeğ²,Î²(âˆiexp(ğšiâŠ¤ğ²+Î²)1+exp(ğšiâŠ¤ğ²+Î²))(âˆj(1âˆ’exp(ğ›jâŠ¤ğ²+Î²)1+exp(ğ›jâŠ¤ğ²+Î²))) \\operatorname{maximize}_{\\bm{y}, \\beta} \n\\left(\\prod_i \\frac{\\operatorname{exp}(\\bm{a}_i^\\top \\bm{y} + \\beta)}{1 +\n\\operatorname{exp}(\\bm{a}_i^\\top \\bm{y} + \\beta)}\\right) \\left(\\prod_j\n\\left(1 - \\frac{\\operatorname{exp}(\\bm{b}_j^\\top \\bm{y} + \\beta)}{1 +\n\\operatorname{exp}(\\bm{b}_j^\\top \\bm{y} + \\beta)} \\right) \\right)\n\nwhich may equivalently be expressed using a log transformation as\nminimizeğ²,Î²âˆ‘ilog(1+exp(âˆ’ğšiâŠ¤ğ²âˆ’Î²))+âˆ‘jlog(1+exp(ğ›iâŠ¤ğ²+Î²)).\n\\operatorname{minimize}_{\\bm{y}, \\beta} \\sum_i \\operatorname{log}\\left(1 +\n\\operatorname{exp}(-\\bm{a}_i^\\top \\bm{y} - \\beta) \\right) + \\sum_j\n\\operatorname{log}\\left(1 + \\operatorname{exp}(\\bm{b}_i^\\top \\bm{y} + \\beta)\n\\right)."
  },
  {
    "objectID": "06_basic_unc.html#example-2-parametric-estimation-convex",
    "href": "06_basic_unc.html#example-2-parametric-estimation-convex",
    "title": "06_basic_unc",
    "section": "Example 2 â€“ Parametric Estimation (Convex)",
    "text": "Example 2 â€“ Parametric Estimation (Convex)\n\nA common use of optimization is for the purpose of function approximation.\nSuppose that through an experiment the value of a function gg is observed at mm points: x1,x2,â€¦,xmx_1, x_2, \\ldots, x_m. Thus the values g(x1),g(x2),â€¦,g(xm)g(x_1), g(x_2), \\ldots, g(x_m) are known.\nWe wish to approximate the function by a polynomial of degree n&lt;mn &lt; m:\n\nh(x)=a0+a1x+â‹¯+anâˆ’1xnâˆ’1+anxn. h(x) = a_0 + a_1 x + \\cdots + a_{n-1}x^{n-1} + a_nx^n. \n\nDefine the errors Îµk=g(xk)âˆ’h(xk)\\varepsilon_k = g(x_k) - h(x_k) and define the best approximation as the polynomial that minimizes the sum-of-squares of these errors\n\nminimizeğšf(ğš)=âˆ‘k=1nÎµk2=âˆ‘k=1n[g(xk)âˆ’(a0+a1x+â‹¯+anâˆ’1xnâˆ’1+anxn)]2. \\operatorname{minimize}_\\bm{a} f(\\bm{a}) = \\sum_{k=1}^n \\varepsilon_k^2 =\n\\sum_{k=1}^n \\left[g(x_k) - \\left(a_0 + a_1 x + \\cdots + a_{n-1}x^{n-1} +\na_nx^n\\right) \\right]^2.  \n\nTo find a compact representation for this objective, define\n\nqijâ‰œâˆ‘k=1mxki+j,bj=âˆ‘k=1ng(xk)xkj, and c=âˆ‘k=1ng(xk)2. q_{ij} \\triangleq \\sum_{k=1}^m x_k^{i+j}, \\quad b_j = \\sum_{k=1}^n\ng(x_k)x_k^j, \\quad \\text{ and } c = \\sum_{k=1}^n g(x_k)^2. \n\nThen after a bit of algebra, it can be shown that f(ğš)=ğšâŠ¤ğğšâˆ’2ğ›âŠ¤ğš+c\\quad f(\\bm{a}) = \\bm{a}^\\top \\bm{Qa} - 2\\bm{b}^\\top\\bm{a} + c."
  },
  {
    "objectID": "06_basic_unc.html#example-2-parametric-estimation-nonconvex",
    "href": "06_basic_unc.html#example-2-parametric-estimation-nonconvex",
    "title": "06_basic_unc",
    "section": " Example 2 â€“ Parametric Estimation (Nonconvex) ",
    "text": "Example 2 â€“ Parametric Estimation (Nonconvex) \n\n\n\nEstimating the parameters of a neural network is typically nonconvex.\nThis network has 66 layers, where the initial layer is the input vector ğ±=ğŸ0\\bm{x} = \\bm{f}^0 and the last layer is the function output ğŸ(ğ±)=ğŸ5\\bm{f}(\\bm{x}) = \\bm{f}^5.\n\n\n\n\n\n\n\n\nThe vector function ğŸâ„“\\bm{f}^\\ell, â„“=0,1,â€¦,5\\ell = 0, 1, \\ldots, 5, is defined recursively by the parameter weights between two consecutive layers wijâ„“âˆ’1w_{ij}^{\\ell-1} as a piecewise linear/affine function\n\nfjâ„“=max{0,âˆ‘iwijâ„“âˆ’1fiâ„“âˆ’1},âˆ€j. f_j^\\ell = \\operatorname{max}\\left\\{0, \\sum_i w_{ij}^{\\ell-1} f_i^{\\ell -1\n}\\right\\}, \\quad \\forall j. \n\nSimilarly, for a sequence of variable value vector ğ±k\\bm{x}^k and observed function value vector ğ (ğ±k)\\bm{g}(\\bm{x}^k),\n\nWe would like to find all weights (wijâ„“)\\left(w_{ij}^\\ell \\right)â€™s to minimize the total difference between ğŸ(ğ±k)\\bm{f}(\\bm{x}^k) and ğ (ğ±k)\\bm{g}(\\bm{x}^k) for all kk. âˆ‘k|ğŸ(ğ±k)âˆ’ğ (ğ±k)|2. \\sum_k \\left| \\bm{f}(\\bm{x}^k) - \\bm{g}(\\bm{x}^k) \\right|^2."
  },
  {
    "objectID": "06_basic_unc.html#second-order-necessary-conditions",
    "href": "06_basic_unc.html#second-order-necessary-conditions",
    "title": "06_basic_unc",
    "section": "Second-Order Necessary Conditions",
    "text": "Second-Order Necessary Conditions\n\n\n\nProposition (Second-Order Necessary Conditions)\n\n\nLet Î©âŠ†â„n\\Omega \\subseteq \\mathbb{R}^n and let fâˆˆC2f \\in C^2 on Î©\\Omega. If ğ±*\\bm{x}^\\ast is a relative minimum point of ff over Î©\\Omega, then for any ğâˆˆâ„n\\bm{d} \\in \\mathbb{R}^n that is a feasible direction at ğ±*\\bm{x}^\\ast we have\nâˆ‡f(ğ±*)ğâ‰¥0(2) \\nabla f(\\bm{x}^\\ast) \\bm{d} \\geq 0  \\qquad(2) âˆ‡f(ğ±*)ğ=0, then ğâŠ¤âˆ‡2f(ğ±*)ğ=ğâŠ¤ğ…(ğ±*)ğâ‰¥0.(3) \\nabla f(\\bm{x}^\\ast) \\bm{d} = 0, \\;\\; \\text{ then } \\;\\; \\bm{d}^\\top\n\\nabla^2 f(\\bm{x}^\\ast) \\bm{d} = \\bm{d}^\\top \\bm{F}(\\bm{x}^\\ast) \\bm{d} \\geq 0. \\qquad(3)\n\n\n\n\n\n\nProof\n\n\nThe first condition is just the FONC, and the second applies only if âˆ‡f(ğ±*)ğ=0\\nabla f(\\bm{x}^\\ast)\\bm{d} = 0. In this case, introducing ğ±(Î±)=ğ±*+Î±ğ\\bm{x}(\\alpha) = \\bm{x}^\\ast + \\alpha \\bm{d} and g(Î±)=f(ğ±(Î±))g(\\alpha) = f(\\bm{x}(\\alpha)) as before, we have, in view of gâ€²(Î±)=0g'(\\alpha) = 0,\ng(Î±)âˆ’g(0)=12gâ€³(0)Î±2+o(Î±2). g(\\alpha) - g(0) = \\frac{1}{2}g''(0)\\alpha^2 + o(\\alpha^2). \nIf gâ€³(0)&lt;0g''(0) &lt; 0a the rhs of the above equation is negative for sufficiently small Î±\\alpha which contradicts the relative minimum nature of g(0)g(0). Thus\ngâ€³(0)=ğâŠ¤âˆ‡2f(ğ±*)ğâ‰¥0. g''(0) = \\bm{d}^\\top \\nabla^2 f(\\bm{x}^\\ast)\\bm{d} \\geq 0. \n\n\n\n\nSee Example 2 from FOSC slide with d2=0d_2 = 0 whence ğâŠ¤âˆ‡2f(ğ±*)ğ=2d12â‰¥0\\bm{d}^\\top \\nabla^2 f(\\bm{x}^\\ast) \\bm{d} = 2d_1^2 \\geq 0, satisfying the second condition."
  },
  {
    "objectID": "06_basic_unc.html#second-order-necessary-conditions-1",
    "href": "06_basic_unc.html#second-order-necessary-conditions-1",
    "title": "06_basic_unc",
    "section": "Second-Order Necessary Conditions",
    "text": "Second-Order Necessary Conditions\n\n\n\nProposition (Second-Order Necessary Conditions â€” Unconstrained Case)\n\n\nLet ğ±*âˆˆÎ©ÌŠ\\bm{x}^\\ast \\in \\mathring{\\Omega} and suppose ğ±*\\bm{x}^\\ast is a relative minimum point over Î©\\Omega of fâˆˆC2f \\in C^2. Then\nâˆ‡f(ğ±*)=0(4) \\nabla f(\\bm{x}^\\ast) = 0  \\qquad(4) âˆ€ğ,ğâŠ¤ğ…(ğ±*)ğâ‰¥0.(5) \\forall \\bm{d},  \\;\\; \\bm{d}^\\top \\bm{F}(\\bm{x}^\\ast) \\bm{d} \\geq 0. \\qquad(5)\n\nThe second condition is equivalent to stating that the matrix ğ…(ğ±*)\\bm{F}(\\bm{x}^\\ast) is positive semidefinite.\n\n\n\n\n\n\n\n\n\nExample\n\n\nminimizef(x1,x2)=x13âˆ’x12x2+2x22subject tox1,x2â‰¥0.\n\\begin{align}\n\\operatorname{minimize} & f(x_1, x_2) = x_1^3 - x_1^2x_2 + 2x_2^2 \\\\\n\\text{subject to} & x_1, x_2 \\geq 0.\n\\end{align}\n\n\n\n\n\nIf x1,x2&gt;0x_1, x_2 &gt; 0, then the FONC are\n3x12âˆ’2x1x2=0,âˆ’x12+4x2=0. 3x_1^2 - 2x_1x_2 = 0, \\quad -x_1^2 + 4x_2 = 0. \nwith a solution at x1=6x_1 = 6, x2=9x_2 = 9.\n\n\n\n\n\nNote that for x1x_1 fixed at x1=6x_1 = 6, the objective attains a relative minimum w.r.t. x2x_2 at $x_2 = 9.\nConversely, with x2x_2 fixed at x2=9x_2 = 9, the objective attains a relative minimum w.r.t. x1x_1 at x1=6x_1 = 6.\n\n\n\nDespite this fact, the point x1=6x_1 = 6, x2=9x_2 =9 is not a relative minimum because the Hessian matrix is\nğ…(ğ±)=[6x1âˆ’2x2âˆ’2x1âˆ’2x14];ğ…(ğ±*)=[18âˆ’12âˆ’124]â‹¡0.\n\\bm{F}(\\bm{x}) = \\begin{bmatrix} 6x_1 - 2x_2 & -2x_1 \\\\ -2x_1 & 4 \\end{bmatrix};\n\\qquad \n\\bm{F}(\\bm{x}^\\ast) = \\begin{bmatrix} 18 & -12 \\\\ -12 & 4 \\end{bmatrix} \\nsucceq  0."
  },
  {
    "objectID": "06_basic_unc.html#second-order-sufficient-conditions",
    "href": "06_basic_unc.html#second-order-sufficient-conditions",
    "title": "06_basic_unc",
    "section": "Second-Order Sufficient Conditions",
    "text": "Second-Order Sufficient Conditions\n\n\n\nProposition (Second-Order Sufficient Conditions)\n\n\nLet fâˆˆC2f \\in C^2 be a function defined on a region in which the point ğ±*\\bm{x}^\\ast is an interior point. Suppose\nâˆ‡f(ğ±*)=0(6) \\nabla f(\\bm{x}^\\ast) = 0  \\qquad(6) ğ…(ğ±*)â‰»0(7) \\bm{F}(\\bm{x}^\\ast) \\succ 0  \\qquad(7)\nThen ğ±*\\bm{x}^\\ast is a strict relative minimum of ff.\n\n\n\n\n\n\nProof\n\n\nSince ğ…(ğ±*)\\bm{F}(\\bm{x}^\\ast) is positive definite, there is an a&gt;0a &gt; 0 such that for all ğ\\bm{d}, ğâŠ¤ğ…(ğ±*)ğâ‰¥a|ğ|2\\bm{d}^\\top \\bm{F}(\\bm{x}^\\ast)\\bm{d} \\geq a |\\bm{d}|^2. By Taylorâ€™s Theorem (with remainder)\nf(ğ±*+ğ)âˆ’f(ğ±*)=12ğâŠ¤ğ…(ğ±*)ğ+o(|ğ|2)â‰¥a2|ğ|2+o(|ğ|2). f(\\bm{x}^\\ast + \\bm{d}) - f(\\bm{x}^\\ast) = \\frac{1}{2} \\bm{d}^\\top\n\\bm{F}(\\bm{x}^\\ast)\\bm{d} + o(|\\bm{d}|^2) \\geq \\frac{a}{2}|\\bm{d}|^2 +\no(|\\bm{d}|^2). \nFor small |ğ||\\bm{d}|, the first term on the right dominates the second, implying that both sides are positive for small ğ\\bm{d}."
  }
]